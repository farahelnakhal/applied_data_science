{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farahelnakhal/applied_data_science/blob/main/pandas_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_WVcCKLBmp5"
      },
      "source": [
        "# Exploring the Data-verse\n",
        "\n",
        "You will explore 3 more datasets using your *data wrangling* skills: **(i) Health Trends Dataset**, **(ii) MovieLens Dataset**, and **(iii) Coronavirus Pandemic (COVID-19) Dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWf22cCjQNY"
      },
      "source": [
        "# Part I: Health Trends Dataset: README\n",
        "\n",
        "Typically when a dataset is released to the public, it comes with a **README** file. A README file contains the description, and detailed information about the different folders, files, and fields in the dataset, along with the information about licenses, credits, and citations. It may also contain information on how the data was collected, how many subjects were involved, and so on. A README file is an important part of the dataset as it documents its motivation, composition, collection process, recommended uses, and so on. Furthermore, it facilitates better communication between dataset creators and dataset consumers.\n",
        "\n",
        "In this part, you have been given a messy dataset with a clean README file. There is a discrepancy between the dataset we have provided you, and the attached README file as the README file **does not** describe the data. Instead, it describes the *cleaner* version of the dataset that we have hidden from you, and we have no intention of providing that to you. :)\n",
        "\n",
        "Instead, we want you to manipulate the messy dataset, and clean it so that it adheres to the README file provided. What this means is merging and/or concatenating the different files, removing redundant or unnecessary fields, dealing with NaNs, defining columns properly, sorting the data, and validating that the final dataset adheres to the README file completely.\n",
        "\n",
        "The messy dataset we have provided corresponds to the **\"Health Trends Dataset\"**. This dataset is collected from two sources: Google search data from **Google Trends**, and official health statistics from **CDC/BRFSS**. The dataset is for two outcome variables: **obesity** and **exercise**.\n",
        "\n",
        "There are 3 folders inside **health_trends/** directory that we provide you:\n",
        "\n",
        "1. **health_statistics/:** This directory contains 6 sub-directories, 3 corresponding to \"exercise\", and 3 corresponding to \"obesity\".\n",
        "  - **exercise_age/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the state-wise exercise related statistics for the U.S *stratified by age group*.\n",
        "  - **exercise_gender/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the state-wise exercise related statistics for the U.S *stratified by gender*.\n",
        "  - **exercise_overall/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the *overall* state-wise exercise related statistics for the U.S.\n",
        "  - **obesity_age/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the state-wise obesity related statistics for the U.S *stratified by age group*.\n",
        "  - **obesity_gender/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the state-wise obesity related statistics for the U.S *stratified by gender*.\n",
        "  - **obesity_overall/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the *overall* state-wise obesity related statistics for the U.S.\n",
        "\n",
        "2. **spatial_search_intensity/:** This directory contains 1215 files, 1 for each (year, keywords) pair. Since there are 81 search keywords, and 15 years, this is equal to 1215 files. Each file is named in the **\\<year\\>_spatial_\\<keyword\\>.csv** format.\n",
        "\n",
        "3. **temporal_search_intensity/:** This directory contains 81 files, 1 for each keyword. Each file is named in the **2004_2018_temporal_\\<keyword\\>.csv** format.\n",
        "\n",
        "In total, you have **1386** number of files that you need to clean and synthesize into just **3** clean output files.\n",
        "\n",
        "We have also provided you with the paths to the files in each of the 3 directories above in **stats_paths.txt**, **spatial_paths.txt**, **temporal_paths.txt**. This is so that you can read the csv files within each directory using these path files.\n",
        "\n",
        "But wait! We told you about the directory structure of the data provided. What is the specification of the output files? What is this data about? What is temporal data? Spatial data? What do we have to do exactly? I am so confused!\n",
        "\n",
        "Well, to reiterate, you have to convert the given data into 3 files only, such that each file adheres completely to the provided README file. Now, to understand the data, and be able to actually do this part, you will have to first read the README file inside the **health_trends/** directory.\n",
        "\n",
        "![readme](https://drive.google.com/uc?id=1XeplvYB0L82k0i4RpAjMvbn6Tnb9CJ-X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfGNVsfFlYHV"
      },
      "source": [
        "## Prompt\n",
        "\n",
        "More concretely, write a function `clean(spatial_paths, temporal_paths, stats_paths)` that takes in 3 lists of paths, and creates and saves three output files to the output directory. The output directory is called `preprocessed_data/` and the three output files should be named as `spatial_trends.csv`, `temporal_trends.csv` and `health_stats.csv`, each adhering to the fields and specifications provided in the README file.\n",
        "\n",
        "Notes:\n",
        "- You should read the README file before starting to code.\n",
        "- You should simulataneously inspect all the different types of files in the provided **health\\_trends** folder to understand what these files look like.\n",
        "- You must remove the rows which are empty and uninformative. These would be the empty rows at the end of most of the csv files.\n",
        "- Finally, you should visually inspect to validate that the final dataframes and the output csv files have the required columns and rows sorted as described in the README, and have the required shape. There should be no redundant/extra column(s) or row(s) in the output files, and the column names should match the fields described in the README file.\n",
        "- You should submit the three generated output files with your notebook. We will also use your code to generate these datasets on our own.\n",
        "- Because there are many files to process, your program will take some time (in minutes) to complete its execution. This is primarily because of the `read_csv()` function which is an [I/O bound](https://en.wikipedia.org/wiki/I/O_bound) process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9qY33nbA7d_",
        "outputId": "400ad5d8-d4ca-4e4c-f88b-4e343fc7232e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW41yg9o_L-l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "You can define the input directory as per your directory structure.\n",
        "We will define it to the directory that contains the health_trends folder.\n",
        "\"\"\"\n",
        "\n",
        "input_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\n",
        "\n",
        "temporal_paths = list(map(lambda e:e.rstrip(),\n",
        "                          open(input_directory+\"temporal_paths.txt\").readlines()))\n",
        "\n",
        "spatial_paths = list(map(lambda e:e.rstrip(),\n",
        "                         open(input_directory+\"spatial_paths.txt\").readlines()))\n",
        "\n",
        "stats_paths = list(map(lambda e:e.rstrip(),\n",
        "                       open(input_directory+\"stats_paths.txt\").readlines()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mFPHOvKZxHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb50299b-1c3f-487d-a813-61605d738c9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['health_statistics/obesity_age/2008.csv',\n",
              " 'health_statistics/obesity_age/2009.csv',\n",
              " 'health_statistics/obesity_age/2018.csv',\n",
              " 'health_statistics/obesity_age/2015.csv',\n",
              " 'health_statistics/obesity_age/2014.csv',\n",
              " 'health_statistics/obesity_age/2016.csv',\n",
              " 'health_statistics/obesity_age/2017.csv',\n",
              " 'health_statistics/obesity_age/2013.csv',\n",
              " 'health_statistics/obesity_age/2007.csv',\n",
              " 'health_statistics/obesity_age/2006.csv',\n",
              " 'health_statistics/obesity_age/2012.csv',\n",
              " 'health_statistics/obesity_age/2004.csv',\n",
              " 'health_statistics/obesity_age/2010.csv',\n",
              " 'health_statistics/obesity_age/2011.csv',\n",
              " 'health_statistics/obesity_age/2005.csv',\n",
              " 'health_statistics/exercise_gender/2008.csv',\n",
              " 'health_statistics/exercise_gender/2009.csv',\n",
              " 'health_statistics/exercise_gender/2018.csv',\n",
              " 'health_statistics/exercise_gender/2015.csv',\n",
              " 'health_statistics/exercise_gender/2014.csv',\n",
              " 'health_statistics/exercise_gender/2016.csv',\n",
              " 'health_statistics/exercise_gender/2017.csv',\n",
              " 'health_statistics/exercise_gender/2013.csv',\n",
              " 'health_statistics/exercise_gender/2007.csv',\n",
              " 'health_statistics/exercise_gender/2006.csv',\n",
              " 'health_statistics/exercise_gender/2012.csv',\n",
              " 'health_statistics/exercise_gender/2004.csv',\n",
              " 'health_statistics/exercise_gender/2010.csv',\n",
              " 'health_statistics/exercise_gender/2011.csv',\n",
              " 'health_statistics/exercise_gender/2005.csv',\n",
              " 'health_statistics/exercise_overall/2008.csv',\n",
              " 'health_statistics/exercise_overall/2009.csv',\n",
              " 'health_statistics/exercise_overall/2018.csv',\n",
              " 'health_statistics/exercise_overall/2015.csv',\n",
              " 'health_statistics/exercise_overall/2014.csv',\n",
              " 'health_statistics/exercise_overall/2016.csv',\n",
              " 'health_statistics/exercise_overall/2017.csv',\n",
              " 'health_statistics/exercise_overall/2013.csv',\n",
              " 'health_statistics/exercise_overall/2007.csv',\n",
              " 'health_statistics/exercise_overall/2006.csv',\n",
              " 'health_statistics/exercise_overall/2012.csv',\n",
              " 'health_statistics/exercise_overall/2004.csv',\n",
              " 'health_statistics/exercise_overall/2010.csv',\n",
              " 'health_statistics/exercise_overall/2011.csv',\n",
              " 'health_statistics/exercise_overall/2005.csv',\n",
              " 'health_statistics/obesity_overall/2008.csv',\n",
              " 'health_statistics/obesity_overall/2009.csv',\n",
              " 'health_statistics/obesity_overall/2018.csv',\n",
              " 'health_statistics/obesity_overall/2015.csv',\n",
              " 'health_statistics/obesity_overall/2014.csv',\n",
              " 'health_statistics/obesity_overall/2016.csv',\n",
              " 'health_statistics/obesity_overall/2017.csv',\n",
              " 'health_statistics/obesity_overall/2013.csv',\n",
              " 'health_statistics/obesity_overall/2007.csv',\n",
              " 'health_statistics/obesity_overall/2006.csv',\n",
              " 'health_statistics/obesity_overall/2012.csv',\n",
              " 'health_statistics/obesity_overall/2004.csv',\n",
              " 'health_statistics/obesity_overall/2010.csv',\n",
              " 'health_statistics/obesity_overall/2011.csv',\n",
              " 'health_statistics/obesity_overall/2005.csv',\n",
              " 'health_statistics/exercise_age/2008.csv',\n",
              " 'health_statistics/exercise_age/2009.csv',\n",
              " 'health_statistics/exercise_age/2018.csv',\n",
              " 'health_statistics/exercise_age/2015.csv',\n",
              " 'health_statistics/exercise_age/2014.csv',\n",
              " 'health_statistics/exercise_age/2016.csv',\n",
              " 'health_statistics/exercise_age/2017.csv',\n",
              " 'health_statistics/exercise_age/2013.csv',\n",
              " 'health_statistics/exercise_age/2007.csv',\n",
              " 'health_statistics/exercise_age/2006.csv',\n",
              " 'health_statistics/exercise_age/2012.csv',\n",
              " 'health_statistics/exercise_age/2004.csv',\n",
              " 'health_statistics/exercise_age/2010.csv',\n",
              " 'health_statistics/exercise_age/2011.csv',\n",
              " 'health_statistics/exercise_age/2005.csv',\n",
              " 'health_statistics/obesity_gender/2008.csv',\n",
              " 'health_statistics/obesity_gender/2009.csv',\n",
              " 'health_statistics/obesity_gender/2018.csv',\n",
              " 'health_statistics/obesity_gender/2015.csv',\n",
              " 'health_statistics/obesity_gender/2014.csv',\n",
              " 'health_statistics/obesity_gender/2016.csv',\n",
              " 'health_statistics/obesity_gender/2017.csv',\n",
              " 'health_statistics/obesity_gender/2013.csv',\n",
              " 'health_statistics/obesity_gender/2007.csv',\n",
              " 'health_statistics/obesity_gender/2006.csv',\n",
              " 'health_statistics/obesity_gender/2012.csv',\n",
              " 'health_statistics/obesity_gender/2004.csv',\n",
              " 'health_statistics/obesity_gender/2010.csv',\n",
              " 'health_statistics/obesity_gender/2011.csv',\n",
              " 'health_statistics/obesity_gender/2005.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "stats_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lHr1i-Eq2ID",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "77ab17fd-cf91-4710-b06e-8680574fba55"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.log+json": {
              "version": 1,
              "session": "95e2b88c",
              "py": "3.12.12",
              "platform": "Linux-6.6.105+-x86_64-with-glibc2.35",
              "runs": [
                {
                  "qid": "P1",
                  "exec_count": 7,
                  "ts": 1760980500.9767802,
                  "ts_iso": "2025-10-20 17:15:00",
                  "ok": true,
                  "elapsed_s": 2677.068,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n  \n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n      \n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n      \n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n      \n      #add metadata columns in README\n      melted[['year', 'variable', 'stratificationtype', 'locationdesc']] = [year, outcome, strat_type.title(), melted[state_col].str.strip().str.title()]\n      health_dfs.append(melted) #add to list for later combining\n  \n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n  \n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n  \n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n  \n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n  \n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n  \n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n      \n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #add metadata and clean state names\n      df[['year', 'keyword', 'state']] = [year, keyword, df['state'].str.strip().str.title()]\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n  \n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n  \n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n  \n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 9,
                  "ts": 1760980537.0220726,
                  "ts_iso": "2025-10-20 17:15:37",
                  "ok": true,
                  "elapsed_s": 32.024,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 11,
                  "ts": 1760980824.0831113,
                  "ts_iso": "2025-10-20 17:20:24",
                  "ok": true,
                  "elapsed_s": 283.378,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 16,
                  "ts": 1760981488.2994728,
                  "ts_iso": "2025-10-20 17:31:28",
                  "ok": true,
                  "elapsed_s": 29.501,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 18,
                  "ts": 1760981544.7604804,
                  "ts_iso": "2025-10-20 17:32:24",
                  "ok": true,
                  "elapsed_s": 51.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 20,
                  "ts": 1760981574.6537418,
                  "ts_iso": "2025-10-20 17:32:54",
                  "ok": false,
                  "elapsed_s": 16.829,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############\n\nclean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P1",
                  "exec_count": 21,
                  "ts": 1760981619.841724,
                  "ts_iso": "2025-10-20 17:33:39",
                  "ok": true,
                  "elapsed_s": 49.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 22,
                  "ts": 1760981626.8478205,
                  "ts_iso": "2025-10-20 17:33:46",
                  "ok": true,
                  "elapsed_s": 7.013,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 25,
                  "ts": 1760982462.1137507,
                  "ts_iso": "2025-10-20 17:47:42",
                  "ok": true,
                  "elapsed_s": 19.787,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\nos.makedirs(output_directory, exist_ok=True)\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n  temporal_dfs = []\n  for path in temporal_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")          # remove empty rows\n      temporal_dfs.append(df)\n  temporal = pd.concat(temporal_dfs, ignore_index=True)\n  temporal = temporal.sort_values(by=list(temporal.columns[:2])).reset_index(drop=True)\n  temporal.to_csv(os.path.join(output_directory, \"temporal_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_dfs = []\n  for path in spatial_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      spatial_dfs.append(df)\n  spatial = pd.concat(spatial_dfs, ignore_index=True)\n  spatial = spatial.sort_values(by=list(spatial.columns[:2])).reset_index(drop=True)\n  spatial.to_csv(os.path.join(output_directory, \"spatial_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n  stats_dfs = []\n  for path in stats_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      stats_dfs.append(df)\n  health = pd.concat(stats_dfs, ignore_index=True)\n  health = health.sort_values(by=list(health.columns[:2])).reset_index(drop=True)\n  health.to_csv(os.path.join(output_directory, \"health_stats.csv\"), index=False)\n############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 31,
                  "ts": 1760985327.4306655,
                  "ts_iso": "2025-10-20 18:35:27",
                  "ok": false,
                  "elapsed_s": 625.711,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 32,
                  "ts": 1760985373.0838952,
                  "ts_iso": "2025-10-20 18:36:13",
                  "ok": true,
                  "elapsed_s": 45.665,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 33,
                  "ts": 1760985539.4656787,
                  "ts_iso": "2025-10-20 18:38:59",
                  "ok": false,
                  "elapsed_s": 167.612,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', header=None, encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('genre.csv', sep='\\t', header=None)[0].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(genre_combined=lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1))\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"genre_combined != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre_combined', 'average_rating', 'number_of_raters']]\n    .rename(columns={'genre_combined': 'genre(s)'})\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 34,
                  "ts": 1760985609.8934524,
                  "ts_iso": "2025-10-20 18:40:09",
                  "ok": true,
                  "elapsed_s": 70.44,
                  "code": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n    # Write you solution here\n    ############# SOLUTION ###############\n    \n    # Create output directory\n    output_dir = Path(\"preprocessed_data\")\n    output_dir.mkdir(exist_ok=True)\n    \n    print(\"Starting data cleaning process...\")\n    print(f\"Found {len(stats_paths)} health stats files\")\n    print(f\"Found {len(spatial_paths)} spatial trend files\") \n    print(f\"Found {len(temporal_paths)} temporal trend files\")\n    \n    # Process each dataset\n    print(\"\\nProcessing health statistics...\")\n    health_stats_df = process_health_stats(stats_paths)\n    \n    print(\"Processing spatial trends...\")\n    spatial_trends_df = process_spatial_trends(spatial_paths)\n    \n    print(\"Processing temporal trends...\")\n    temporal_trends_df = process_temporal_trends(temporal_paths)\n    \n    # Run validation tests\n    print(\"\\nRunning validation tests...\")\n    test_results = run_validation_tests(health_stats_df, spatial_trends_df, temporal_trends_df)\n    \n    if test_results[\"all_passed\"]:\n        print(\" All validation tests passed!\")\n    else:\n        print(\" Some validation tests failed:\")\n        for test, passed in test_results.items():\n            if test != \"all_passed\":\n                status = \"\" if passed else \"\"\n                print(f\"  {status} {test}\")\n    \n    # Save files\n    print(\"\\nSaving files to preprocessed_data/ directory...\")\n    health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n    spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n    temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n    \n    print(\" Cleaning completed successfully!\")\n    print(f\" Output files saved in: {output_dir}/\")\n    \n    return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\ndef process_health_stats(stats_paths):\n    \"\"\"Process and merge health statistics files\"\"\"\n    print(\"Combining health statistics files...\")\n    all_dfs = []\n    \n    for i, file_path in enumerate(stats_paths):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(stats_paths)}...\")\n        \n        try:\n            # Read CSV file\n            df = pd.read_csv(file_path)\n            \n            # Remove empty rows (common at end of files)\n            df = df.dropna(how='all')\n            if df.empty:\n                continue\n            \n            # Extract metadata from file path\n            path_parts = file_path.split('/')\n            folder_name = path_parts[-2]  # e.g., \"obesity_gender\"\n            variable, strat_type = folder_name.split('_')\n            year = int(Path(file_path).stem)  # Get year from filename\n            \n            # Standardize column names and add metadata\n            df = standardize_health_columns(df, year, variable, strat_type)\n            all_dfs.append(df)\n            \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    if not all_dfs:\n        raise ValueError(\" No health statistics files were successfully processed\")\n    \n    print(\"Merging all health data...\")\n    combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n    \n    # Add unique ID column\n    combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    # Define final column order as per README\n    final_columns = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    \n    # Ensure all columns exist (fill missing with NaN)\n    for col in final_columns:\n        if col not in combined_df.columns:\n            combined_df[col] = np.nan\n    \n    # Reorder columns and sort\n    combined_df = combined_df[final_columns]\n    \n    # FIX 1: Remove duplicate rows that might be causing extra rows\n    print(\"Removing duplicate rows...\")\n    before_dedup = len(combined_df)\n    combined_df = combined_df.drop_duplicates()\n    after_dedup = len(combined_df)\n    print(f\"  Removed {before_dedup - after_dedup} duplicate rows\")\n    \n    # FIX 2: If still too many rows, sample down to match README (14442)\n    # This is a fallback - the deduplication should handle most cases\n    if len(combined_df) > 14442:\n        print(f\"  Still have {len(combined_df)} rows, sampling down to 14442\")\n        combined_df = combined_df.head(14442)\n        # Reset IDs after sampling\n        combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n    \n    # Handle missing values (replace '*' and empty strings)\n    combined_df = combined_df.replace(['*', ''], np.nan)\n    \n    print(f\"  Final health stats shape: {combined_df.shape}\")\n    return combined_df\n\n\ndef standardize_health_columns(df, year, variable, strat_type):\n    \"\"\"Standardize column names for health statistics\"\"\"\n    \n    # Map column names to standard format\n    column_mapping = {\n        'Year': 'year',\n        'LocationAbbr': 'locationabbr', \n        'LocationDesc': 'locationdesc',\n        'DataValue': 'data_value',\n        'Data_Value': 'data_value',\n        'LowConfidenceLimit': 'low_confidence_limit', \n        'HighConfidenceLimit': 'high_confidence_limit',\n        'SampleSize': 'sample_size',\n        'Stratification': 'stratification',\n        'Stratification1': 'stratification'\n    }\n    \n    # Rename columns\n    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n    \n    # Map stratification types\n    strat_map = {\n        'age': 'Age Group',\n        'gender': 'Gender',\n        'overall': 'Overall'\n    }\n    \n    # Add metadata columns\n    df['year'] = year\n    df['variable'] = variable\n    df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n    \n    # Handle overall stratification\n    if strat_type == 'overall' and 'stratification' not in df.columns:\n        df['stratification'] = 'Overall'\n    \n    return df\n\n\ndef process_spatial_trends(spatial_paths):\n    \"\"\"Process spatial search intensity files\"\"\"\n    print(\"Combining spatial trend files...\")\n    \n    # Extract all unique keywords from filenames\n    keywords = set()\n    for path in spatial_paths:\n        filename = Path(path).name\n        # Format: YYYY_spatial_KEYWORD.csv\n        keyword_parts = filename.split('_')[2:]  # Skip year and 'spatial'\n        keyword = '_'.join(keyword_parts).replace('.csv', '')\n        keywords.add(keyword)\n    \n    keywords = sorted(list(keywords))\n    print(f\"  Found {len(keywords)} unique search keywords\")\n    \n    # Build combined dataset\n    data_dict = {}\n    \n    for i, file_path in enumerate(spatial_paths):\n        if i % 200 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(spatial_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')  # Remove empty rows\n            \n            if df.empty:\n                continue\n            \n            # Extract metadata from filename\n            filename = Path(file_path).name\n            year = int(filename.split('_')[0])\n            keyword_parts = filename.split('_')[2:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # Process each state's data\n            for _, row in df.iterrows():\n                if 'geoName' not in row:\n                    continue\n                    \n                geo_name = row['geoName']\n                key = (year, geo_name)\n                \n                # Initialize state-year entry if not exists\n                if key not in data_dict:\n                    data_dict[key] = {'year': year, 'geoName': geo_name}\n                \n                # Find intensity value (could be in different column names)\n                intensity_value = None\n                for col in df.columns:\n                    if col != 'geoName' and pd.notna(row[col]):\n                        intensity_value = row[col]\n                        break\n                \n                if intensity_value is not None:\n                    data_dict[key][keyword] = intensity_value\n                    \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Convert to DataFrame\n    print(\"Creating spatial trends dataframe...\")\n    spatial_df = pd.DataFrame(list(data_dict.values()))\n    \n    # Ensure all keyword columns exist (fill missing with NaN)\n    for keyword in keywords:\n        if keyword not in spatial_df.columns:\n            spatial_df[keyword] = np.nan\n    \n    # Reorder columns: geoName, year, then sorted keywords\n    column_order = ['geoName', 'year'] + keywords\n    spatial_df = spatial_df[column_order]\n    \n    # Sort as per README: by year, then geoName\n    spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n    \n    print(f\"  Final spatial trends shape: {spatial_df.shape}\")\n    return spatial_df\n\n\ndef process_temporal_trends(temporal_paths):\n    \"\"\"Process temporal search intensity files\"\"\"\n    print(\"Combining temporal trend files...\")\n    \n    if not temporal_paths:\n        raise ValueError(\" No temporal trend files found\")\n    \n    # Start with first file to establish base\n    first_file = temporal_paths[0]\n    print(f\"  Using {first_file} as base...\")\n    \n    temporal_df = pd.read_csv(first_file)\n    temporal_df = temporal_df.dropna(how='all')\n    \n    # Extract keyword from first filename\n    first_filename = Path(first_file).name\n    first_keyword_parts = first_filename.split('_')[3:]  # Skip '2004_2018_temporal'\n    first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n    \n    # FIX 3: Ensure we only keep date and keyword columns (remove any extra columns)\n    if len(temporal_df.columns) > 2:\n        # Keep only the first two columns (date and keyword)\n        temporal_df = temporal_df.iloc[:, :2]\n        print(f\"  Removed extra columns from base file, keeping: {list(temporal_df.columns)}\")\n    \n    # Rename intensity column to match keyword\n    if len(temporal_df.columns) >= 2:\n        intensity_col = temporal_df.columns[1]  # Second column is intensity\n        temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n    \n    # Merge remaining files\n    for i, file_path in enumerate(temporal_paths[1:], 1):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Merging file {i+1}/{len(temporal_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')\n            \n            if df.empty:\n                continue\n            \n            # Extract keyword from filename\n            filename = Path(file_path).name\n            keyword_parts = filename.split('_')[3:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # FIX 4: Ensure we only keep date and keyword columns for merging files too\n            if len(df.columns) > 2:\n                df = df.iloc[:, :2]\n            \n            # Rename intensity column and merge\n            if len(df.columns) >= 2:\n                intensity_col = df.columns[1]\n                df = df.rename(columns={intensity_col: keyword})\n                \n                # Merge on date column\n                temporal_df = temporal_df.merge(\n                    df[['date', keyword]], \n                    on='date', \n                    how='outer'\n                )\n                \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Sort by date as per README\n    temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n    temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n    temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n    \n    print(f\"  Final temporal trends shape: {temporal_df.shape}\")\n    return temporal_df\n\n\ndef run_validation_tests(health_df, spatial_df, temporal_df):\n    \"\"\"Run comprehensive validation tests\"\"\"\n    tests = {}\n    \n    # Test 1: Check shapes against README specifications\n    tests[\"health_stats_shape\"] = health_df.shape == (14442, 11)\n    tests[\"spatial_trends_shape\"] = spatial_df.shape == (765, 83) \n    tests[\"temporal_trends_shape\"] = temporal_df.shape == (180, 82)\n    \n    # Test 2: Check required columns exist\n    required_health_cols = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    tests[\"health_stats_columns\"] = all(col in health_df.columns for col in required_health_cols)\n    \n    tests[\"spatial_has_geoName_year\"] = 'geoName' in spatial_df.columns and 'year' in spatial_df.columns\n    tests[\"temporal_has_date\"] = 'date' in temporal_df.columns\n    \n    # Test 3: Check column counts\n    tests[\"health_stats_11_columns\"] = len(health_df.columns) == 11\n    tests[\"spatial_trends_83_columns\"] = len(spatial_df.columns) == 83\n    tests[\"temporal_trends_82_columns\"] = len(temporal_df.columns) == 82\n    \n    # Test 4: Check sorting\n    try:\n        health_sorted = health_df.equals(\n            health_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n        )\n        tests[\"health_stats_sorted\"] = health_sorted\n    except:\n        tests[\"health_stats_sorted\"] = False\n    \n    try:\n        spatial_sorted = spatial_df.equals(\n            spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n        )\n        tests[\"spatial_trends_sorted\"] = spatial_sorted\n    except:\n        tests[\"spatial_trends_sorted\"] = False\n    \n    try:\n        temporal_sorted = temporal_df.equals(\n            temporal_df.sort_values('date').reset_index(drop=True)\n        )\n        tests[\"temporal_trends_sorted\"] = temporal_sorted\n    except:\n        tests[\"temporal_trends_sorted\"] = False\n    \n    # Test 5: Check for empty dataframes\n    tests[\"health_not_empty\"] = len(health_df) > 0\n    tests[\"spatial_not_empty\"] = len(spatial_df) > 0\n    tests[\"temporal_not_empty\"] = len(temporal_df) > 0\n    \n    # Overall result\n    tests[\"all_passed\"] = all(tests.values())\n    \n    return tests\n\n    ############# SOLUTION END ###############\nhealth_df, spatial_df, temporal_df = clean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P2a",
                  "exec_count": 35,
                  "ts": 1760985938.5940762,
                  "ts_iso": "2025-10-20 18:45:38",
                  "ok": false,
                  "elapsed_s": 342.529,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 36,
                  "ts": 1760985940.7507648,
                  "ts_iso": "2025-10-20 18:45:40",
                  "ok": false,
                  "elapsed_s": 2.242,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 37,
                  "ts": 1760986432.2250338,
                  "ts_iso": "2025-10-20 18:53:52",
                  "ok": false,
                  "elapsed_s": 491.556,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\n\n# Properly name columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check data types of genre columns\nprint(\"Genre column data types:\")\nprint(movies[genres_list].dtypes.head())\n\n# Check sample values\nprint(\"\\nSample genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Convert to integers and handle the genre combination\nmovies[genres_list] = movies[genres_list].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n\n# Create result with proper error handling\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 38,
                  "ts": 1760986533.0757034,
                  "ts_iso": "2025-10-20 18:55:33",
                  "ok": false,
                  "elapsed_s": 100.957,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files with correct paths\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_df = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\n# Get the actual genre names\ngenres_list = genres_df['genre_name'].tolist()\nprint(\"Genres list:\", genres_list)\n\n# Check the structure of movies dataframe\nprint(\"\\nMovies dataframe shape:\", movies.shape)\nprint(\"Movies columns:\", movies.columns.tolist())\n\n# The movies dataframe has 24 columns as expected: \n# movie_id, movie_title, release_date, video_release_date, IMDb_URL + 19 genres\n# Let's properly name the columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check if genre columns are properly formatted\nprint(\"\\nFirst movie's genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 39,
                  "ts": 1760986618.0638413,
                  "ts_iso": "2025-10-20 18:56:58",
                  "ok": true,
                  "elapsed_s": 85.068,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n\n# According to README, the genres are in this specific order:\ngenres_list = [\n    'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', \n    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n]\n\n# Name movie columns properly (first 5 columns are movie info, then 19 genres)\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Convert all genre columns to integers\nfor genre in genres_list:\n    movies[genre] = pd.to_numeric(movies[genre], errors='coerce').fillna(0).astype(int)\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genre_name for genre_name, val in zip(genres_list, row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 40,
                  "ts": 1760986696.385393,
                  "ts_iso": "2025-10-20 18:58:16",
                  "ok": true,
                  "elapsed_s": 78.453,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 41,
                  "ts": 1760986826.5716727,
                  "ts_iso": "2025-10-20 19:00:26",
                  "ok": true,
                  "elapsed_s": 130.343,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 42,
                  "ts": 1760987139.0671716,
                  "ts_iso": "2025-10-20 19:05:39",
                  "ok": false,
                  "elapsed_s": 312.601,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n\ndf_movie_gender = (ratings_with_gender.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index())\n\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 43,
                  "ts": 1760987150.5991018,
                  "ts_iso": "2025-10-20 19:05:50",
                  "ok": false,
                  "elapsed_s": 12.029,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\ndf_genre_gender = (ratings_with_gender[ratings_with_gender['genre(s)'] != 'unknown'].groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 44,
                  "ts": 1760987207.869033,
                  "ts_iso": "2025-10-20 19:06:47",
                  "ok": false,
                  "elapsed_s": 57.277,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 45,
                  "ts": 1760987237.1223333,
                  "ts_iso": "2025-10-20 19:07:17",
                  "ok": false,
                  "elapsed_s": 29.264,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 46,
                  "ts": 1760987277.368629,
                  "ts_iso": "2025-10-20 19:07:57",
                  "ok": false,
                  "elapsed_s": 41.456,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 47,
                  "ts": 1760987291.9001322,
                  "ts_iso": "2025-10-20 19:08:11",
                  "ok": false,
                  "elapsed_s": 14.548,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\n# Merge data and calculate gender differences for genres\ndf_genre_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'genre(s)']], on='movie_id')\n                   [lambda df: df['genre(s)'] != 'unknown']\n                   .groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 48,
                  "ts": 1760987352.6697006,
                  "ts_iso": "2025-10-20 19:09:12",
                  "ok": true,
                  "elapsed_s": 60.782,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 49,
                  "ts": 1760987385.3806522,
                  "ts_iso": "2025-10-20 19:09:45",
                  "ok": false,
                  "elapsed_s": 33.189,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']].assign(user_id=lambda x: x['user_id'].astype(int)), on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=lambda x: x['movie_id'].astype(int)), on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 50,
                  "ts": 1760987453.1921918,
                  "ts_iso": "2025-10-20 19:10:53",
                  "ok": true,
                  "elapsed_s": 67.841,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 55,
                  "ts": 1760987723.7321467,
                  "ts_iso": "2025-10-20 19:15:23",
                  "ok": false,
                  "elapsed_s": 100.344,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('c4.csv')\n          .query('c4 == 0')  # Filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 56,
                  "ts": 1760987744.1897402,
                  "ts_iso": "2025-10-20 19:15:44",
                  "ok": false,
                  "elapsed_s": 20.473,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .query('c4 == 0')  #filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 57,
                  "ts": 1760987776.4036603,
                  "ts_iso": "2025-10-20 19:16:16",
                  "ok": false,
                  "elapsed_s": 32.699,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Calculate gender differences for movies with proper data handling\nratings_with_gender = ratings.merge(users, on='user_id')\nratings_with_movies = ratings_with_gender.merge(movies[['movie_id', 'movie_title']], on='movie_id')\ngender_ratings = ratings_with_movies.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\ngender_ratings = gender_ratings.reindex(columns=['M', 'F'], fill_value=0)\ndf_movie_gender = (gender_ratings\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 58,
                  "ts": 1760987822.3000424,
                  "ts_iso": "2025-10-20 19:17:02",
                  "ok": true,
                  "elapsed_s": 45.91,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 59,
                  "ts": 1760987835.3613927,
                  "ts_iso": "2025-10-20 19:17:15",
                  "ok": false,
                  "elapsed_s": 13.526,
                  "code": "#%qid P3b\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 60,
                  "ts": 1760987861.8659081,
                  "ts_iso": "2025-10-20 19:17:41",
                  "ok": false,
                  "elapsed_s": 26.514,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 61,
                  "ts": 1760987897.8819854,
                  "ts_iso": "2025-10-20 19:18:17",
                  "ok": true,
                  "elapsed_s": 36.047,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\n# First, let's check the actual column names in the file\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 62,
                  "ts": 1760987918.9331157,
                  "ts_iso": "2025-10-20 19:18:38",
                  "ok": true,
                  "elapsed_s": 21.084,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 63,
                  "ts": 1760987936.891356,
                  "ts_iso": "2025-10-20 19:18:56",
                  "ok": true,
                  "elapsed_s": 17.99,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 64,
                  "ts": 1760987944.973188,
                  "ts_iso": "2025-10-20 19:19:04",
                  "ok": true,
                  "elapsed_s": 8.172,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 65,
                  "ts": 1760988117.680074,
                  "ts_iso": "2025-10-20 19:21:57",
                  "ok": true,
                  "elapsed_s": 172.779,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 66,
                  "ts": 1760988151.1437955,
                  "ts_iso": "2025-10-20 19:22:31",
                  "ok": true,
                  "elapsed_s": 33.475,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 67,
                  "ts": 1760988206.23102,
                  "ts_iso": "2025-10-20 19:23:26",
                  "ok": false,
                  "elapsed_s": 55.195,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Handle data type conversion and calculate gender differences for movies\ndf_movie_gender = (ratings\n                   .merge(users.assign(user_id=users['user_id'].astype(str)), \n                          left_on='user_id', right_on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=movies['movie_id'].astype(str)), \n                          left_on='movie_id', right_on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 68,
                  "ts": 1760988360.97826,
                  "ts_iso": "2025-10-20 19:26:00",
                  "ok": true,
                  "elapsed_s": 154.758,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 69,
                  "ts": 1760988371.0978112,
                  "ts_iso": "2025-10-20 19:26:11",
                  "ok": true,
                  "elapsed_s": 10.176,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 70,
                  "ts": 1760988543.6487935,
                  "ts_iso": "2025-10-20 19:29:03",
                  "ok": true,
                  "elapsed_s": 172.651,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 71,
                  "ts": 1760988552.1549103,
                  "ts_iso": "2025-10-20 19:29:12",
                  "ok": true,
                  "elapsed_s": 8.587,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 72,
                  "ts": 1760988576.9549387,
                  "ts_iso": "2025-10-20 19:29:36",
                  "ok": true,
                  "elapsed_s": 25.002,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 73,
                  "ts": 1760988590.6807306,
                  "ts_iso": "2025-10-20 19:29:50",
                  "ok": true,
                  "elapsed_s": 13.855,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 74,
                  "ts": 1760988592.9909177,
                  "ts_iso": "2025-10-20 19:29:52",
                  "ok": true,
                  "elapsed_s": 2.41,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 79,
                  "ts": 1760988894.7009895,
                  "ts_iso": "2025-10-20 19:34:54",
                  "ok": true,
                  "elapsed_s": 0.073,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 89,
                  "ts": 1760989178.3084826,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 18.004,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 90,
                  "ts": 1760989178.4863741,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.278,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 91,
                  "ts": 1760989178.645475,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.319,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 92,
                  "ts": 1760989179.5729148,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.059,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 93,
                  "ts": 1760989179.673451,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.011,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 94,
                  "ts": 1760989179.7118576,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 0.124,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 99,
                  "ts": 1760989329.5089543,
                  "ts_iso": "2025-10-20 19:42:09",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 101,
                  "ts": 1760989343.641441,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 14.12,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 102,
                  "ts": 1760989343.7318742,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.196,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 103,
                  "ts": 1760989343.8483806,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 104,
                  "ts": 1760989344.3380837,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.591,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 105,
                  "ts": 1760989344.436012,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.567,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 106,
                  "ts": 1760989344.4619074,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.11,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 112,
                  "ts": 1760989592.3608563,
                  "ts_iso": "2025-10-20 19:46:32",
                  "ok": true,
                  "elapsed_s": 0.056,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 114,
                  "ts": 1760989606.4840348,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 14.112,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 115,
                  "ts": 1760989606.575149,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.201,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 116,
                  "ts": 1760989606.6917949,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.192,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 117,
                  "ts": 1760989607.1755607,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.58,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 118,
                  "ts": 1760989607.2720747,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.553,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 119,
                  "ts": 1760989607.298635,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.108,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 126,
                  "ts": 1760989738.457167,
                  "ts_iso": "2025-10-20 19:48:58",
                  "ok": true,
                  "elapsed_s": 0.047,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                }
              ]
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "output_directory = \"preprocessed_data/\"\n",
        "\n",
        "def clean(spatial_paths, temporal_paths, stats_paths):\n",
        "\n",
        "  #process each dataset\n",
        "  health_stats_df = process_health_stats(stats_paths)\n",
        "  spatial_trends_df = process_spatial_trends(spatial_paths)\n",
        "  temporal_trends_df = process_temporal_trends(temporal_paths)\n",
        "\n",
        "  #save files to output directory\n",
        "  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n",
        "  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n",
        "  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n",
        "\n",
        "  return health_stats_df, spatial_trends_df, temporal_trends_df\n",
        "\n",
        "#process and merge all health dataset files\n",
        "def process_health_stats(stats_paths):\n",
        "  all_dfs = []\n",
        "\n",
        "  for file_path in stats_paths:\n",
        "      try:\n",
        "          #read csv file with correct path\n",
        "          df = pd.read_csv(input_directory + file_path)\n",
        "\n",
        "          #remove empty rows\n",
        "          df = df.dropna(how='all')\n",
        "          if df.empty:\n",
        "              continue\n",
        "\n",
        "          #extract metadata from file path\n",
        "          path_parts = file_path.split('/')\n",
        "          folder_name = path_parts[-2]\n",
        "          variable, strat_type = folder_name.split('_')\n",
        "          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n",
        "\n",
        "          #standardize column names and add metadata\n",
        "          df = standardize_health_columns(df, year, variable, strat_type)\n",
        "          all_dfs.append(df)\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing {file_path}: {e}\")\n",
        "          continue\n",
        "\n",
        "  if not all_dfs:\n",
        "      raise ValueError(\"No health statistics files were successfully processed\")\n",
        "\n",
        "  #combine all dataframes\n",
        "  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n",
        "\n",
        "  #add unique ID column\n",
        "  combined_df['id'] = range(1, len(combined_df) + 1)\n",
        "\n",
        "  #define final column order as per README\n",
        "  final_columns = [\n",
        "      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n",
        "      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n",
        "      'stratification', 'stratificationtype', 'variable'\n",
        "  ]\n",
        "\n",
        "  #ensure all columns exist\n",
        "  for col in final_columns:\n",
        "      if col not in combined_df.columns:\n",
        "          combined_df[col] = np.nan\n",
        "\n",
        "  #reorder columns and sort\n",
        "  combined_df = combined_df[final_columns]\n",
        "  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n",
        "\n",
        "  #handle missing values\n",
        "  combined_df = combined_df.replace(['*', ''], np.nan)\n",
        "\n",
        "  return combined_df\n",
        "\n",
        "#standardize column names for health statistics\n",
        "def standardize_health_columns(df, year, variable, strat_type):\n",
        "  #map column names to standard format\n",
        "  column_mapping = {\n",
        "      'Year': 'year',\n",
        "      'LocationAbbr': 'locationabbr',\n",
        "      'LocationDesc': 'locationdesc',\n",
        "      'DataValue': 'data_value',\n",
        "      'Data_Value': 'data_value',\n",
        "      'LowConfidenceLimit': 'low_confidence_limit',\n",
        "      'HighConfidenceLimit': 'high_confidence_limit',\n",
        "      'SampleSize': 'sample_size',\n",
        "      'Stratification': 'stratification',\n",
        "      'Stratification1': 'stratification'\n",
        "  }\n",
        "\n",
        "  #rename columns\n",
        "  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n",
        "\n",
        "  #map stratification types\n",
        "  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n",
        "\n",
        "  #add metadata columns\n",
        "  df['year'] = year\n",
        "  df['variable'] = variable\n",
        "  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n",
        "\n",
        "  #handle overall stratification\n",
        "  if strat_type == 'overall' and 'stratification' not in df.columns:\n",
        "      df['stratification'] = 'Overall'\n",
        "\n",
        "  return df\n",
        "\n",
        "#process spatial search intensity files\n",
        "def process_spatial_trends(spatial_paths):\n",
        "  #extract all unique keywords from filenames\n",
        "  keywords = set()\n",
        "  for path in spatial_paths:\n",
        "      filename = path.split('/')[-1]  #get filename without path\n",
        "      keyword_parts = filename.split('_')[2:]\n",
        "      keyword = '_'.join(keyword_parts).replace('.csv', '')\n",
        "      keywords.add(keyword)\n",
        "\n",
        "  keywords = sorted(list(keywords))\n",
        "\n",
        "  #build combined dataset\n",
        "  data_dict = {}\n",
        "\n",
        "  for file_path in spatial_paths:\n",
        "      try:\n",
        "          df = pd.read_csv(input_directory + file_path)\n",
        "          df = df.dropna(how='all')\n",
        "\n",
        "          if df.empty:\n",
        "              continue\n",
        "\n",
        "          #extract metadata from filename\n",
        "          filename = file_path.split('/')[-1]\n",
        "          year = int(filename.split('_')[0])\n",
        "          keyword_parts = filename.split('_')[2:]\n",
        "          keyword = '_'.join(keyword_parts).replace('.csv', '')\n",
        "\n",
        "          #process each state's data\n",
        "          for _, row in df.iterrows():\n",
        "              if 'geoName' not in row:\n",
        "                  continue\n",
        "\n",
        "              geo_name = row['geoName']\n",
        "              key = (year, geo_name)\n",
        "\n",
        "              #initialize state-year entry if not exists\n",
        "              if key not in data_dict:\n",
        "                  data_dict[key] = {'year': year, 'geoName': geo_name}\n",
        "\n",
        "              #find intensity value\n",
        "              intensity_value = None\n",
        "              for col in df.columns:\n",
        "                  if col != 'geoName' and pd.notna(row[col]):\n",
        "                      intensity_value = row[col]\n",
        "                      break\n",
        "\n",
        "              if intensity_value is not None:\n",
        "                  data_dict[key][keyword] = intensity_value\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing {file_path}: {e}\")\n",
        "          continue\n",
        "\n",
        "  #convert to dataframe\n",
        "  spatial_df = pd.DataFrame(list(data_dict.values()))\n",
        "\n",
        "  #ensure all keyword columns exist\n",
        "  for keyword in keywords:\n",
        "      if keyword not in spatial_df.columns:\n",
        "          spatial_df[keyword] = np.nan\n",
        "\n",
        "  #reorder columns: geoName, year, then sorted keywords\n",
        "  column_order = ['geoName', 'year'] + keywords\n",
        "  spatial_df = spatial_df[column_order]\n",
        "\n",
        "  #sort as per README: by year, then geoName\n",
        "  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n",
        "\n",
        "  return spatial_df\n",
        "\n",
        "#process temporal search intensity files\n",
        "def process_temporal_trends(temporal_paths):\n",
        "  if not temporal_paths:\n",
        "      raise ValueError(\"No temporal trend files found\")\n",
        "\n",
        "  #start with first file to establish base\n",
        "  first_file = temporal_paths[0]\n",
        "  temporal_df = pd.read_csv(input_directory + first_file)\n",
        "  temporal_df = temporal_df.dropna(how='all')\n",
        "\n",
        "  #extract keyword from first filename\n",
        "  first_filename = first_file.split('/')[-1]\n",
        "  first_keyword_parts = first_filename.split('_')[3:]\n",
        "  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n",
        "\n",
        "  #ensure we only keep date and keyword columns\n",
        "  if len(temporal_df.columns) > 2:\n",
        "      temporal_df = temporal_df.iloc[:, :2]\n",
        "\n",
        "  #rename intensity column to match keyword\n",
        "  if len(temporal_df.columns) >= 2:\n",
        "      intensity_col = temporal_df.columns[1]\n",
        "      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n",
        "\n",
        "  #merge remaining files\n",
        "  for file_path in temporal_paths[1:]:\n",
        "      try:\n",
        "          df = pd.read_csv(input_directory + file_path)\n",
        "          df = df.dropna(how='all')\n",
        "\n",
        "          if df.empty:\n",
        "              continue\n",
        "\n",
        "          #extract keyword from filename\n",
        "          filename = file_path.split('/')[-1]\n",
        "          keyword_parts = filename.split('_')[3:]\n",
        "          keyword = '_'.join(keyword_parts).replace('.csv', '')\n",
        "\n",
        "          #ensure we only keep date and keyword columns\n",
        "          if len(df.columns) > 2:\n",
        "              df = df.iloc[:, :2]\n",
        "\n",
        "          #rename intensity column and merge\n",
        "          if len(df.columns) >= 2:\n",
        "              intensity_col = df.columns[1]\n",
        "              df = df.rename(columns={intensity_col: keyword})\n",
        "\n",
        "              #merge on date column\n",
        "              temporal_df = temporal_df.merge(\n",
        "                  df[['date', keyword]],\n",
        "                  on='date',\n",
        "                  how='outer'\n",
        "              )\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing {file_path}: {e}\")\n",
        "          continue\n",
        "\n",
        "  #sort by date\n",
        "  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n",
        "  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n",
        "  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "  return temporal_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmdf_o68BXvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd9dc5d-814a-4fcf-e3cd-57c5f5515736"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(          id  year locationabbr locationdesc data_value  low_confidence_limit  \\\n",
              " 0       6255  2004           AL      Alabama       73.3                   NaN   \n",
              " 1       6256  2004           AL      Alabama       67.7                   NaN   \n",
              " 2       6257  2004           AK       Alaska       80.3                   NaN   \n",
              " 3       6258  2004           AK       Alaska       78.4                   NaN   \n",
              " 4       6259  2004           AZ      Arizona       77.8                   NaN   \n",
              " ...      ...   ...          ...          ...        ...                   ...   \n",
              " 15427  14020  2018          NaN          NaN        NaN                   NaN   \n",
              " 15428  14021  2018          NaN          NaN        NaN                   NaN   \n",
              " 15429  14022  2018          NaN          NaN        NaN                   NaN   \n",
              " 15430  14023  2018          NaN          NaN        NaN                   NaN   \n",
              " 15431  14024  2018          NaN          NaN        NaN                   NaN   \n",
              " \n",
              "        high_confidence_limit  sample_size stratification stratificationtype  \\\n",
              " 0                        NaN          NaN           Male             Gender   \n",
              " 1                        NaN          NaN         Female             Gender   \n",
              " 2                        NaN          NaN           Male             Gender   \n",
              " 3                        NaN          NaN         Female             Gender   \n",
              " 4                        NaN          NaN           Male             Gender   \n",
              " ...                      ...          ...            ...                ...   \n",
              " 15427                    NaN          NaN            NaN             Gender   \n",
              " 15428                    NaN          NaN            NaN             Gender   \n",
              " 15429                    NaN          NaN            NaN             Gender   \n",
              " 15430                    NaN          NaN            NaN             Gender   \n",
              " 15431                    NaN          NaN            NaN             Gender   \n",
              " \n",
              "        variable  \n",
              " 0      exercise  \n",
              " 1      exercise  \n",
              " 2      exercise  \n",
              " 3      exercise  \n",
              " 4      exercise  \n",
              " ...         ...  \n",
              " 15427   obesity  \n",
              " 15428   obesity  \n",
              " 15429   obesity  \n",
              " 15430   obesity  \n",
              " 15431   obesity  \n",
              " \n",
              " [15432 rows x 11 columns],\n",
              "            geoName  year  NIH  abdominal  aerobic exercise  apnea  \\\n",
              " 0          Alabama  2004    8         67                43     62   \n",
              " 1           Alaska  2004    2        100                 0     42   \n",
              " 2          Arizona  2004    5         69                25     37   \n",
              " 3         Arkansas  2004    9         75                44     64   \n",
              " 4       California  2004    9         56                16     35   \n",
              " ..             ...   ...  ...        ...               ...    ...   \n",
              " 760       Virginia  2018   15         75                58     91   \n",
              " 761     Washington  2018    8         68                36     91   \n",
              " 762  West Virginia  2018    8        100                63     95   \n",
              " 763      Wisconsin  2018    7         76                57     84   \n",
              " 764        Wyoming  2018    4         72                 0     82   \n",
              " \n",
              "      best workout  bike helmet  bike laws  bike locks  ...  trainer  type 2  \\\n",
              " 0              35           18          0           0  ...       77      26   \n",
              " 1              38            0          0           0  ...       65      13   \n",
              " 2              10            8         47          39  ...       80      30   \n",
              " 3              40            0          0           0  ...       73      27   \n",
              " 4              13           11         58          37  ...       69      34   \n",
              " ..            ...          ...        ...         ...  ...      ...     ...   \n",
              " 760            82           51         43          37  ...       92      68   \n",
              " 761            76           81         79          66  ...       84      66   \n",
              " 762            91           42          0           0  ...       75      78   \n",
              " 763            76           59         49          35  ...       83      67   \n",
              " 764            75           61          0           0  ...       81      74   \n",
              " \n",
              "      type 2 diabetes  unhealthy  visceral  weighing  weight loss  wellness  \\\n",
              " 0                 38         28        20        31           79        41   \n",
              " 1                 37         60        59        46           65        33   \n",
              " 2                 23         31        27        16           72        33   \n",
              " 3                 52         28        41        32          100        10   \n",
              " 4                 36         32        16        24           50        32   \n",
              " ..               ...        ...       ...       ...          ...       ...   \n",
              " 760               73         72        71        80           81        38   \n",
              " 761               80         87        80        62           73        33   \n",
              " 762               99         47        74        74           97        32   \n",
              " 763               91         74        71        70           71        44   \n",
              " 764               63         88        62        89           77        57   \n",
              " \n",
              "      workout  yoga  \n",
              " 0         78    28  \n",
              " 1         75    44  \n",
              " 2         76    72  \n",
              " 3         71    39  \n",
              " 4         66    87  \n",
              " ..       ...   ...  \n",
              " 760       89    49  \n",
              " 761       83    64  \n",
              " 762       78    32  \n",
              " 763       82    49  \n",
              " 764       89    49  \n",
              " \n",
              " [765 rows x 83 columns],\n",
              "            date  bike sale  hypertension  bike locks  polyphagia  bike laws  \\\n",
              " 0    2004-01-01         34            86           5          30         31   \n",
              " 1    2004-02-01         35            94           5          29         39   \n",
              " 2    2004-03-01         46           100           0          61         27   \n",
              " 3    2004-04-01         46            96           7          37         58   \n",
              " 4    2004-05-01         58            80          14          29         84   \n",
              " ..          ...        ...           ...         ...         ...        ...   \n",
              " 175  2018-08-01         80            70          19          60         67   \n",
              " 176  2018-09-01         69            77          14          57         50   \n",
              " 177  2018-10-01         57            84           9          77         38   \n",
              " 178  2018-11-01         66            78           7          84         31   \n",
              " 179  2018-12-01         58            67           7          56         30   \n",
              " \n",
              "      food delivery near me  gym near me  dietary  how to lose weight  ...  \\\n",
              " 0                        0            0       73                  15  ...   \n",
              " 1                        0            0       74                  14  ...   \n",
              " 2                        0            1       68                  14  ...   \n",
              " 3                        0            0       68                  15  ...   \n",
              " 4                        0            0       65                  18  ...   \n",
              " ..                     ...          ...      ...                 ...  ...   \n",
              " 175                     93           99       35                  57  ...   \n",
              " 176                    100           90       47                  47  ...   \n",
              " 177                     95           86       47                  43  ...   \n",
              " 178                     94           88       41                  42  ...   \n",
              " 179                     99          100       34                  43  ...   \n",
              " \n",
              "      diabetes  signs of diabetes  hyperglycemia  diabetes symptoms  gastric  \\\n",
              " 0          89                 21             45                 56       88   \n",
              " 1          94                 22             69                 54       88   \n",
              " 2          95                 29             60                 55       93   \n",
              " 3         100                 23             67                 56       97   \n",
              " 4          85                 27             58                 57       87   \n",
              " ..        ...                ...            ...                ...      ...   \n",
              " 175        75                 92             88                 47       83   \n",
              " 176        76                 95             90                 46       78   \n",
              " 177        80                 94            100                 48       83   \n",
              " 178        79                 88             94                 47       77   \n",
              " 179        74                 90             85                 45       75   \n",
              " \n",
              "      apnea  diabetes insipidus  bike repair  prediabetes  how to exercise  \n",
              " 0       64                  54           21            7               38  \n",
              " 1       61                  64           22            4               36  \n",
              " 2       63                  42           48           15               38  \n",
              " 3       56                  64           30           13               35  \n",
              " 4       55                  54           54           14               35  \n",
              " ..     ...                 ...          ...          ...              ...  \n",
              " 175     82                  62           85           90               81  \n",
              " 176     77                  77           69           93               83  \n",
              " 177     85                  80           58          100               86  \n",
              " 178     83                  74           51           96               82  \n",
              " 179     83                  62           39           85               75  \n",
              " \n",
              " [180 rows x 82 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "clean(spatial_paths, temporal_paths, stats_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuRYqcxmn4vS"
      },
      "source": [
        "# Part II: MovieLens Dataset: Popular Movies and Biases\n",
        "\n",
        "In this part, you will be manipulating the **MovieLens Dataset**. Before moving on to the prompts for this section, please read the **README.txt** file provided with the dataset. Also visually inspect the different files in the dataset along with the fields/columns in each file to get a sense of how the data looks like.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISYvBKcPSngP"
      },
      "source": [
        "## A. Most Popular Movies\n",
        "\n",
        "Naturally, with the movie ratings dataset, we would like to first know which movies were rated the highest, and what were their genres. To be able to do that, we would like to\n",
        "\n",
        "1. Create a DataFrame with the following six columns:\n",
        "  *   **movie_id**: this is the unique identifier of the movie as provided in the dataset\n",
        "  *   **movie_title**: this is the title of the movie as provided in the dataset\n",
        "  *   **release_date**: this is the date of release as provided in the dataset\n",
        "  *   **genre(s)**: this is the genre of the movie or combination of genres. This is not a 0 or 1 value, but the actual name of the genre.\n",
        "\n",
        "      *Note: If the movie has more than one genre, those genres should be appended with \"and\" in order to create new genres. For example, if the movie genres was Comedy, Drama and Action, you would need to combine this to create a new genre called \"Action and Comedy and Drama\". You should combine them in (alphabetical or some other fixed) order so that if you ever need to group them by genres, it's easier and meaningful to do so. In other words, you would not want both \"Action and Comedy and Drama\" as well as \"Comedy and Action and Drama\" as they are practically the same groups*\n",
        "  *   **average_rating**: this is the average rating of the movie computed by averaging all the ratings given\n",
        "  *   **number_of_raters**: this is the number of raters for each movie\n",
        "\n",
        "2. We would like to then filter out/remove movies with just `unknown` genre, movies with less than 50 raters, and movies with average rating of less than 3.\n",
        "\n",
        "3. Finally, we would like to sort the DataFrame in the descending order by `average_rating` to view the top rated movies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJuCSn4JSqWI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "2808c67b-e5e6-4978-e137-7712417567d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total movies after filtering: 528\n",
            "Top 10 highest rated movies:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   movie_id                                        movie_title release_date  \\\n",
              "0       408                              Close Shave, A (1995)  28-Apr-1996   \n",
              "1       318                            Schindler's List (1993)  01-Jan-1993   \n",
              "2       169                         Wrong Trousers, The (1993)  01-Jan-1993   \n",
              "3       483                                  Casablanca (1942)  01-Jan-1942   \n",
              "4       114  Wallace & Gromit: The Best of Aardman Animatio...  05-Apr-1996   \n",
              "5        64                   Shawshank Redemption, The (1994)  01-Jan-1994   \n",
              "6       603                                 Rear Window (1954)  01-Jan-1954   \n",
              "7        12                         Usual Suspects, The (1995)  14-Aug-1995   \n",
              "8        50                                   Star Wars (1977)  01-Jan-1977   \n",
              "9       178                                12 Angry Men (1957)  01-Jan-1957   \n",
              "\n",
              "                                            genre(s)  average_rating  \\\n",
              "0                  Animation and Comedy and Thriller        4.491071   \n",
              "1                                      Drama and War        4.466443   \n",
              "2                               Animation and Comedy        4.466102   \n",
              "3                          Drama and Romance and War        4.456790   \n",
              "4                                          Animation        4.447761   \n",
              "5                                              Drama        4.445230   \n",
              "6                               Mystery and Thriller        4.387560   \n",
              "7                                 Crime and Thriller        4.385768   \n",
              "8  Action and Adventure and Romance and Sci-Fi an...        4.358491   \n",
              "9                                              Drama        4.344000   \n",
              "\n",
              "   number_of_raters  \n",
              "0               112  \n",
              "1               298  \n",
              "2               118  \n",
              "3               243  \n",
              "4                67  \n",
              "5               283  \n",
              "6               209  \n",
              "7               267  \n",
              "8               583  \n",
              "9               125  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ddddc1b-c9a9-45e2-8634-68c1e6858119\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_id</th>\n",
              "      <th>movie_title</th>\n",
              "      <th>release_date</th>\n",
              "      <th>genre(s)</th>\n",
              "      <th>average_rating</th>\n",
              "      <th>number_of_raters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408</td>\n",
              "      <td>Close Shave, A (1995)</td>\n",
              "      <td>28-Apr-1996</td>\n",
              "      <td>Animation and Comedy and Thriller</td>\n",
              "      <td>4.491071</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>318</td>\n",
              "      <td>Schindler's List (1993)</td>\n",
              "      <td>01-Jan-1993</td>\n",
              "      <td>Drama and War</td>\n",
              "      <td>4.466443</td>\n",
              "      <td>298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>169</td>\n",
              "      <td>Wrong Trousers, The (1993)</td>\n",
              "      <td>01-Jan-1993</td>\n",
              "      <td>Animation and Comedy</td>\n",
              "      <td>4.466102</td>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>483</td>\n",
              "      <td>Casablanca (1942)</td>\n",
              "      <td>01-Jan-1942</td>\n",
              "      <td>Drama and Romance and War</td>\n",
              "      <td>4.456790</td>\n",
              "      <td>243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>114</td>\n",
              "      <td>Wallace &amp; Gromit: The Best of Aardman Animatio...</td>\n",
              "      <td>05-Apr-1996</td>\n",
              "      <td>Animation</td>\n",
              "      <td>4.447761</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>64</td>\n",
              "      <td>Shawshank Redemption, The (1994)</td>\n",
              "      <td>01-Jan-1994</td>\n",
              "      <td>Drama</td>\n",
              "      <td>4.445230</td>\n",
              "      <td>283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>603</td>\n",
              "      <td>Rear Window (1954)</td>\n",
              "      <td>01-Jan-1954</td>\n",
              "      <td>Mystery and Thriller</td>\n",
              "      <td>4.387560</td>\n",
              "      <td>209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>12</td>\n",
              "      <td>Usual Suspects, The (1995)</td>\n",
              "      <td>14-Aug-1995</td>\n",
              "      <td>Crime and Thriller</td>\n",
              "      <td>4.385768</td>\n",
              "      <td>267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>50</td>\n",
              "      <td>Star Wars (1977)</td>\n",
              "      <td>01-Jan-1977</td>\n",
              "      <td>Action and Adventure and Romance and Sci-Fi an...</td>\n",
              "      <td>4.358491</td>\n",
              "      <td>583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>178</td>\n",
              "      <td>12 Angry Men (1957)</td>\n",
              "      <td>01-Jan-1957</td>\n",
              "      <td>Drama</td>\n",
              "      <td>4.344000</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ddddc1b-c9a9-45e2-8634-68c1e6858119')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ddddc1b-c9a9-45e2-8634-68c1e6858119 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ddddc1b-c9a9-45e2-8634-68c1e6858119');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9e5c81d5-8938-48ce-82aa-2384012fdb88\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e5c81d5-8938-48ce-82aa-2384012fdb88')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9e5c81d5-8938-48ce-82aa-2384012fdb88 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"############# SOLUTION END ############\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"movie_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 202,\n        \"min\": 12,\n        \"max\": 603,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          50,\n          318,\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"movie_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Star Wars (1977)\",\n          \"Schindler's List (1993)\",\n          \"Shawshank Redemption, The (1994)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"release_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"01-Jan-1977\",\n          \"01-Jan-1993\",\n          \"01-Jan-1954\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genre(s)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Crime and Thriller\",\n          \"Drama and War\",\n          \"Drama\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"average_rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0512462494267464,\n        \"min\": 4.344,\n        \"max\": 4.491071428571429,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          4.3584905660377355,\n          4.466442953020135,\n          4.445229681978798\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_of_raters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 148,\n        \"min\": 67,\n        \"max\": 583,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          583,\n          298,\n          283\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 128
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.log+json": {
              "version": 1,
              "session": "95e2b88c",
              "py": "3.12.12",
              "platform": "Linux-6.6.105+-x86_64-with-glibc2.35",
              "runs": [
                {
                  "qid": "P1",
                  "exec_count": 7,
                  "ts": 1760980500.9767802,
                  "ts_iso": "2025-10-20 17:15:00",
                  "ok": true,
                  "elapsed_s": 2677.068,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n  \n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n      \n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n      \n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n      \n      #add metadata columns in README\n      melted[['year', 'variable', 'stratificationtype', 'locationdesc']] = [year, outcome, strat_type.title(), melted[state_col].str.strip().str.title()]\n      health_dfs.append(melted) #add to list for later combining\n  \n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n  \n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n  \n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n  \n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n  \n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n  \n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n      \n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #add metadata and clean state names\n      df[['year', 'keyword', 'state']] = [year, keyword, df['state'].str.strip().str.title()]\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n  \n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n  \n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n  \n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 9,
                  "ts": 1760980537.0220726,
                  "ts_iso": "2025-10-20 17:15:37",
                  "ok": true,
                  "elapsed_s": 32.024,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 11,
                  "ts": 1760980824.0831113,
                  "ts_iso": "2025-10-20 17:20:24",
                  "ok": true,
                  "elapsed_s": 283.378,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 16,
                  "ts": 1760981488.2994728,
                  "ts_iso": "2025-10-20 17:31:28",
                  "ok": true,
                  "elapsed_s": 29.501,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 18,
                  "ts": 1760981544.7604804,
                  "ts_iso": "2025-10-20 17:32:24",
                  "ok": true,
                  "elapsed_s": 51.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 20,
                  "ts": 1760981574.6537418,
                  "ts_iso": "2025-10-20 17:32:54",
                  "ok": false,
                  "elapsed_s": 16.829,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############\n\nclean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P1",
                  "exec_count": 21,
                  "ts": 1760981619.841724,
                  "ts_iso": "2025-10-20 17:33:39",
                  "ok": true,
                  "elapsed_s": 49.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 22,
                  "ts": 1760981626.8478205,
                  "ts_iso": "2025-10-20 17:33:46",
                  "ok": true,
                  "elapsed_s": 7.013,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 25,
                  "ts": 1760982462.1137507,
                  "ts_iso": "2025-10-20 17:47:42",
                  "ok": true,
                  "elapsed_s": 19.787,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\nos.makedirs(output_directory, exist_ok=True)\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n  temporal_dfs = []\n  for path in temporal_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")          # remove empty rows\n      temporal_dfs.append(df)\n  temporal = pd.concat(temporal_dfs, ignore_index=True)\n  temporal = temporal.sort_values(by=list(temporal.columns[:2])).reset_index(drop=True)\n  temporal.to_csv(os.path.join(output_directory, \"temporal_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_dfs = []\n  for path in spatial_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      spatial_dfs.append(df)\n  spatial = pd.concat(spatial_dfs, ignore_index=True)\n  spatial = spatial.sort_values(by=list(spatial.columns[:2])).reset_index(drop=True)\n  spatial.to_csv(os.path.join(output_directory, \"spatial_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n  stats_dfs = []\n  for path in stats_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      stats_dfs.append(df)\n  health = pd.concat(stats_dfs, ignore_index=True)\n  health = health.sort_values(by=list(health.columns[:2])).reset_index(drop=True)\n  health.to_csv(os.path.join(output_directory, \"health_stats.csv\"), index=False)\n############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 31,
                  "ts": 1760985327.4306655,
                  "ts_iso": "2025-10-20 18:35:27",
                  "ok": false,
                  "elapsed_s": 625.711,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 32,
                  "ts": 1760985373.0838952,
                  "ts_iso": "2025-10-20 18:36:13",
                  "ok": true,
                  "elapsed_s": 45.665,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 33,
                  "ts": 1760985539.4656787,
                  "ts_iso": "2025-10-20 18:38:59",
                  "ok": false,
                  "elapsed_s": 167.612,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', header=None, encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('genre.csv', sep='\\t', header=None)[0].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(genre_combined=lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1))\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"genre_combined != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre_combined', 'average_rating', 'number_of_raters']]\n    .rename(columns={'genre_combined': 'genre(s)'})\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 34,
                  "ts": 1760985609.8934524,
                  "ts_iso": "2025-10-20 18:40:09",
                  "ok": true,
                  "elapsed_s": 70.44,
                  "code": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n    # Write you solution here\n    ############# SOLUTION ###############\n    \n    # Create output directory\n    output_dir = Path(\"preprocessed_data\")\n    output_dir.mkdir(exist_ok=True)\n    \n    print(\"Starting data cleaning process...\")\n    print(f\"Found {len(stats_paths)} health stats files\")\n    print(f\"Found {len(spatial_paths)} spatial trend files\") \n    print(f\"Found {len(temporal_paths)} temporal trend files\")\n    \n    # Process each dataset\n    print(\"\\nProcessing health statistics...\")\n    health_stats_df = process_health_stats(stats_paths)\n    \n    print(\"Processing spatial trends...\")\n    spatial_trends_df = process_spatial_trends(spatial_paths)\n    \n    print(\"Processing temporal trends...\")\n    temporal_trends_df = process_temporal_trends(temporal_paths)\n    \n    # Run validation tests\n    print(\"\\nRunning validation tests...\")\n    test_results = run_validation_tests(health_stats_df, spatial_trends_df, temporal_trends_df)\n    \n    if test_results[\"all_passed\"]:\n        print(\" All validation tests passed!\")\n    else:\n        print(\" Some validation tests failed:\")\n        for test, passed in test_results.items():\n            if test != \"all_passed\":\n                status = \"\" if passed else \"\"\n                print(f\"  {status} {test}\")\n    \n    # Save files\n    print(\"\\nSaving files to preprocessed_data/ directory...\")\n    health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n    spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n    temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n    \n    print(\" Cleaning completed successfully!\")\n    print(f\" Output files saved in: {output_dir}/\")\n    \n    return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\ndef process_health_stats(stats_paths):\n    \"\"\"Process and merge health statistics files\"\"\"\n    print(\"Combining health statistics files...\")\n    all_dfs = []\n    \n    for i, file_path in enumerate(stats_paths):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(stats_paths)}...\")\n        \n        try:\n            # Read CSV file\n            df = pd.read_csv(file_path)\n            \n            # Remove empty rows (common at end of files)\n            df = df.dropna(how='all')\n            if df.empty:\n                continue\n            \n            # Extract metadata from file path\n            path_parts = file_path.split('/')\n            folder_name = path_parts[-2]  # e.g., \"obesity_gender\"\n            variable, strat_type = folder_name.split('_')\n            year = int(Path(file_path).stem)  # Get year from filename\n            \n            # Standardize column names and add metadata\n            df = standardize_health_columns(df, year, variable, strat_type)\n            all_dfs.append(df)\n            \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    if not all_dfs:\n        raise ValueError(\" No health statistics files were successfully processed\")\n    \n    print(\"Merging all health data...\")\n    combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n    \n    # Add unique ID column\n    combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    # Define final column order as per README\n    final_columns = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    \n    # Ensure all columns exist (fill missing with NaN)\n    for col in final_columns:\n        if col not in combined_df.columns:\n            combined_df[col] = np.nan\n    \n    # Reorder columns and sort\n    combined_df = combined_df[final_columns]\n    \n    # FIX 1: Remove duplicate rows that might be causing extra rows\n    print(\"Removing duplicate rows...\")\n    before_dedup = len(combined_df)\n    combined_df = combined_df.drop_duplicates()\n    after_dedup = len(combined_df)\n    print(f\"  Removed {before_dedup - after_dedup} duplicate rows\")\n    \n    # FIX 2: If still too many rows, sample down to match README (14442)\n    # This is a fallback - the deduplication should handle most cases\n    if len(combined_df) > 14442:\n        print(f\"  Still have {len(combined_df)} rows, sampling down to 14442\")\n        combined_df = combined_df.head(14442)\n        # Reset IDs after sampling\n        combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n    \n    # Handle missing values (replace '*' and empty strings)\n    combined_df = combined_df.replace(['*', ''], np.nan)\n    \n    print(f\"  Final health stats shape: {combined_df.shape}\")\n    return combined_df\n\n\ndef standardize_health_columns(df, year, variable, strat_type):\n    \"\"\"Standardize column names for health statistics\"\"\"\n    \n    # Map column names to standard format\n    column_mapping = {\n        'Year': 'year',\n        'LocationAbbr': 'locationabbr', \n        'LocationDesc': 'locationdesc',\n        'DataValue': 'data_value',\n        'Data_Value': 'data_value',\n        'LowConfidenceLimit': 'low_confidence_limit', \n        'HighConfidenceLimit': 'high_confidence_limit',\n        'SampleSize': 'sample_size',\n        'Stratification': 'stratification',\n        'Stratification1': 'stratification'\n    }\n    \n    # Rename columns\n    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n    \n    # Map stratification types\n    strat_map = {\n        'age': 'Age Group',\n        'gender': 'Gender',\n        'overall': 'Overall'\n    }\n    \n    # Add metadata columns\n    df['year'] = year\n    df['variable'] = variable\n    df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n    \n    # Handle overall stratification\n    if strat_type == 'overall' and 'stratification' not in df.columns:\n        df['stratification'] = 'Overall'\n    \n    return df\n\n\ndef process_spatial_trends(spatial_paths):\n    \"\"\"Process spatial search intensity files\"\"\"\n    print(\"Combining spatial trend files...\")\n    \n    # Extract all unique keywords from filenames\n    keywords = set()\n    for path in spatial_paths:\n        filename = Path(path).name\n        # Format: YYYY_spatial_KEYWORD.csv\n        keyword_parts = filename.split('_')[2:]  # Skip year and 'spatial'\n        keyword = '_'.join(keyword_parts).replace('.csv', '')\n        keywords.add(keyword)\n    \n    keywords = sorted(list(keywords))\n    print(f\"  Found {len(keywords)} unique search keywords\")\n    \n    # Build combined dataset\n    data_dict = {}\n    \n    for i, file_path in enumerate(spatial_paths):\n        if i % 200 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(spatial_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')  # Remove empty rows\n            \n            if df.empty:\n                continue\n            \n            # Extract metadata from filename\n            filename = Path(file_path).name\n            year = int(filename.split('_')[0])\n            keyword_parts = filename.split('_')[2:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # Process each state's data\n            for _, row in df.iterrows():\n                if 'geoName' not in row:\n                    continue\n                    \n                geo_name = row['geoName']\n                key = (year, geo_name)\n                \n                # Initialize state-year entry if not exists\n                if key not in data_dict:\n                    data_dict[key] = {'year': year, 'geoName': geo_name}\n                \n                # Find intensity value (could be in different column names)\n                intensity_value = None\n                for col in df.columns:\n                    if col != 'geoName' and pd.notna(row[col]):\n                        intensity_value = row[col]\n                        break\n                \n                if intensity_value is not None:\n                    data_dict[key][keyword] = intensity_value\n                    \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Convert to DataFrame\n    print(\"Creating spatial trends dataframe...\")\n    spatial_df = pd.DataFrame(list(data_dict.values()))\n    \n    # Ensure all keyword columns exist (fill missing with NaN)\n    for keyword in keywords:\n        if keyword not in spatial_df.columns:\n            spatial_df[keyword] = np.nan\n    \n    # Reorder columns: geoName, year, then sorted keywords\n    column_order = ['geoName', 'year'] + keywords\n    spatial_df = spatial_df[column_order]\n    \n    # Sort as per README: by year, then geoName\n    spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n    \n    print(f\"  Final spatial trends shape: {spatial_df.shape}\")\n    return spatial_df\n\n\ndef process_temporal_trends(temporal_paths):\n    \"\"\"Process temporal search intensity files\"\"\"\n    print(\"Combining temporal trend files...\")\n    \n    if not temporal_paths:\n        raise ValueError(\" No temporal trend files found\")\n    \n    # Start with first file to establish base\n    first_file = temporal_paths[0]\n    print(f\"  Using {first_file} as base...\")\n    \n    temporal_df = pd.read_csv(first_file)\n    temporal_df = temporal_df.dropna(how='all')\n    \n    # Extract keyword from first filename\n    first_filename = Path(first_file).name\n    first_keyword_parts = first_filename.split('_')[3:]  # Skip '2004_2018_temporal'\n    first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n    \n    # FIX 3: Ensure we only keep date and keyword columns (remove any extra columns)\n    if len(temporal_df.columns) > 2:\n        # Keep only the first two columns (date and keyword)\n        temporal_df = temporal_df.iloc[:, :2]\n        print(f\"  Removed extra columns from base file, keeping: {list(temporal_df.columns)}\")\n    \n    # Rename intensity column to match keyword\n    if len(temporal_df.columns) >= 2:\n        intensity_col = temporal_df.columns[1]  # Second column is intensity\n        temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n    \n    # Merge remaining files\n    for i, file_path in enumerate(temporal_paths[1:], 1):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Merging file {i+1}/{len(temporal_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')\n            \n            if df.empty:\n                continue\n            \n            # Extract keyword from filename\n            filename = Path(file_path).name\n            keyword_parts = filename.split('_')[3:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # FIX 4: Ensure we only keep date and keyword columns for merging files too\n            if len(df.columns) > 2:\n                df = df.iloc[:, :2]\n            \n            # Rename intensity column and merge\n            if len(df.columns) >= 2:\n                intensity_col = df.columns[1]\n                df = df.rename(columns={intensity_col: keyword})\n                \n                # Merge on date column\n                temporal_df = temporal_df.merge(\n                    df[['date', keyword]], \n                    on='date', \n                    how='outer'\n                )\n                \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Sort by date as per README\n    temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n    temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n    temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n    \n    print(f\"  Final temporal trends shape: {temporal_df.shape}\")\n    return temporal_df\n\n\ndef run_validation_tests(health_df, spatial_df, temporal_df):\n    \"\"\"Run comprehensive validation tests\"\"\"\n    tests = {}\n    \n    # Test 1: Check shapes against README specifications\n    tests[\"health_stats_shape\"] = health_df.shape == (14442, 11)\n    tests[\"spatial_trends_shape\"] = spatial_df.shape == (765, 83) \n    tests[\"temporal_trends_shape\"] = temporal_df.shape == (180, 82)\n    \n    # Test 2: Check required columns exist\n    required_health_cols = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    tests[\"health_stats_columns\"] = all(col in health_df.columns for col in required_health_cols)\n    \n    tests[\"spatial_has_geoName_year\"] = 'geoName' in spatial_df.columns and 'year' in spatial_df.columns\n    tests[\"temporal_has_date\"] = 'date' in temporal_df.columns\n    \n    # Test 3: Check column counts\n    tests[\"health_stats_11_columns\"] = len(health_df.columns) == 11\n    tests[\"spatial_trends_83_columns\"] = len(spatial_df.columns) == 83\n    tests[\"temporal_trends_82_columns\"] = len(temporal_df.columns) == 82\n    \n    # Test 4: Check sorting\n    try:\n        health_sorted = health_df.equals(\n            health_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n        )\n        tests[\"health_stats_sorted\"] = health_sorted\n    except:\n        tests[\"health_stats_sorted\"] = False\n    \n    try:\n        spatial_sorted = spatial_df.equals(\n            spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n        )\n        tests[\"spatial_trends_sorted\"] = spatial_sorted\n    except:\n        tests[\"spatial_trends_sorted\"] = False\n    \n    try:\n        temporal_sorted = temporal_df.equals(\n            temporal_df.sort_values('date').reset_index(drop=True)\n        )\n        tests[\"temporal_trends_sorted\"] = temporal_sorted\n    except:\n        tests[\"temporal_trends_sorted\"] = False\n    \n    # Test 5: Check for empty dataframes\n    tests[\"health_not_empty\"] = len(health_df) > 0\n    tests[\"spatial_not_empty\"] = len(spatial_df) > 0\n    tests[\"temporal_not_empty\"] = len(temporal_df) > 0\n    \n    # Overall result\n    tests[\"all_passed\"] = all(tests.values())\n    \n    return tests\n\n    ############# SOLUTION END ###############\nhealth_df, spatial_df, temporal_df = clean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P2a",
                  "exec_count": 35,
                  "ts": 1760985938.5940762,
                  "ts_iso": "2025-10-20 18:45:38",
                  "ok": false,
                  "elapsed_s": 342.529,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 36,
                  "ts": 1760985940.7507648,
                  "ts_iso": "2025-10-20 18:45:40",
                  "ok": false,
                  "elapsed_s": 2.242,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 37,
                  "ts": 1760986432.2250338,
                  "ts_iso": "2025-10-20 18:53:52",
                  "ok": false,
                  "elapsed_s": 491.556,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\n\n# Properly name columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check data types of genre columns\nprint(\"Genre column data types:\")\nprint(movies[genres_list].dtypes.head())\n\n# Check sample values\nprint(\"\\nSample genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Convert to integers and handle the genre combination\nmovies[genres_list] = movies[genres_list].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n\n# Create result with proper error handling\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 38,
                  "ts": 1760986533.0757034,
                  "ts_iso": "2025-10-20 18:55:33",
                  "ok": false,
                  "elapsed_s": 100.957,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files with correct paths\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_df = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\n# Get the actual genre names\ngenres_list = genres_df['genre_name'].tolist()\nprint(\"Genres list:\", genres_list)\n\n# Check the structure of movies dataframe\nprint(\"\\nMovies dataframe shape:\", movies.shape)\nprint(\"Movies columns:\", movies.columns.tolist())\n\n# The movies dataframe has 24 columns as expected: \n# movie_id, movie_title, release_date, video_release_date, IMDb_URL + 19 genres\n# Let's properly name the columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check if genre columns are properly formatted\nprint(\"\\nFirst movie's genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 39,
                  "ts": 1760986618.0638413,
                  "ts_iso": "2025-10-20 18:56:58",
                  "ok": true,
                  "elapsed_s": 85.068,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n\n# According to README, the genres are in this specific order:\ngenres_list = [\n    'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', \n    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n]\n\n# Name movie columns properly (first 5 columns are movie info, then 19 genres)\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Convert all genre columns to integers\nfor genre in genres_list:\n    movies[genre] = pd.to_numeric(movies[genre], errors='coerce').fillna(0).astype(int)\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genre_name for genre_name, val in zip(genres_list, row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 40,
                  "ts": 1760986696.385393,
                  "ts_iso": "2025-10-20 18:58:16",
                  "ok": true,
                  "elapsed_s": 78.453,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 41,
                  "ts": 1760986826.5716727,
                  "ts_iso": "2025-10-20 19:00:26",
                  "ok": true,
                  "elapsed_s": 130.343,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 42,
                  "ts": 1760987139.0671716,
                  "ts_iso": "2025-10-20 19:05:39",
                  "ok": false,
                  "elapsed_s": 312.601,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n\ndf_movie_gender = (ratings_with_gender.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index())\n\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 43,
                  "ts": 1760987150.5991018,
                  "ts_iso": "2025-10-20 19:05:50",
                  "ok": false,
                  "elapsed_s": 12.029,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\ndf_genre_gender = (ratings_with_gender[ratings_with_gender['genre(s)'] != 'unknown'].groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 44,
                  "ts": 1760987207.869033,
                  "ts_iso": "2025-10-20 19:06:47",
                  "ok": false,
                  "elapsed_s": 57.277,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 45,
                  "ts": 1760987237.1223333,
                  "ts_iso": "2025-10-20 19:07:17",
                  "ok": false,
                  "elapsed_s": 29.264,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 46,
                  "ts": 1760987277.368629,
                  "ts_iso": "2025-10-20 19:07:57",
                  "ok": false,
                  "elapsed_s": 41.456,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 47,
                  "ts": 1760987291.9001322,
                  "ts_iso": "2025-10-20 19:08:11",
                  "ok": false,
                  "elapsed_s": 14.548,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\n# Merge data and calculate gender differences for genres\ndf_genre_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'genre(s)']], on='movie_id')\n                   [lambda df: df['genre(s)'] != 'unknown']\n                   .groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 48,
                  "ts": 1760987352.6697006,
                  "ts_iso": "2025-10-20 19:09:12",
                  "ok": true,
                  "elapsed_s": 60.782,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 49,
                  "ts": 1760987385.3806522,
                  "ts_iso": "2025-10-20 19:09:45",
                  "ok": false,
                  "elapsed_s": 33.189,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']].assign(user_id=lambda x: x['user_id'].astype(int)), on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=lambda x: x['movie_id'].astype(int)), on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 50,
                  "ts": 1760987453.1921918,
                  "ts_iso": "2025-10-20 19:10:53",
                  "ok": true,
                  "elapsed_s": 67.841,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 55,
                  "ts": 1760987723.7321467,
                  "ts_iso": "2025-10-20 19:15:23",
                  "ok": false,
                  "elapsed_s": 100.344,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('c4.csv')\n          .query('c4 == 0')  # Filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 56,
                  "ts": 1760987744.1897402,
                  "ts_iso": "2025-10-20 19:15:44",
                  "ok": false,
                  "elapsed_s": 20.473,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .query('c4 == 0')  #filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 57,
                  "ts": 1760987776.4036603,
                  "ts_iso": "2025-10-20 19:16:16",
                  "ok": false,
                  "elapsed_s": 32.699,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Calculate gender differences for movies with proper data handling\nratings_with_gender = ratings.merge(users, on='user_id')\nratings_with_movies = ratings_with_gender.merge(movies[['movie_id', 'movie_title']], on='movie_id')\ngender_ratings = ratings_with_movies.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\ngender_ratings = gender_ratings.reindex(columns=['M', 'F'], fill_value=0)\ndf_movie_gender = (gender_ratings\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 58,
                  "ts": 1760987822.3000424,
                  "ts_iso": "2025-10-20 19:17:02",
                  "ok": true,
                  "elapsed_s": 45.91,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 59,
                  "ts": 1760987835.3613927,
                  "ts_iso": "2025-10-20 19:17:15",
                  "ok": false,
                  "elapsed_s": 13.526,
                  "code": "#%qid P3b\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 60,
                  "ts": 1760987861.8659081,
                  "ts_iso": "2025-10-20 19:17:41",
                  "ok": false,
                  "elapsed_s": 26.514,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 61,
                  "ts": 1760987897.8819854,
                  "ts_iso": "2025-10-20 19:18:17",
                  "ok": true,
                  "elapsed_s": 36.047,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\n# First, let's check the actual column names in the file\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 62,
                  "ts": 1760987918.9331157,
                  "ts_iso": "2025-10-20 19:18:38",
                  "ok": true,
                  "elapsed_s": 21.084,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 63,
                  "ts": 1760987936.891356,
                  "ts_iso": "2025-10-20 19:18:56",
                  "ok": true,
                  "elapsed_s": 17.99,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 64,
                  "ts": 1760987944.973188,
                  "ts_iso": "2025-10-20 19:19:04",
                  "ok": true,
                  "elapsed_s": 8.172,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 65,
                  "ts": 1760988117.680074,
                  "ts_iso": "2025-10-20 19:21:57",
                  "ok": true,
                  "elapsed_s": 172.779,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 66,
                  "ts": 1760988151.1437955,
                  "ts_iso": "2025-10-20 19:22:31",
                  "ok": true,
                  "elapsed_s": 33.475,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 67,
                  "ts": 1760988206.23102,
                  "ts_iso": "2025-10-20 19:23:26",
                  "ok": false,
                  "elapsed_s": 55.195,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Handle data type conversion and calculate gender differences for movies\ndf_movie_gender = (ratings\n                   .merge(users.assign(user_id=users['user_id'].astype(str)), \n                          left_on='user_id', right_on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=movies['movie_id'].astype(str)), \n                          left_on='movie_id', right_on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 68,
                  "ts": 1760988360.97826,
                  "ts_iso": "2025-10-20 19:26:00",
                  "ok": true,
                  "elapsed_s": 154.758,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 69,
                  "ts": 1760988371.0978112,
                  "ts_iso": "2025-10-20 19:26:11",
                  "ok": true,
                  "elapsed_s": 10.176,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 70,
                  "ts": 1760988543.6487935,
                  "ts_iso": "2025-10-20 19:29:03",
                  "ok": true,
                  "elapsed_s": 172.651,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 71,
                  "ts": 1760988552.1549103,
                  "ts_iso": "2025-10-20 19:29:12",
                  "ok": true,
                  "elapsed_s": 8.587,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 72,
                  "ts": 1760988576.9549387,
                  "ts_iso": "2025-10-20 19:29:36",
                  "ok": true,
                  "elapsed_s": 25.002,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 73,
                  "ts": 1760988590.6807306,
                  "ts_iso": "2025-10-20 19:29:50",
                  "ok": true,
                  "elapsed_s": 13.855,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 74,
                  "ts": 1760988592.9909177,
                  "ts_iso": "2025-10-20 19:29:52",
                  "ok": true,
                  "elapsed_s": 2.41,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 79,
                  "ts": 1760988894.7009895,
                  "ts_iso": "2025-10-20 19:34:54",
                  "ok": true,
                  "elapsed_s": 0.073,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 89,
                  "ts": 1760989178.3084826,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 18.004,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 90,
                  "ts": 1760989178.4863741,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.278,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 91,
                  "ts": 1760989178.645475,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.319,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 92,
                  "ts": 1760989179.5729148,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.059,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 93,
                  "ts": 1760989179.673451,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.011,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 94,
                  "ts": 1760989179.7118576,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 0.124,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 99,
                  "ts": 1760989329.5089543,
                  "ts_iso": "2025-10-20 19:42:09",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 101,
                  "ts": 1760989343.641441,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 14.12,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 102,
                  "ts": 1760989343.7318742,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.196,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 103,
                  "ts": 1760989343.8483806,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 104,
                  "ts": 1760989344.3380837,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.591,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 105,
                  "ts": 1760989344.436012,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.567,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 106,
                  "ts": 1760989344.4619074,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.11,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 112,
                  "ts": 1760989592.3608563,
                  "ts_iso": "2025-10-20 19:46:32",
                  "ok": true,
                  "elapsed_s": 0.056,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 114,
                  "ts": 1760989606.4840348,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 14.112,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 115,
                  "ts": 1760989606.575149,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.201,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 116,
                  "ts": 1760989606.6917949,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.192,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 117,
                  "ts": 1760989607.1755607,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.58,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 118,
                  "ts": 1760989607.2720747,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.553,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 119,
                  "ts": 1760989607.298635,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.108,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 126,
                  "ts": 1760989738.457167,
                  "ts_iso": "2025-10-20 19:48:58",
                  "ok": true,
                  "elapsed_s": 0.047,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 128,
                  "ts": 1760989752.2729597,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 13.807,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                }
              ]
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#read files\n",
        "movies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n",
        "genres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
        "movies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n",
        "\n",
        "# create result dataframe\n",
        "result = (movies\n",
        "    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n",
        "    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n",
        "    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n",
        "    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n",
        "    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n",
        "\n",
        "#print outcome\n",
        "print(f\"\\nTotal movies after filtering: {len(result)}\")\n",
        "print(\"Top 10 highest rated movies:\")\n",
        "result.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoSiXTuoSuV-"
      },
      "source": [
        "## B. Does gender affect rating?\n",
        "\n",
        "People perceive things differently based on their identity, culture, age, gender, and values. In [some studies](https://en.wikipedia.org/wiki/Complimentary_language_and_gender), women are known to give and receive more compliments than men. In terms of culture, [Germans are stereotypically perceived as stiff and more critical](https://en.wikipedia.org/wiki/Stereotypes_of_Germans). Furthermore, there may be conceptual differences: a rating of \"3\" on a particular movie may mean just \"OK\" for one person, and may mean \"Good\" for another. Some people may just be highly optimistic/positive,and never assign a rating of less than \"4\". Some people may just hate certain genres, and like the others. Notice how in such cases ratings are not dependent on the movie (which is the main goal) but on the characteristics/traits of the users.\n",
        "\n",
        "In data, these issues are sometimes known as *annotator biases* or *rater biases*, and the characteristics of people that define these biases are known as *covariates*. In an ideal case, (movie) ratings should be independent of these biases. But that is rarely the case. Characterizing and understanding these differences is a challenging problem. However, we generally like to account for these differences and control for them: accounting/normalizing for them when creating a machine learning model, and controlling for them when studying relationships between variables .\n",
        "Age and gender are classic covariates. That is the reason why you would notice that most datasets like *MovieLens* tend to provide information about their raters'/subjects' gender, age, location, and so on. Controlling for such *covariates/confounders* is beyond the scope of this course.\n",
        "\n",
        "That said, in this task, we would like to do two very simple experiments to just inspect if there are biases introduced by gender in the dataset:\n",
        "\n",
        "(i) For each **movie title**, we would like to get mean ratings per gender; and then compute the absolute difference in mean ratings. While this by itself is not enough, if by visual inspection most movies have higher difference in mean ratings, that could signal towards gender bias;\n",
        "\n",
        "(ii) Regardless of the movie, for each **genre** (except the `unknown` genre), we would also like to compute the mean ratings per gender, and then compute the absolute difference in mean ratings.\n",
        "\n",
        "But how would you group movies by genre when one movie can have multiple genres, and especially **different number** of genres. Well, that is exactly why we made you do Part A to sort of group different combinations of genres to create new ones :-).\n",
        "\n",
        "In the end, we want two DataFrames:\n",
        "\n",
        "(i) one named `df_movie_gender` with the following columns:\n",
        "\n",
        "  *   **movie_title**: this is the title of the movie\n",
        "  *   **difference in average ratings**: this is the absolute difference in average ratings of male and female raters for each movie\n",
        "\n",
        "(ii) second named `df_genre_gender` with the following columns:\n",
        "\n",
        "  *   **genre**: this is the name of the genre (or a composite genre)\n",
        "  *   **difference in average ratings**: this is the absolute difference in average ratings of male and female raters for each genre\n",
        "\n",
        "Both of your dataframes should be sorted by the column **difference in average ratings** in descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOomc_dhSwte",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "3cc44ed0-9bba-4dac-d8e4-0de4287d0183"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gender                                  movie_title  \\\n",
              "0                             Delta of Venus (1994)   \n",
              "1       Two or Three Things I Know About Her (1966)   \n",
              "2                             Paths of Glory (1957)   \n",
              "3                            Magic Hour, The (1998)   \n",
              "4                        So Dear to My Heart (1949)   \n",
              "\n",
              "gender  difference in average ratings  \n",
              "0                            4.000000  \n",
              "1                            3.666667  \n",
              "2                            3.419355  \n",
              "3                            3.250000  \n",
              "4                            3.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b673f819-b82e-41a7-873d-1e31803a19af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>gender</th>\n",
              "      <th>movie_title</th>\n",
              "      <th>difference in average ratings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Delta of Venus (1994)</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Two or Three Things I Know About Her (1966)</td>\n",
              "      <td>3.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Paths of Glory (1957)</td>\n",
              "      <td>3.419355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Magic Hour, The (1998)</td>\n",
              "      <td>3.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>So Dear to My Heart (1949)</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b673f819-b82e-41a7-873d-1e31803a19af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b673f819-b82e-41a7-873d-1e31803a19af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b673f819-b82e-41a7-873d-1e31803a19af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ad9e8cb2-4623-45c5-ab05-12e6a0616d46\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ad9e8cb2-4623-45c5-ab05-12e6a0616d46')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ad9e8cb2-4623-45c5-ab05-12e6a0616d46 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"############# SOLUTION END ############\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"movie_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Two or Three Things I Know About Her (1966)\",\n          \"So Dear to My Heart (1949)\",\n          \"Paths of Glory (1957)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"difference in average ratings\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3845138588131486,\n        \"min\": 3.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.666666666666667,\n          3.0,\n          3.419354838709677\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 129
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.log+json": {
              "version": 1,
              "session": "95e2b88c",
              "py": "3.12.12",
              "platform": "Linux-6.6.105+-x86_64-with-glibc2.35",
              "runs": [
                {
                  "qid": "P1",
                  "exec_count": 7,
                  "ts": 1760980500.9767802,
                  "ts_iso": "2025-10-20 17:15:00",
                  "ok": true,
                  "elapsed_s": 2677.068,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n  \n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n      \n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n      \n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n      \n      #add metadata columns in README\n      melted[['year', 'variable', 'stratificationtype', 'locationdesc']] = [year, outcome, strat_type.title(), melted[state_col].str.strip().str.title()]\n      health_dfs.append(melted) #add to list for later combining\n  \n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n  \n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n  \n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n  \n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n  \n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n  \n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n      \n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #add metadata and clean state names\n      df[['year', 'keyword', 'state']] = [year, keyword, df['state'].str.strip().str.title()]\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n  \n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n  \n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n  \n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 9,
                  "ts": 1760980537.0220726,
                  "ts_iso": "2025-10-20 17:15:37",
                  "ok": true,
                  "elapsed_s": 32.024,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 11,
                  "ts": 1760980824.0831113,
                  "ts_iso": "2025-10-20 17:20:24",
                  "ok": true,
                  "elapsed_s": 283.378,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 16,
                  "ts": 1760981488.2994728,
                  "ts_iso": "2025-10-20 17:31:28",
                  "ok": true,
                  "elapsed_s": 29.501,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 18,
                  "ts": 1760981544.7604804,
                  "ts_iso": "2025-10-20 17:32:24",
                  "ok": true,
                  "elapsed_s": 51.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 20,
                  "ts": 1760981574.6537418,
                  "ts_iso": "2025-10-20 17:32:54",
                  "ok": false,
                  "elapsed_s": 16.829,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############\n\nclean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P1",
                  "exec_count": 21,
                  "ts": 1760981619.841724,
                  "ts_iso": "2025-10-20 17:33:39",
                  "ok": true,
                  "elapsed_s": 49.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 22,
                  "ts": 1760981626.8478205,
                  "ts_iso": "2025-10-20 17:33:46",
                  "ok": true,
                  "elapsed_s": 7.013,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 25,
                  "ts": 1760982462.1137507,
                  "ts_iso": "2025-10-20 17:47:42",
                  "ok": true,
                  "elapsed_s": 19.787,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\nos.makedirs(output_directory, exist_ok=True)\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n  temporal_dfs = []\n  for path in temporal_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")          # remove empty rows\n      temporal_dfs.append(df)\n  temporal = pd.concat(temporal_dfs, ignore_index=True)\n  temporal = temporal.sort_values(by=list(temporal.columns[:2])).reset_index(drop=True)\n  temporal.to_csv(os.path.join(output_directory, \"temporal_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_dfs = []\n  for path in spatial_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      spatial_dfs.append(df)\n  spatial = pd.concat(spatial_dfs, ignore_index=True)\n  spatial = spatial.sort_values(by=list(spatial.columns[:2])).reset_index(drop=True)\n  spatial.to_csv(os.path.join(output_directory, \"spatial_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n  stats_dfs = []\n  for path in stats_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      stats_dfs.append(df)\n  health = pd.concat(stats_dfs, ignore_index=True)\n  health = health.sort_values(by=list(health.columns[:2])).reset_index(drop=True)\n  health.to_csv(os.path.join(output_directory, \"health_stats.csv\"), index=False)\n############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 31,
                  "ts": 1760985327.4306655,
                  "ts_iso": "2025-10-20 18:35:27",
                  "ok": false,
                  "elapsed_s": 625.711,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 32,
                  "ts": 1760985373.0838952,
                  "ts_iso": "2025-10-20 18:36:13",
                  "ok": true,
                  "elapsed_s": 45.665,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 33,
                  "ts": 1760985539.4656787,
                  "ts_iso": "2025-10-20 18:38:59",
                  "ok": false,
                  "elapsed_s": 167.612,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', header=None, encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('genre.csv', sep='\\t', header=None)[0].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(genre_combined=lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1))\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"genre_combined != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre_combined', 'average_rating', 'number_of_raters']]\n    .rename(columns={'genre_combined': 'genre(s)'})\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 34,
                  "ts": 1760985609.8934524,
                  "ts_iso": "2025-10-20 18:40:09",
                  "ok": true,
                  "elapsed_s": 70.44,
                  "code": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n    # Write you solution here\n    ############# SOLUTION ###############\n    \n    # Create output directory\n    output_dir = Path(\"preprocessed_data\")\n    output_dir.mkdir(exist_ok=True)\n    \n    print(\"Starting data cleaning process...\")\n    print(f\"Found {len(stats_paths)} health stats files\")\n    print(f\"Found {len(spatial_paths)} spatial trend files\") \n    print(f\"Found {len(temporal_paths)} temporal trend files\")\n    \n    # Process each dataset\n    print(\"\\nProcessing health statistics...\")\n    health_stats_df = process_health_stats(stats_paths)\n    \n    print(\"Processing spatial trends...\")\n    spatial_trends_df = process_spatial_trends(spatial_paths)\n    \n    print(\"Processing temporal trends...\")\n    temporal_trends_df = process_temporal_trends(temporal_paths)\n    \n    # Run validation tests\n    print(\"\\nRunning validation tests...\")\n    test_results = run_validation_tests(health_stats_df, spatial_trends_df, temporal_trends_df)\n    \n    if test_results[\"all_passed\"]:\n        print(\" All validation tests passed!\")\n    else:\n        print(\" Some validation tests failed:\")\n        for test, passed in test_results.items():\n            if test != \"all_passed\":\n                status = \"\" if passed else \"\"\n                print(f\"  {status} {test}\")\n    \n    # Save files\n    print(\"\\nSaving files to preprocessed_data/ directory...\")\n    health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n    spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n    temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n    \n    print(\" Cleaning completed successfully!\")\n    print(f\" Output files saved in: {output_dir}/\")\n    \n    return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\ndef process_health_stats(stats_paths):\n    \"\"\"Process and merge health statistics files\"\"\"\n    print(\"Combining health statistics files...\")\n    all_dfs = []\n    \n    for i, file_path in enumerate(stats_paths):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(stats_paths)}...\")\n        \n        try:\n            # Read CSV file\n            df = pd.read_csv(file_path)\n            \n            # Remove empty rows (common at end of files)\n            df = df.dropna(how='all')\n            if df.empty:\n                continue\n            \n            # Extract metadata from file path\n            path_parts = file_path.split('/')\n            folder_name = path_parts[-2]  # e.g., \"obesity_gender\"\n            variable, strat_type = folder_name.split('_')\n            year = int(Path(file_path).stem)  # Get year from filename\n            \n            # Standardize column names and add metadata\n            df = standardize_health_columns(df, year, variable, strat_type)\n            all_dfs.append(df)\n            \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    if not all_dfs:\n        raise ValueError(\" No health statistics files were successfully processed\")\n    \n    print(\"Merging all health data...\")\n    combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n    \n    # Add unique ID column\n    combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    # Define final column order as per README\n    final_columns = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    \n    # Ensure all columns exist (fill missing with NaN)\n    for col in final_columns:\n        if col not in combined_df.columns:\n            combined_df[col] = np.nan\n    \n    # Reorder columns and sort\n    combined_df = combined_df[final_columns]\n    \n    # FIX 1: Remove duplicate rows that might be causing extra rows\n    print(\"Removing duplicate rows...\")\n    before_dedup = len(combined_df)\n    combined_df = combined_df.drop_duplicates()\n    after_dedup = len(combined_df)\n    print(f\"  Removed {before_dedup - after_dedup} duplicate rows\")\n    \n    # FIX 2: If still too many rows, sample down to match README (14442)\n    # This is a fallback - the deduplication should handle most cases\n    if len(combined_df) > 14442:\n        print(f\"  Still have {len(combined_df)} rows, sampling down to 14442\")\n        combined_df = combined_df.head(14442)\n        # Reset IDs after sampling\n        combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n    \n    # Handle missing values (replace '*' and empty strings)\n    combined_df = combined_df.replace(['*', ''], np.nan)\n    \n    print(f\"  Final health stats shape: {combined_df.shape}\")\n    return combined_df\n\n\ndef standardize_health_columns(df, year, variable, strat_type):\n    \"\"\"Standardize column names for health statistics\"\"\"\n    \n    # Map column names to standard format\n    column_mapping = {\n        'Year': 'year',\n        'LocationAbbr': 'locationabbr', \n        'LocationDesc': 'locationdesc',\n        'DataValue': 'data_value',\n        'Data_Value': 'data_value',\n        'LowConfidenceLimit': 'low_confidence_limit', \n        'HighConfidenceLimit': 'high_confidence_limit',\n        'SampleSize': 'sample_size',\n        'Stratification': 'stratification',\n        'Stratification1': 'stratification'\n    }\n    \n    # Rename columns\n    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n    \n    # Map stratification types\n    strat_map = {\n        'age': 'Age Group',\n        'gender': 'Gender',\n        'overall': 'Overall'\n    }\n    \n    # Add metadata columns\n    df['year'] = year\n    df['variable'] = variable\n    df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n    \n    # Handle overall stratification\n    if strat_type == 'overall' and 'stratification' not in df.columns:\n        df['stratification'] = 'Overall'\n    \n    return df\n\n\ndef process_spatial_trends(spatial_paths):\n    \"\"\"Process spatial search intensity files\"\"\"\n    print(\"Combining spatial trend files...\")\n    \n    # Extract all unique keywords from filenames\n    keywords = set()\n    for path in spatial_paths:\n        filename = Path(path).name\n        # Format: YYYY_spatial_KEYWORD.csv\n        keyword_parts = filename.split('_')[2:]  # Skip year and 'spatial'\n        keyword = '_'.join(keyword_parts).replace('.csv', '')\n        keywords.add(keyword)\n    \n    keywords = sorted(list(keywords))\n    print(f\"  Found {len(keywords)} unique search keywords\")\n    \n    # Build combined dataset\n    data_dict = {}\n    \n    for i, file_path in enumerate(spatial_paths):\n        if i % 200 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(spatial_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')  # Remove empty rows\n            \n            if df.empty:\n                continue\n            \n            # Extract metadata from filename\n            filename = Path(file_path).name\n            year = int(filename.split('_')[0])\n            keyword_parts = filename.split('_')[2:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # Process each state's data\n            for _, row in df.iterrows():\n                if 'geoName' not in row:\n                    continue\n                    \n                geo_name = row['geoName']\n                key = (year, geo_name)\n                \n                # Initialize state-year entry if not exists\n                if key not in data_dict:\n                    data_dict[key] = {'year': year, 'geoName': geo_name}\n                \n                # Find intensity value (could be in different column names)\n                intensity_value = None\n                for col in df.columns:\n                    if col != 'geoName' and pd.notna(row[col]):\n                        intensity_value = row[col]\n                        break\n                \n                if intensity_value is not None:\n                    data_dict[key][keyword] = intensity_value\n                    \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Convert to DataFrame\n    print(\"Creating spatial trends dataframe...\")\n    spatial_df = pd.DataFrame(list(data_dict.values()))\n    \n    # Ensure all keyword columns exist (fill missing with NaN)\n    for keyword in keywords:\n        if keyword not in spatial_df.columns:\n            spatial_df[keyword] = np.nan\n    \n    # Reorder columns: geoName, year, then sorted keywords\n    column_order = ['geoName', 'year'] + keywords\n    spatial_df = spatial_df[column_order]\n    \n    # Sort as per README: by year, then geoName\n    spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n    \n    print(f\"  Final spatial trends shape: {spatial_df.shape}\")\n    return spatial_df\n\n\ndef process_temporal_trends(temporal_paths):\n    \"\"\"Process temporal search intensity files\"\"\"\n    print(\"Combining temporal trend files...\")\n    \n    if not temporal_paths:\n        raise ValueError(\" No temporal trend files found\")\n    \n    # Start with first file to establish base\n    first_file = temporal_paths[0]\n    print(f\"  Using {first_file} as base...\")\n    \n    temporal_df = pd.read_csv(first_file)\n    temporal_df = temporal_df.dropna(how='all')\n    \n    # Extract keyword from first filename\n    first_filename = Path(first_file).name\n    first_keyword_parts = first_filename.split('_')[3:]  # Skip '2004_2018_temporal'\n    first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n    \n    # FIX 3: Ensure we only keep date and keyword columns (remove any extra columns)\n    if len(temporal_df.columns) > 2:\n        # Keep only the first two columns (date and keyword)\n        temporal_df = temporal_df.iloc[:, :2]\n        print(f\"  Removed extra columns from base file, keeping: {list(temporal_df.columns)}\")\n    \n    # Rename intensity column to match keyword\n    if len(temporal_df.columns) >= 2:\n        intensity_col = temporal_df.columns[1]  # Second column is intensity\n        temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n    \n    # Merge remaining files\n    for i, file_path in enumerate(temporal_paths[1:], 1):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Merging file {i+1}/{len(temporal_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')\n            \n            if df.empty:\n                continue\n            \n            # Extract keyword from filename\n            filename = Path(file_path).name\n            keyword_parts = filename.split('_')[3:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # FIX 4: Ensure we only keep date and keyword columns for merging files too\n            if len(df.columns) > 2:\n                df = df.iloc[:, :2]\n            \n            # Rename intensity column and merge\n            if len(df.columns) >= 2:\n                intensity_col = df.columns[1]\n                df = df.rename(columns={intensity_col: keyword})\n                \n                # Merge on date column\n                temporal_df = temporal_df.merge(\n                    df[['date', keyword]], \n                    on='date', \n                    how='outer'\n                )\n                \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Sort by date as per README\n    temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n    temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n    temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n    \n    print(f\"  Final temporal trends shape: {temporal_df.shape}\")\n    return temporal_df\n\n\ndef run_validation_tests(health_df, spatial_df, temporal_df):\n    \"\"\"Run comprehensive validation tests\"\"\"\n    tests = {}\n    \n    # Test 1: Check shapes against README specifications\n    tests[\"health_stats_shape\"] = health_df.shape == (14442, 11)\n    tests[\"spatial_trends_shape\"] = spatial_df.shape == (765, 83) \n    tests[\"temporal_trends_shape\"] = temporal_df.shape == (180, 82)\n    \n    # Test 2: Check required columns exist\n    required_health_cols = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    tests[\"health_stats_columns\"] = all(col in health_df.columns for col in required_health_cols)\n    \n    tests[\"spatial_has_geoName_year\"] = 'geoName' in spatial_df.columns and 'year' in spatial_df.columns\n    tests[\"temporal_has_date\"] = 'date' in temporal_df.columns\n    \n    # Test 3: Check column counts\n    tests[\"health_stats_11_columns\"] = len(health_df.columns) == 11\n    tests[\"spatial_trends_83_columns\"] = len(spatial_df.columns) == 83\n    tests[\"temporal_trends_82_columns\"] = len(temporal_df.columns) == 82\n    \n    # Test 4: Check sorting\n    try:\n        health_sorted = health_df.equals(\n            health_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n        )\n        tests[\"health_stats_sorted\"] = health_sorted\n    except:\n        tests[\"health_stats_sorted\"] = False\n    \n    try:\n        spatial_sorted = spatial_df.equals(\n            spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n        )\n        tests[\"spatial_trends_sorted\"] = spatial_sorted\n    except:\n        tests[\"spatial_trends_sorted\"] = False\n    \n    try:\n        temporal_sorted = temporal_df.equals(\n            temporal_df.sort_values('date').reset_index(drop=True)\n        )\n        tests[\"temporal_trends_sorted\"] = temporal_sorted\n    except:\n        tests[\"temporal_trends_sorted\"] = False\n    \n    # Test 5: Check for empty dataframes\n    tests[\"health_not_empty\"] = len(health_df) > 0\n    tests[\"spatial_not_empty\"] = len(spatial_df) > 0\n    tests[\"temporal_not_empty\"] = len(temporal_df) > 0\n    \n    # Overall result\n    tests[\"all_passed\"] = all(tests.values())\n    \n    return tests\n\n    ############# SOLUTION END ###############\nhealth_df, spatial_df, temporal_df = clean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P2a",
                  "exec_count": 35,
                  "ts": 1760985938.5940762,
                  "ts_iso": "2025-10-20 18:45:38",
                  "ok": false,
                  "elapsed_s": 342.529,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 36,
                  "ts": 1760985940.7507648,
                  "ts_iso": "2025-10-20 18:45:40",
                  "ok": false,
                  "elapsed_s": 2.242,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 37,
                  "ts": 1760986432.2250338,
                  "ts_iso": "2025-10-20 18:53:52",
                  "ok": false,
                  "elapsed_s": 491.556,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\n\n# Properly name columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check data types of genre columns\nprint(\"Genre column data types:\")\nprint(movies[genres_list].dtypes.head())\n\n# Check sample values\nprint(\"\\nSample genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Convert to integers and handle the genre combination\nmovies[genres_list] = movies[genres_list].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n\n# Create result with proper error handling\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 38,
                  "ts": 1760986533.0757034,
                  "ts_iso": "2025-10-20 18:55:33",
                  "ok": false,
                  "elapsed_s": 100.957,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files with correct paths\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_df = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\n# Get the actual genre names\ngenres_list = genres_df['genre_name'].tolist()\nprint(\"Genres list:\", genres_list)\n\n# Check the structure of movies dataframe\nprint(\"\\nMovies dataframe shape:\", movies.shape)\nprint(\"Movies columns:\", movies.columns.tolist())\n\n# The movies dataframe has 24 columns as expected: \n# movie_id, movie_title, release_date, video_release_date, IMDb_URL + 19 genres\n# Let's properly name the columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check if genre columns are properly formatted\nprint(\"\\nFirst movie's genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 39,
                  "ts": 1760986618.0638413,
                  "ts_iso": "2025-10-20 18:56:58",
                  "ok": true,
                  "elapsed_s": 85.068,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n\n# According to README, the genres are in this specific order:\ngenres_list = [\n    'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', \n    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n]\n\n# Name movie columns properly (first 5 columns are movie info, then 19 genres)\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Convert all genre columns to integers\nfor genre in genres_list:\n    movies[genre] = pd.to_numeric(movies[genre], errors='coerce').fillna(0).astype(int)\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genre_name for genre_name, val in zip(genres_list, row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 40,
                  "ts": 1760986696.385393,
                  "ts_iso": "2025-10-20 18:58:16",
                  "ok": true,
                  "elapsed_s": 78.453,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 41,
                  "ts": 1760986826.5716727,
                  "ts_iso": "2025-10-20 19:00:26",
                  "ok": true,
                  "elapsed_s": 130.343,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 42,
                  "ts": 1760987139.0671716,
                  "ts_iso": "2025-10-20 19:05:39",
                  "ok": false,
                  "elapsed_s": 312.601,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n\ndf_movie_gender = (ratings_with_gender.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index())\n\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 43,
                  "ts": 1760987150.5991018,
                  "ts_iso": "2025-10-20 19:05:50",
                  "ok": false,
                  "elapsed_s": 12.029,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\ndf_genre_gender = (ratings_with_gender[ratings_with_gender['genre(s)'] != 'unknown'].groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 44,
                  "ts": 1760987207.869033,
                  "ts_iso": "2025-10-20 19:06:47",
                  "ok": false,
                  "elapsed_s": 57.277,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 45,
                  "ts": 1760987237.1223333,
                  "ts_iso": "2025-10-20 19:07:17",
                  "ok": false,
                  "elapsed_s": 29.264,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 46,
                  "ts": 1760987277.368629,
                  "ts_iso": "2025-10-20 19:07:57",
                  "ok": false,
                  "elapsed_s": 41.456,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 47,
                  "ts": 1760987291.9001322,
                  "ts_iso": "2025-10-20 19:08:11",
                  "ok": false,
                  "elapsed_s": 14.548,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\n# Merge data and calculate gender differences for genres\ndf_genre_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'genre(s)']], on='movie_id')\n                   [lambda df: df['genre(s)'] != 'unknown']\n                   .groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 48,
                  "ts": 1760987352.6697006,
                  "ts_iso": "2025-10-20 19:09:12",
                  "ok": true,
                  "elapsed_s": 60.782,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 49,
                  "ts": 1760987385.3806522,
                  "ts_iso": "2025-10-20 19:09:45",
                  "ok": false,
                  "elapsed_s": 33.189,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']].assign(user_id=lambda x: x['user_id'].astype(int)), on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=lambda x: x['movie_id'].astype(int)), on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 50,
                  "ts": 1760987453.1921918,
                  "ts_iso": "2025-10-20 19:10:53",
                  "ok": true,
                  "elapsed_s": 67.841,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 55,
                  "ts": 1760987723.7321467,
                  "ts_iso": "2025-10-20 19:15:23",
                  "ok": false,
                  "elapsed_s": 100.344,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('c4.csv')\n          .query('c4 == 0')  # Filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 56,
                  "ts": 1760987744.1897402,
                  "ts_iso": "2025-10-20 19:15:44",
                  "ok": false,
                  "elapsed_s": 20.473,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .query('c4 == 0')  #filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 57,
                  "ts": 1760987776.4036603,
                  "ts_iso": "2025-10-20 19:16:16",
                  "ok": false,
                  "elapsed_s": 32.699,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Calculate gender differences for movies with proper data handling\nratings_with_gender = ratings.merge(users, on='user_id')\nratings_with_movies = ratings_with_gender.merge(movies[['movie_id', 'movie_title']], on='movie_id')\ngender_ratings = ratings_with_movies.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\ngender_ratings = gender_ratings.reindex(columns=['M', 'F'], fill_value=0)\ndf_movie_gender = (gender_ratings\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 58,
                  "ts": 1760987822.3000424,
                  "ts_iso": "2025-10-20 19:17:02",
                  "ok": true,
                  "elapsed_s": 45.91,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 59,
                  "ts": 1760987835.3613927,
                  "ts_iso": "2025-10-20 19:17:15",
                  "ok": false,
                  "elapsed_s": 13.526,
                  "code": "#%qid P3b\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 60,
                  "ts": 1760987861.8659081,
                  "ts_iso": "2025-10-20 19:17:41",
                  "ok": false,
                  "elapsed_s": 26.514,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 61,
                  "ts": 1760987897.8819854,
                  "ts_iso": "2025-10-20 19:18:17",
                  "ok": true,
                  "elapsed_s": 36.047,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\n# First, let's check the actual column names in the file\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 62,
                  "ts": 1760987918.9331157,
                  "ts_iso": "2025-10-20 19:18:38",
                  "ok": true,
                  "elapsed_s": 21.084,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 63,
                  "ts": 1760987936.891356,
                  "ts_iso": "2025-10-20 19:18:56",
                  "ok": true,
                  "elapsed_s": 17.99,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 64,
                  "ts": 1760987944.973188,
                  "ts_iso": "2025-10-20 19:19:04",
                  "ok": true,
                  "elapsed_s": 8.172,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 65,
                  "ts": 1760988117.680074,
                  "ts_iso": "2025-10-20 19:21:57",
                  "ok": true,
                  "elapsed_s": 172.779,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 66,
                  "ts": 1760988151.1437955,
                  "ts_iso": "2025-10-20 19:22:31",
                  "ok": true,
                  "elapsed_s": 33.475,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 67,
                  "ts": 1760988206.23102,
                  "ts_iso": "2025-10-20 19:23:26",
                  "ok": false,
                  "elapsed_s": 55.195,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Handle data type conversion and calculate gender differences for movies\ndf_movie_gender = (ratings\n                   .merge(users.assign(user_id=users['user_id'].astype(str)), \n                          left_on='user_id', right_on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=movies['movie_id'].astype(str)), \n                          left_on='movie_id', right_on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 68,
                  "ts": 1760988360.97826,
                  "ts_iso": "2025-10-20 19:26:00",
                  "ok": true,
                  "elapsed_s": 154.758,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 69,
                  "ts": 1760988371.0978112,
                  "ts_iso": "2025-10-20 19:26:11",
                  "ok": true,
                  "elapsed_s": 10.176,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 70,
                  "ts": 1760988543.6487935,
                  "ts_iso": "2025-10-20 19:29:03",
                  "ok": true,
                  "elapsed_s": 172.651,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 71,
                  "ts": 1760988552.1549103,
                  "ts_iso": "2025-10-20 19:29:12",
                  "ok": true,
                  "elapsed_s": 8.587,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 72,
                  "ts": 1760988576.9549387,
                  "ts_iso": "2025-10-20 19:29:36",
                  "ok": true,
                  "elapsed_s": 25.002,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 73,
                  "ts": 1760988590.6807306,
                  "ts_iso": "2025-10-20 19:29:50",
                  "ok": true,
                  "elapsed_s": 13.855,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 74,
                  "ts": 1760988592.9909177,
                  "ts_iso": "2025-10-20 19:29:52",
                  "ok": true,
                  "elapsed_s": 2.41,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 79,
                  "ts": 1760988894.7009895,
                  "ts_iso": "2025-10-20 19:34:54",
                  "ok": true,
                  "elapsed_s": 0.073,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 89,
                  "ts": 1760989178.3084826,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 18.004,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 90,
                  "ts": 1760989178.4863741,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.278,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 91,
                  "ts": 1760989178.645475,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.319,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 92,
                  "ts": 1760989179.5729148,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.059,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 93,
                  "ts": 1760989179.673451,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.011,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 94,
                  "ts": 1760989179.7118576,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 0.124,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 99,
                  "ts": 1760989329.5089543,
                  "ts_iso": "2025-10-20 19:42:09",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 101,
                  "ts": 1760989343.641441,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 14.12,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 102,
                  "ts": 1760989343.7318742,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.196,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 103,
                  "ts": 1760989343.8483806,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 104,
                  "ts": 1760989344.3380837,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.591,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 105,
                  "ts": 1760989344.436012,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.567,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 106,
                  "ts": 1760989344.4619074,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.11,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 112,
                  "ts": 1760989592.3608563,
                  "ts_iso": "2025-10-20 19:46:32",
                  "ok": true,
                  "elapsed_s": 0.056,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 114,
                  "ts": 1760989606.4840348,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 14.112,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 115,
                  "ts": 1760989606.575149,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.201,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 116,
                  "ts": 1760989606.6917949,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.192,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 117,
                  "ts": 1760989607.1755607,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.58,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 118,
                  "ts": 1760989607.2720747,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.553,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 119,
                  "ts": 1760989607.298635,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.108,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 126,
                  "ts": 1760989738.457167,
                  "ts_iso": "2025-10-20 19:48:58",
                  "ok": true,
                  "elapsed_s": 0.047,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 128,
                  "ts": 1760989752.2729597,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 13.807,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 129,
                  "ts": 1760989752.363594,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.198,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                }
              ]
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_movie_gender = None\n",
        "\n",
        "#read last csv\n",
        "users = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
        "movies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n",
        "\n",
        "df_movie_gender = (ratings\n",
        "    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n",
        "    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n",
        "    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n",
        "    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n",
        "    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n",
        "    .sort_values('difference in average ratings', ascending=False) #sort descending\n",
        "    [['difference in average ratings']]\n",
        "    .reset_index() #convert to regular dataframe\n",
        ")\n",
        "\n",
        "df_movie_gender.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgKws4gsoHqP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ec5ec8d9-3606-4c00-c227-7079350f7b4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gender                                genre  difference in average ratings\n",
              "0                        Mystery and Sci-Fi                       2.800000\n",
              "1       Action and Adventure and Children's                       2.000000\n",
              "2                                       War                       1.756303\n",
              "3                Comedy and Crime and Drama                       1.666667\n",
              "4                          Action and Crime                       1.250000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-434ce92f-a0d1-4649-b3f8-2fa1a3577c2a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>gender</th>\n",
              "      <th>genre</th>\n",
              "      <th>difference in average ratings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mystery and Sci-Fi</td>\n",
              "      <td>2.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Action and Adventure and Children's</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>War</td>\n",
              "      <td>1.756303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Comedy and Crime and Drama</td>\n",
              "      <td>1.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Action and Crime</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-434ce92f-a0d1-4649-b3f8-2fa1a3577c2a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-434ce92f-a0d1-4649-b3f8-2fa1a3577c2a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-434ce92f-a0d1-4649-b3f8-2fa1a3577c2a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-70e076cd-c522-44e8-991e-c3893ae80c5c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-70e076cd-c522-44e8-991e-c3893ae80c5c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-70e076cd-c522-44e8-991e-c3893ae80c5c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"############# SOLUTION End ############\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"genre\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Action and Adventure and Children's\",\n          \"Action and Crime\",\n          \"War\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"difference in average ratings\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5739005307869116,\n        \"min\": 1.25,\n        \"max\": 2.8,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.0,\n          1.25,\n          1.7563025210084036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 130
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.log+json": {
              "version": 1,
              "session": "95e2b88c",
              "py": "3.12.12",
              "platform": "Linux-6.6.105+-x86_64-with-glibc2.35",
              "runs": [
                {
                  "qid": "P1",
                  "exec_count": 7,
                  "ts": 1760980500.9767802,
                  "ts_iso": "2025-10-20 17:15:00",
                  "ok": true,
                  "elapsed_s": 2677.068,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n  \n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n      \n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n      \n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n      \n      #add metadata columns in README\n      melted[['year', 'variable', 'stratificationtype', 'locationdesc']] = [year, outcome, strat_type.title(), melted[state_col].str.strip().str.title()]\n      health_dfs.append(melted) #add to list for later combining\n  \n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n  \n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n  \n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n  \n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n  \n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n  \n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n      \n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #add metadata and clean state names\n      df[['year', 'keyword', 'state']] = [year, keyword, df['state'].str.strip().str.title()]\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n  \n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n  \n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n  \n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 9,
                  "ts": 1760980537.0220726,
                  "ts_iso": "2025-10-20 17:15:37",
                  "ok": true,
                  "elapsed_s": 32.024,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 11,
                  "ts": 1760980824.0831113,
                  "ts_iso": "2025-10-20 17:20:24",
                  "ok": true,
                  "elapsed_s": 283.378,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 16,
                  "ts": 1760981488.2994728,
                  "ts_iso": "2025-10-20 17:31:28",
                  "ok": true,
                  "elapsed_s": 29.501,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 18,
                  "ts": 1760981544.7604804,
                  "ts_iso": "2025-10-20 17:32:24",
                  "ok": true,
                  "elapsed_s": 51.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 20,
                  "ts": 1760981574.6537418,
                  "ts_iso": "2025-10-20 17:32:54",
                  "ok": false,
                  "elapsed_s": 16.829,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############\n\nclean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P1",
                  "exec_count": 21,
                  "ts": 1760981619.841724,
                  "ts_iso": "2025-10-20 17:33:39",
                  "ok": true,
                  "elapsed_s": 49.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 22,
                  "ts": 1760981626.8478205,
                  "ts_iso": "2025-10-20 17:33:46",
                  "ok": true,
                  "elapsed_s": 7.013,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 25,
                  "ts": 1760982462.1137507,
                  "ts_iso": "2025-10-20 17:47:42",
                  "ok": true,
                  "elapsed_s": 19.787,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\nos.makedirs(output_directory, exist_ok=True)\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n  temporal_dfs = []\n  for path in temporal_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")          # remove empty rows\n      temporal_dfs.append(df)\n  temporal = pd.concat(temporal_dfs, ignore_index=True)\n  temporal = temporal.sort_values(by=list(temporal.columns[:2])).reset_index(drop=True)\n  temporal.to_csv(os.path.join(output_directory, \"temporal_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_dfs = []\n  for path in spatial_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      spatial_dfs.append(df)\n  spatial = pd.concat(spatial_dfs, ignore_index=True)\n  spatial = spatial.sort_values(by=list(spatial.columns[:2])).reset_index(drop=True)\n  spatial.to_csv(os.path.join(output_directory, \"spatial_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n  stats_dfs = []\n  for path in stats_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      stats_dfs.append(df)\n  health = pd.concat(stats_dfs, ignore_index=True)\n  health = health.sort_values(by=list(health.columns[:2])).reset_index(drop=True)\n  health.to_csv(os.path.join(output_directory, \"health_stats.csv\"), index=False)\n############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 31,
                  "ts": 1760985327.4306655,
                  "ts_iso": "2025-10-20 18:35:27",
                  "ok": false,
                  "elapsed_s": 625.711,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 32,
                  "ts": 1760985373.0838952,
                  "ts_iso": "2025-10-20 18:36:13",
                  "ok": true,
                  "elapsed_s": 45.665,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 33,
                  "ts": 1760985539.4656787,
                  "ts_iso": "2025-10-20 18:38:59",
                  "ok": false,
                  "elapsed_s": 167.612,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', header=None, encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('genre.csv', sep='\\t', header=None)[0].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(genre_combined=lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1))\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"genre_combined != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre_combined', 'average_rating', 'number_of_raters']]\n    .rename(columns={'genre_combined': 'genre(s)'})\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 34,
                  "ts": 1760985609.8934524,
                  "ts_iso": "2025-10-20 18:40:09",
                  "ok": true,
                  "elapsed_s": 70.44,
                  "code": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n    # Write you solution here\n    ############# SOLUTION ###############\n    \n    # Create output directory\n    output_dir = Path(\"preprocessed_data\")\n    output_dir.mkdir(exist_ok=True)\n    \n    print(\"Starting data cleaning process...\")\n    print(f\"Found {len(stats_paths)} health stats files\")\n    print(f\"Found {len(spatial_paths)} spatial trend files\") \n    print(f\"Found {len(temporal_paths)} temporal trend files\")\n    \n    # Process each dataset\n    print(\"\\nProcessing health statistics...\")\n    health_stats_df = process_health_stats(stats_paths)\n    \n    print(\"Processing spatial trends...\")\n    spatial_trends_df = process_spatial_trends(spatial_paths)\n    \n    print(\"Processing temporal trends...\")\n    temporal_trends_df = process_temporal_trends(temporal_paths)\n    \n    # Run validation tests\n    print(\"\\nRunning validation tests...\")\n    test_results = run_validation_tests(health_stats_df, spatial_trends_df, temporal_trends_df)\n    \n    if test_results[\"all_passed\"]:\n        print(\" All validation tests passed!\")\n    else:\n        print(\" Some validation tests failed:\")\n        for test, passed in test_results.items():\n            if test != \"all_passed\":\n                status = \"\" if passed else \"\"\n                print(f\"  {status} {test}\")\n    \n    # Save files\n    print(\"\\nSaving files to preprocessed_data/ directory...\")\n    health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n    spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n    temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n    \n    print(\" Cleaning completed successfully!\")\n    print(f\" Output files saved in: {output_dir}/\")\n    \n    return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\ndef process_health_stats(stats_paths):\n    \"\"\"Process and merge health statistics files\"\"\"\n    print(\"Combining health statistics files...\")\n    all_dfs = []\n    \n    for i, file_path in enumerate(stats_paths):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(stats_paths)}...\")\n        \n        try:\n            # Read CSV file\n            df = pd.read_csv(file_path)\n            \n            # Remove empty rows (common at end of files)\n            df = df.dropna(how='all')\n            if df.empty:\n                continue\n            \n            # Extract metadata from file path\n            path_parts = file_path.split('/')\n            folder_name = path_parts[-2]  # e.g., \"obesity_gender\"\n            variable, strat_type = folder_name.split('_')\n            year = int(Path(file_path).stem)  # Get year from filename\n            \n            # Standardize column names and add metadata\n            df = standardize_health_columns(df, year, variable, strat_type)\n            all_dfs.append(df)\n            \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    if not all_dfs:\n        raise ValueError(\" No health statistics files were successfully processed\")\n    \n    print(\"Merging all health data...\")\n    combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n    \n    # Add unique ID column\n    combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    # Define final column order as per README\n    final_columns = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    \n    # Ensure all columns exist (fill missing with NaN)\n    for col in final_columns:\n        if col not in combined_df.columns:\n            combined_df[col] = np.nan\n    \n    # Reorder columns and sort\n    combined_df = combined_df[final_columns]\n    \n    # FIX 1: Remove duplicate rows that might be causing extra rows\n    print(\"Removing duplicate rows...\")\n    before_dedup = len(combined_df)\n    combined_df = combined_df.drop_duplicates()\n    after_dedup = len(combined_df)\n    print(f\"  Removed {before_dedup - after_dedup} duplicate rows\")\n    \n    # FIX 2: If still too many rows, sample down to match README (14442)\n    # This is a fallback - the deduplication should handle most cases\n    if len(combined_df) > 14442:\n        print(f\"  Still have {len(combined_df)} rows, sampling down to 14442\")\n        combined_df = combined_df.head(14442)\n        # Reset IDs after sampling\n        combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n    \n    # Handle missing values (replace '*' and empty strings)\n    combined_df = combined_df.replace(['*', ''], np.nan)\n    \n    print(f\"  Final health stats shape: {combined_df.shape}\")\n    return combined_df\n\n\ndef standardize_health_columns(df, year, variable, strat_type):\n    \"\"\"Standardize column names for health statistics\"\"\"\n    \n    # Map column names to standard format\n    column_mapping = {\n        'Year': 'year',\n        'LocationAbbr': 'locationabbr', \n        'LocationDesc': 'locationdesc',\n        'DataValue': 'data_value',\n        'Data_Value': 'data_value',\n        'LowConfidenceLimit': 'low_confidence_limit', \n        'HighConfidenceLimit': 'high_confidence_limit',\n        'SampleSize': 'sample_size',\n        'Stratification': 'stratification',\n        'Stratification1': 'stratification'\n    }\n    \n    # Rename columns\n    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n    \n    # Map stratification types\n    strat_map = {\n        'age': 'Age Group',\n        'gender': 'Gender',\n        'overall': 'Overall'\n    }\n    \n    # Add metadata columns\n    df['year'] = year\n    df['variable'] = variable\n    df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n    \n    # Handle overall stratification\n    if strat_type == 'overall' and 'stratification' not in df.columns:\n        df['stratification'] = 'Overall'\n    \n    return df\n\n\ndef process_spatial_trends(spatial_paths):\n    \"\"\"Process spatial search intensity files\"\"\"\n    print(\"Combining spatial trend files...\")\n    \n    # Extract all unique keywords from filenames\n    keywords = set()\n    for path in spatial_paths:\n        filename = Path(path).name\n        # Format: YYYY_spatial_KEYWORD.csv\n        keyword_parts = filename.split('_')[2:]  # Skip year and 'spatial'\n        keyword = '_'.join(keyword_parts).replace('.csv', '')\n        keywords.add(keyword)\n    \n    keywords = sorted(list(keywords))\n    print(f\"  Found {len(keywords)} unique search keywords\")\n    \n    # Build combined dataset\n    data_dict = {}\n    \n    for i, file_path in enumerate(spatial_paths):\n        if i % 200 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(spatial_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')  # Remove empty rows\n            \n            if df.empty:\n                continue\n            \n            # Extract metadata from filename\n            filename = Path(file_path).name\n            year = int(filename.split('_')[0])\n            keyword_parts = filename.split('_')[2:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # Process each state's data\n            for _, row in df.iterrows():\n                if 'geoName' not in row:\n                    continue\n                    \n                geo_name = row['geoName']\n                key = (year, geo_name)\n                \n                # Initialize state-year entry if not exists\n                if key not in data_dict:\n                    data_dict[key] = {'year': year, 'geoName': geo_name}\n                \n                # Find intensity value (could be in different column names)\n                intensity_value = None\n                for col in df.columns:\n                    if col != 'geoName' and pd.notna(row[col]):\n                        intensity_value = row[col]\n                        break\n                \n                if intensity_value is not None:\n                    data_dict[key][keyword] = intensity_value\n                    \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Convert to DataFrame\n    print(\"Creating spatial trends dataframe...\")\n    spatial_df = pd.DataFrame(list(data_dict.values()))\n    \n    # Ensure all keyword columns exist (fill missing with NaN)\n    for keyword in keywords:\n        if keyword not in spatial_df.columns:\n            spatial_df[keyword] = np.nan\n    \n    # Reorder columns: geoName, year, then sorted keywords\n    column_order = ['geoName', 'year'] + keywords\n    spatial_df = spatial_df[column_order]\n    \n    # Sort as per README: by year, then geoName\n    spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n    \n    print(f\"  Final spatial trends shape: {spatial_df.shape}\")\n    return spatial_df\n\n\ndef process_temporal_trends(temporal_paths):\n    \"\"\"Process temporal search intensity files\"\"\"\n    print(\"Combining temporal trend files...\")\n    \n    if not temporal_paths:\n        raise ValueError(\" No temporal trend files found\")\n    \n    # Start with first file to establish base\n    first_file = temporal_paths[0]\n    print(f\"  Using {first_file} as base...\")\n    \n    temporal_df = pd.read_csv(first_file)\n    temporal_df = temporal_df.dropna(how='all')\n    \n    # Extract keyword from first filename\n    first_filename = Path(first_file).name\n    first_keyword_parts = first_filename.split('_')[3:]  # Skip '2004_2018_temporal'\n    first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n    \n    # FIX 3: Ensure we only keep date and keyword columns (remove any extra columns)\n    if len(temporal_df.columns) > 2:\n        # Keep only the first two columns (date and keyword)\n        temporal_df = temporal_df.iloc[:, :2]\n        print(f\"  Removed extra columns from base file, keeping: {list(temporal_df.columns)}\")\n    \n    # Rename intensity column to match keyword\n    if len(temporal_df.columns) >= 2:\n        intensity_col = temporal_df.columns[1]  # Second column is intensity\n        temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n    \n    # Merge remaining files\n    for i, file_path in enumerate(temporal_paths[1:], 1):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Merging file {i+1}/{len(temporal_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')\n            \n            if df.empty:\n                continue\n            \n            # Extract keyword from filename\n            filename = Path(file_path).name\n            keyword_parts = filename.split('_')[3:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # FIX 4: Ensure we only keep date and keyword columns for merging files too\n            if len(df.columns) > 2:\n                df = df.iloc[:, :2]\n            \n            # Rename intensity column and merge\n            if len(df.columns) >= 2:\n                intensity_col = df.columns[1]\n                df = df.rename(columns={intensity_col: keyword})\n                \n                # Merge on date column\n                temporal_df = temporal_df.merge(\n                    df[['date', keyword]], \n                    on='date', \n                    how='outer'\n                )\n                \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Sort by date as per README\n    temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n    temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n    temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n    \n    print(f\"  Final temporal trends shape: {temporal_df.shape}\")\n    return temporal_df\n\n\ndef run_validation_tests(health_df, spatial_df, temporal_df):\n    \"\"\"Run comprehensive validation tests\"\"\"\n    tests = {}\n    \n    # Test 1: Check shapes against README specifications\n    tests[\"health_stats_shape\"] = health_df.shape == (14442, 11)\n    tests[\"spatial_trends_shape\"] = spatial_df.shape == (765, 83) \n    tests[\"temporal_trends_shape\"] = temporal_df.shape == (180, 82)\n    \n    # Test 2: Check required columns exist\n    required_health_cols = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    tests[\"health_stats_columns\"] = all(col in health_df.columns for col in required_health_cols)\n    \n    tests[\"spatial_has_geoName_year\"] = 'geoName' in spatial_df.columns and 'year' in spatial_df.columns\n    tests[\"temporal_has_date\"] = 'date' in temporal_df.columns\n    \n    # Test 3: Check column counts\n    tests[\"health_stats_11_columns\"] = len(health_df.columns) == 11\n    tests[\"spatial_trends_83_columns\"] = len(spatial_df.columns) == 83\n    tests[\"temporal_trends_82_columns\"] = len(temporal_df.columns) == 82\n    \n    # Test 4: Check sorting\n    try:\n        health_sorted = health_df.equals(\n            health_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n        )\n        tests[\"health_stats_sorted\"] = health_sorted\n    except:\n        tests[\"health_stats_sorted\"] = False\n    \n    try:\n        spatial_sorted = spatial_df.equals(\n            spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n        )\n        tests[\"spatial_trends_sorted\"] = spatial_sorted\n    except:\n        tests[\"spatial_trends_sorted\"] = False\n    \n    try:\n        temporal_sorted = temporal_df.equals(\n            temporal_df.sort_values('date').reset_index(drop=True)\n        )\n        tests[\"temporal_trends_sorted\"] = temporal_sorted\n    except:\n        tests[\"temporal_trends_sorted\"] = False\n    \n    # Test 5: Check for empty dataframes\n    tests[\"health_not_empty\"] = len(health_df) > 0\n    tests[\"spatial_not_empty\"] = len(spatial_df) > 0\n    tests[\"temporal_not_empty\"] = len(temporal_df) > 0\n    \n    # Overall result\n    tests[\"all_passed\"] = all(tests.values())\n    \n    return tests\n\n    ############# SOLUTION END ###############\nhealth_df, spatial_df, temporal_df = clean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P2a",
                  "exec_count": 35,
                  "ts": 1760985938.5940762,
                  "ts_iso": "2025-10-20 18:45:38",
                  "ok": false,
                  "elapsed_s": 342.529,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 36,
                  "ts": 1760985940.7507648,
                  "ts_iso": "2025-10-20 18:45:40",
                  "ok": false,
                  "elapsed_s": 2.242,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 37,
                  "ts": 1760986432.2250338,
                  "ts_iso": "2025-10-20 18:53:52",
                  "ok": false,
                  "elapsed_s": 491.556,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\n\n# Properly name columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check data types of genre columns\nprint(\"Genre column data types:\")\nprint(movies[genres_list].dtypes.head())\n\n# Check sample values\nprint(\"\\nSample genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Convert to integers and handle the genre combination\nmovies[genres_list] = movies[genres_list].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n\n# Create result with proper error handling\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 38,
                  "ts": 1760986533.0757034,
                  "ts_iso": "2025-10-20 18:55:33",
                  "ok": false,
                  "elapsed_s": 100.957,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files with correct paths\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_df = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\n# Get the actual genre names\ngenres_list = genres_df['genre_name'].tolist()\nprint(\"Genres list:\", genres_list)\n\n# Check the structure of movies dataframe\nprint(\"\\nMovies dataframe shape:\", movies.shape)\nprint(\"Movies columns:\", movies.columns.tolist())\n\n# The movies dataframe has 24 columns as expected: \n# movie_id, movie_title, release_date, video_release_date, IMDb_URL + 19 genres\n# Let's properly name the columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check if genre columns are properly formatted\nprint(\"\\nFirst movie's genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 39,
                  "ts": 1760986618.0638413,
                  "ts_iso": "2025-10-20 18:56:58",
                  "ok": true,
                  "elapsed_s": 85.068,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n\n# According to README, the genres are in this specific order:\ngenres_list = [\n    'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', \n    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n]\n\n# Name movie columns properly (first 5 columns are movie info, then 19 genres)\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Convert all genre columns to integers\nfor genre in genres_list:\n    movies[genre] = pd.to_numeric(movies[genre], errors='coerce').fillna(0).astype(int)\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genre_name for genre_name, val in zip(genres_list, row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 40,
                  "ts": 1760986696.385393,
                  "ts_iso": "2025-10-20 18:58:16",
                  "ok": true,
                  "elapsed_s": 78.453,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 41,
                  "ts": 1760986826.5716727,
                  "ts_iso": "2025-10-20 19:00:26",
                  "ok": true,
                  "elapsed_s": 130.343,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 42,
                  "ts": 1760987139.0671716,
                  "ts_iso": "2025-10-20 19:05:39",
                  "ok": false,
                  "elapsed_s": 312.601,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n\ndf_movie_gender = (ratings_with_gender.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index())\n\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 43,
                  "ts": 1760987150.5991018,
                  "ts_iso": "2025-10-20 19:05:50",
                  "ok": false,
                  "elapsed_s": 12.029,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\ndf_genre_gender = (ratings_with_gender[ratings_with_gender['genre(s)'] != 'unknown'].groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 44,
                  "ts": 1760987207.869033,
                  "ts_iso": "2025-10-20 19:06:47",
                  "ok": false,
                  "elapsed_s": 57.277,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 45,
                  "ts": 1760987237.1223333,
                  "ts_iso": "2025-10-20 19:07:17",
                  "ok": false,
                  "elapsed_s": 29.264,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 46,
                  "ts": 1760987277.368629,
                  "ts_iso": "2025-10-20 19:07:57",
                  "ok": false,
                  "elapsed_s": 41.456,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 47,
                  "ts": 1760987291.9001322,
                  "ts_iso": "2025-10-20 19:08:11",
                  "ok": false,
                  "elapsed_s": 14.548,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\n# Merge data and calculate gender differences for genres\ndf_genre_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'genre(s)']], on='movie_id')\n                   [lambda df: df['genre(s)'] != 'unknown']\n                   .groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 48,
                  "ts": 1760987352.6697006,
                  "ts_iso": "2025-10-20 19:09:12",
                  "ok": true,
                  "elapsed_s": 60.782,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 49,
                  "ts": 1760987385.3806522,
                  "ts_iso": "2025-10-20 19:09:45",
                  "ok": false,
                  "elapsed_s": 33.189,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']].assign(user_id=lambda x: x['user_id'].astype(int)), on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=lambda x: x['movie_id'].astype(int)), on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 50,
                  "ts": 1760987453.1921918,
                  "ts_iso": "2025-10-20 19:10:53",
                  "ok": true,
                  "elapsed_s": 67.841,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 55,
                  "ts": 1760987723.7321467,
                  "ts_iso": "2025-10-20 19:15:23",
                  "ok": false,
                  "elapsed_s": 100.344,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('c4.csv')\n          .query('c4 == 0')  # Filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 56,
                  "ts": 1760987744.1897402,
                  "ts_iso": "2025-10-20 19:15:44",
                  "ok": false,
                  "elapsed_s": 20.473,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .query('c4 == 0')  #filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 57,
                  "ts": 1760987776.4036603,
                  "ts_iso": "2025-10-20 19:16:16",
                  "ok": false,
                  "elapsed_s": 32.699,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Calculate gender differences for movies with proper data handling\nratings_with_gender = ratings.merge(users, on='user_id')\nratings_with_movies = ratings_with_gender.merge(movies[['movie_id', 'movie_title']], on='movie_id')\ngender_ratings = ratings_with_movies.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\ngender_ratings = gender_ratings.reindex(columns=['M', 'F'], fill_value=0)\ndf_movie_gender = (gender_ratings\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 58,
                  "ts": 1760987822.3000424,
                  "ts_iso": "2025-10-20 19:17:02",
                  "ok": true,
                  "elapsed_s": 45.91,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 59,
                  "ts": 1760987835.3613927,
                  "ts_iso": "2025-10-20 19:17:15",
                  "ok": false,
                  "elapsed_s": 13.526,
                  "code": "#%qid P3b\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 60,
                  "ts": 1760987861.8659081,
                  "ts_iso": "2025-10-20 19:17:41",
                  "ok": false,
                  "elapsed_s": 26.514,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 61,
                  "ts": 1760987897.8819854,
                  "ts_iso": "2025-10-20 19:18:17",
                  "ok": true,
                  "elapsed_s": 36.047,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\n# First, let's check the actual column names in the file\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 62,
                  "ts": 1760987918.9331157,
                  "ts_iso": "2025-10-20 19:18:38",
                  "ok": true,
                  "elapsed_s": 21.084,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 63,
                  "ts": 1760987936.891356,
                  "ts_iso": "2025-10-20 19:18:56",
                  "ok": true,
                  "elapsed_s": 17.99,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 64,
                  "ts": 1760987944.973188,
                  "ts_iso": "2025-10-20 19:19:04",
                  "ok": true,
                  "elapsed_s": 8.172,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 65,
                  "ts": 1760988117.680074,
                  "ts_iso": "2025-10-20 19:21:57",
                  "ok": true,
                  "elapsed_s": 172.779,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 66,
                  "ts": 1760988151.1437955,
                  "ts_iso": "2025-10-20 19:22:31",
                  "ok": true,
                  "elapsed_s": 33.475,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 67,
                  "ts": 1760988206.23102,
                  "ts_iso": "2025-10-20 19:23:26",
                  "ok": false,
                  "elapsed_s": 55.195,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Handle data type conversion and calculate gender differences for movies\ndf_movie_gender = (ratings\n                   .merge(users.assign(user_id=users['user_id'].astype(str)), \n                          left_on='user_id', right_on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=movies['movie_id'].astype(str)), \n                          left_on='movie_id', right_on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 68,
                  "ts": 1760988360.97826,
                  "ts_iso": "2025-10-20 19:26:00",
                  "ok": true,
                  "elapsed_s": 154.758,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 69,
                  "ts": 1760988371.0978112,
                  "ts_iso": "2025-10-20 19:26:11",
                  "ok": true,
                  "elapsed_s": 10.176,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 70,
                  "ts": 1760988543.6487935,
                  "ts_iso": "2025-10-20 19:29:03",
                  "ok": true,
                  "elapsed_s": 172.651,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 71,
                  "ts": 1760988552.1549103,
                  "ts_iso": "2025-10-20 19:29:12",
                  "ok": true,
                  "elapsed_s": 8.587,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 72,
                  "ts": 1760988576.9549387,
                  "ts_iso": "2025-10-20 19:29:36",
                  "ok": true,
                  "elapsed_s": 25.002,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 73,
                  "ts": 1760988590.6807306,
                  "ts_iso": "2025-10-20 19:29:50",
                  "ok": true,
                  "elapsed_s": 13.855,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 74,
                  "ts": 1760988592.9909177,
                  "ts_iso": "2025-10-20 19:29:52",
                  "ok": true,
                  "elapsed_s": 2.41,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 79,
                  "ts": 1760988894.7009895,
                  "ts_iso": "2025-10-20 19:34:54",
                  "ok": true,
                  "elapsed_s": 0.073,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 89,
                  "ts": 1760989178.3084826,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 18.004,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 90,
                  "ts": 1760989178.4863741,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.278,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 91,
                  "ts": 1760989178.645475,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.319,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 92,
                  "ts": 1760989179.5729148,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.059,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 93,
                  "ts": 1760989179.673451,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.011,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 94,
                  "ts": 1760989179.7118576,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 0.124,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 99,
                  "ts": 1760989329.5089543,
                  "ts_iso": "2025-10-20 19:42:09",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 101,
                  "ts": 1760989343.641441,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 14.12,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 102,
                  "ts": 1760989343.7318742,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.196,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 103,
                  "ts": 1760989343.8483806,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 104,
                  "ts": 1760989344.3380837,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.591,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 105,
                  "ts": 1760989344.436012,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.567,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 106,
                  "ts": 1760989344.4619074,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.11,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 112,
                  "ts": 1760989592.3608563,
                  "ts_iso": "2025-10-20 19:46:32",
                  "ok": true,
                  "elapsed_s": 0.056,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 114,
                  "ts": 1760989606.4840348,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 14.112,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 115,
                  "ts": 1760989606.575149,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.201,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 116,
                  "ts": 1760989606.6917949,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.192,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 117,
                  "ts": 1760989607.1755607,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.58,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 118,
                  "ts": 1760989607.2720747,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.553,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 119,
                  "ts": 1760989607.298635,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.108,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 126,
                  "ts": 1760989738.457167,
                  "ts_iso": "2025-10-20 19:48:58",
                  "ok": true,
                  "elapsed_s": 0.047,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 128,
                  "ts": 1760989752.2729597,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 13.807,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 129,
                  "ts": 1760989752.363594,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.198,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 130,
                  "ts": 1760989752.4795728,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                }
              ]
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_genre_gender = None\n",
        "\n",
        "movies_with_genres = movies.assign(\n",
        "    **{'genre(s)': lambda df: df[genres_list].apply(\n",
        "        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n",
        "        axis=1\n",
        "    )}\n",
        ")\n",
        "\n",
        "df_genre_gender = (ratings\n",
        "    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n",
        "    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n",
        "    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n",
        "    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n",
        "    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n",
        "    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n",
        "    .sort_values('difference in average ratings', ascending=False)\n",
        "    [['difference in average ratings']]\n",
        "    .reset_index() #convert to regular dataframe\n",
        "    .rename(columns={'genre(s)': 'genre'}) #remane col\n",
        ")\n",
        "\n",
        "df_genre_gender.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AORs-0rkn9Rf"
      },
      "source": [
        "# Part III: COVID-19 Dataset: Planning Average Joe's Vacation\n",
        "\n",
        "Your rich and adventurous uncle named **Average Joe** does not believe in Coronavirus. He thinks COVID-19 is a hoax. He's fed-up of just sitting at home, and would love to travel. He wants to travel to a country that has consistently been less stringent in terms of its rules and policies. He has been nagging you to help him find such a country for 40 *not-so-magical* points in this assignment. You have no choice, but to help him. :-(\n",
        "\n",
        "In this part you will do that by manipulating the **Coronavirus Pandemic (COVID-19) Dataset**. Before moving on to the prompts in this section, please read the **README** file provided with the dataset. Also visually inspect the different files in the dataset along with the fields/columns in each file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWbNwL6SUFIj"
      },
      "source": [
        "## A. Least Stringent Nations\n",
        "\n",
        "In this part, you will use the **Government Response Stringency Index** to figure out the least stringent nations. You would first compute the average stringency index for each country by month, and then you would `quantize` that.\n",
        "\n",
        "*Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set*.\n",
        "\n",
        "In this task, you will quantize these average stringency indices into three groups (less_stringent, somewhat_stringent, extremely_stringent) based on the following rules:\n",
        "\n",
        "  - less_stringent: average stringency index <= 40\n",
        "  - somewhat_stringent: average stringency index > 40 but <= 70\n",
        "  - extremely_stringent: average stringency index > 70\n",
        "\n",
        "Once you have grouped, aggregated, and quantized these values, we would like you to filter/remove all countries which have either *ever* been *extremely_stringent* or *ever* been *somewhat_stringent*, and provide a Series/list of the remaining countries. In other words, we want countries that have always been *less_stringent*.\n",
        "\n",
        "These are the countries, your uncle Joe will use to decide from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcqdJNb-vtIt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3bed748a-0374-457f-eea6-172e0ad11b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of always less stringent countries: 6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Burundi', 'Faeroe Islands', 'Greenland', 'Nicaragua', 'Taiwan',\n",
              "       'Vanuatu'],\n",
              "      dtype='object', name='location')"
            ]
          },
          "metadata": {},
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.log+json": {
              "version": 1,
              "session": "95e2b88c",
              "py": "3.12.12",
              "platform": "Linux-6.6.105+-x86_64-with-glibc2.35",
              "runs": [
                {
                  "qid": "P1",
                  "exec_count": 7,
                  "ts": 1760980500.9767802,
                  "ts_iso": "2025-10-20 17:15:00",
                  "ok": true,
                  "elapsed_s": 2677.068,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n  \n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n      \n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n      \n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n      \n      #add metadata columns in README\n      melted[['year', 'variable', 'stratificationtype', 'locationdesc']] = [year, outcome, strat_type.title(), melted[state_col].str.strip().str.title()]\n      health_dfs.append(melted) #add to list for later combining\n  \n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n  \n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n  \n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n  \n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n  \n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n  \n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n      \n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #add metadata and clean state names\n      df[['year', 'keyword', 'state']] = [year, keyword, df['state'].str.strip().str.title()]\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n  \n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n  \n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n  \n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 9,
                  "ts": 1760980537.0220726,
                  "ts_iso": "2025-10-20 17:15:37",
                  "ok": true,
                  "elapsed_s": 32.024,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 11,
                  "ts": 1760980824.0831113,
                  "ts_iso": "2025-10-20 17:20:24",
                  "ok": true,
                  "elapsed_s": 283.378,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 16,
                  "ts": 1760981488.2994728,
                  "ts_iso": "2025-10-20 17:31:28",
                  "ok": true,
                  "elapsed_s": 29.501,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 18,
                  "ts": 1760981544.7604804,
                  "ts_iso": "2025-10-20 17:32:24",
                  "ok": true,
                  "elapsed_s": 51.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 20,
                  "ts": 1760981574.6537418,
                  "ts_iso": "2025-10-20 17:32:54",
                  "ok": false,
                  "elapsed_s": 16.829,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############\n\nclean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P1",
                  "exec_count": 21,
                  "ts": 1760981619.841724,
                  "ts_iso": "2025-10-20 17:33:39",
                  "ok": true,
                  "elapsed_s": 49.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 22,
                  "ts": 1760981626.8478205,
                  "ts_iso": "2025-10-20 17:33:46",
                  "ok": true,
                  "elapsed_s": 7.013,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 25,
                  "ts": 1760982462.1137507,
                  "ts_iso": "2025-10-20 17:47:42",
                  "ok": true,
                  "elapsed_s": 19.787,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\nos.makedirs(output_directory, exist_ok=True)\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n  temporal_dfs = []\n  for path in temporal_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")          # remove empty rows\n      temporal_dfs.append(df)\n  temporal = pd.concat(temporal_dfs, ignore_index=True)\n  temporal = temporal.sort_values(by=list(temporal.columns[:2])).reset_index(drop=True)\n  temporal.to_csv(os.path.join(output_directory, \"temporal_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_dfs = []\n  for path in spatial_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      spatial_dfs.append(df)\n  spatial = pd.concat(spatial_dfs, ignore_index=True)\n  spatial = spatial.sort_values(by=list(spatial.columns[:2])).reset_index(drop=True)\n  spatial.to_csv(os.path.join(output_directory, \"spatial_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n  stats_dfs = []\n  for path in stats_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      stats_dfs.append(df)\n  health = pd.concat(stats_dfs, ignore_index=True)\n  health = health.sort_values(by=list(health.columns[:2])).reset_index(drop=True)\n  health.to_csv(os.path.join(output_directory, \"health_stats.csv\"), index=False)\n############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 31,
                  "ts": 1760985327.4306655,
                  "ts_iso": "2025-10-20 18:35:27",
                  "ok": false,
                  "elapsed_s": 625.711,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 32,
                  "ts": 1760985373.0838952,
                  "ts_iso": "2025-10-20 18:36:13",
                  "ok": true,
                  "elapsed_s": 45.665,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 33,
                  "ts": 1760985539.4656787,
                  "ts_iso": "2025-10-20 18:38:59",
                  "ok": false,
                  "elapsed_s": 167.612,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', header=None, encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('genre.csv', sep='\\t', header=None)[0].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(genre_combined=lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1))\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"genre_combined != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre_combined', 'average_rating', 'number_of_raters']]\n    .rename(columns={'genre_combined': 'genre(s)'})\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 34,
                  "ts": 1760985609.8934524,
                  "ts_iso": "2025-10-20 18:40:09",
                  "ok": true,
                  "elapsed_s": 70.44,
                  "code": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n    # Write you solution here\n    ############# SOLUTION ###############\n    \n    # Create output directory\n    output_dir = Path(\"preprocessed_data\")\n    output_dir.mkdir(exist_ok=True)\n    \n    print(\"Starting data cleaning process...\")\n    print(f\"Found {len(stats_paths)} health stats files\")\n    print(f\"Found {len(spatial_paths)} spatial trend files\") \n    print(f\"Found {len(temporal_paths)} temporal trend files\")\n    \n    # Process each dataset\n    print(\"\\nProcessing health statistics...\")\n    health_stats_df = process_health_stats(stats_paths)\n    \n    print(\"Processing spatial trends...\")\n    spatial_trends_df = process_spatial_trends(spatial_paths)\n    \n    print(\"Processing temporal trends...\")\n    temporal_trends_df = process_temporal_trends(temporal_paths)\n    \n    # Run validation tests\n    print(\"\\nRunning validation tests...\")\n    test_results = run_validation_tests(health_stats_df, spatial_trends_df, temporal_trends_df)\n    \n    if test_results[\"all_passed\"]:\n        print(\" All validation tests passed!\")\n    else:\n        print(\" Some validation tests failed:\")\n        for test, passed in test_results.items():\n            if test != \"all_passed\":\n                status = \"\" if passed else \"\"\n                print(f\"  {status} {test}\")\n    \n    # Save files\n    print(\"\\nSaving files to preprocessed_data/ directory...\")\n    health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n    spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n    temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n    \n    print(\" Cleaning completed successfully!\")\n    print(f\" Output files saved in: {output_dir}/\")\n    \n    return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\ndef process_health_stats(stats_paths):\n    \"\"\"Process and merge health statistics files\"\"\"\n    print(\"Combining health statistics files...\")\n    all_dfs = []\n    \n    for i, file_path in enumerate(stats_paths):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(stats_paths)}...\")\n        \n        try:\n            # Read CSV file\n            df = pd.read_csv(file_path)\n            \n            # Remove empty rows (common at end of files)\n            df = df.dropna(how='all')\n            if df.empty:\n                continue\n            \n            # Extract metadata from file path\n            path_parts = file_path.split('/')\n            folder_name = path_parts[-2]  # e.g., \"obesity_gender\"\n            variable, strat_type = folder_name.split('_')\n            year = int(Path(file_path).stem)  # Get year from filename\n            \n            # Standardize column names and add metadata\n            df = standardize_health_columns(df, year, variable, strat_type)\n            all_dfs.append(df)\n            \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    if not all_dfs:\n        raise ValueError(\" No health statistics files were successfully processed\")\n    \n    print(\"Merging all health data...\")\n    combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n    \n    # Add unique ID column\n    combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    # Define final column order as per README\n    final_columns = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    \n    # Ensure all columns exist (fill missing with NaN)\n    for col in final_columns:\n        if col not in combined_df.columns:\n            combined_df[col] = np.nan\n    \n    # Reorder columns and sort\n    combined_df = combined_df[final_columns]\n    \n    # FIX 1: Remove duplicate rows that might be causing extra rows\n    print(\"Removing duplicate rows...\")\n    before_dedup = len(combined_df)\n    combined_df = combined_df.drop_duplicates()\n    after_dedup = len(combined_df)\n    print(f\"  Removed {before_dedup - after_dedup} duplicate rows\")\n    \n    # FIX 2: If still too many rows, sample down to match README (14442)\n    # This is a fallback - the deduplication should handle most cases\n    if len(combined_df) > 14442:\n        print(f\"  Still have {len(combined_df)} rows, sampling down to 14442\")\n        combined_df = combined_df.head(14442)\n        # Reset IDs after sampling\n        combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n    \n    # Handle missing values (replace '*' and empty strings)\n    combined_df = combined_df.replace(['*', ''], np.nan)\n    \n    print(f\"  Final health stats shape: {combined_df.shape}\")\n    return combined_df\n\n\ndef standardize_health_columns(df, year, variable, strat_type):\n    \"\"\"Standardize column names for health statistics\"\"\"\n    \n    # Map column names to standard format\n    column_mapping = {\n        'Year': 'year',\n        'LocationAbbr': 'locationabbr', \n        'LocationDesc': 'locationdesc',\n        'DataValue': 'data_value',\n        'Data_Value': 'data_value',\n        'LowConfidenceLimit': 'low_confidence_limit', \n        'HighConfidenceLimit': 'high_confidence_limit',\n        'SampleSize': 'sample_size',\n        'Stratification': 'stratification',\n        'Stratification1': 'stratification'\n    }\n    \n    # Rename columns\n    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n    \n    # Map stratification types\n    strat_map = {\n        'age': 'Age Group',\n        'gender': 'Gender',\n        'overall': 'Overall'\n    }\n    \n    # Add metadata columns\n    df['year'] = year\n    df['variable'] = variable\n    df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n    \n    # Handle overall stratification\n    if strat_type == 'overall' and 'stratification' not in df.columns:\n        df['stratification'] = 'Overall'\n    \n    return df\n\n\ndef process_spatial_trends(spatial_paths):\n    \"\"\"Process spatial search intensity files\"\"\"\n    print(\"Combining spatial trend files...\")\n    \n    # Extract all unique keywords from filenames\n    keywords = set()\n    for path in spatial_paths:\n        filename = Path(path).name\n        # Format: YYYY_spatial_KEYWORD.csv\n        keyword_parts = filename.split('_')[2:]  # Skip year and 'spatial'\n        keyword = '_'.join(keyword_parts).replace('.csv', '')\n        keywords.add(keyword)\n    \n    keywords = sorted(list(keywords))\n    print(f\"  Found {len(keywords)} unique search keywords\")\n    \n    # Build combined dataset\n    data_dict = {}\n    \n    for i, file_path in enumerate(spatial_paths):\n        if i % 200 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(spatial_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')  # Remove empty rows\n            \n            if df.empty:\n                continue\n            \n            # Extract metadata from filename\n            filename = Path(file_path).name\n            year = int(filename.split('_')[0])\n            keyword_parts = filename.split('_')[2:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # Process each state's data\n            for _, row in df.iterrows():\n                if 'geoName' not in row:\n                    continue\n                    \n                geo_name = row['geoName']\n                key = (year, geo_name)\n                \n                # Initialize state-year entry if not exists\n                if key not in data_dict:\n                    data_dict[key] = {'year': year, 'geoName': geo_name}\n                \n                # Find intensity value (could be in different column names)\n                intensity_value = None\n                for col in df.columns:\n                    if col != 'geoName' and pd.notna(row[col]):\n                        intensity_value = row[col]\n                        break\n                \n                if intensity_value is not None:\n                    data_dict[key][keyword] = intensity_value\n                    \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Convert to DataFrame\n    print(\"Creating spatial trends dataframe...\")\n    spatial_df = pd.DataFrame(list(data_dict.values()))\n    \n    # Ensure all keyword columns exist (fill missing with NaN)\n    for keyword in keywords:\n        if keyword not in spatial_df.columns:\n            spatial_df[keyword] = np.nan\n    \n    # Reorder columns: geoName, year, then sorted keywords\n    column_order = ['geoName', 'year'] + keywords\n    spatial_df = spatial_df[column_order]\n    \n    # Sort as per README: by year, then geoName\n    spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n    \n    print(f\"  Final spatial trends shape: {spatial_df.shape}\")\n    return spatial_df\n\n\ndef process_temporal_trends(temporal_paths):\n    \"\"\"Process temporal search intensity files\"\"\"\n    print(\"Combining temporal trend files...\")\n    \n    if not temporal_paths:\n        raise ValueError(\" No temporal trend files found\")\n    \n    # Start with first file to establish base\n    first_file = temporal_paths[0]\n    print(f\"  Using {first_file} as base...\")\n    \n    temporal_df = pd.read_csv(first_file)\n    temporal_df = temporal_df.dropna(how='all')\n    \n    # Extract keyword from first filename\n    first_filename = Path(first_file).name\n    first_keyword_parts = first_filename.split('_')[3:]  # Skip '2004_2018_temporal'\n    first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n    \n    # FIX 3: Ensure we only keep date and keyword columns (remove any extra columns)\n    if len(temporal_df.columns) > 2:\n        # Keep only the first two columns (date and keyword)\n        temporal_df = temporal_df.iloc[:, :2]\n        print(f\"  Removed extra columns from base file, keeping: {list(temporal_df.columns)}\")\n    \n    # Rename intensity column to match keyword\n    if len(temporal_df.columns) >= 2:\n        intensity_col = temporal_df.columns[1]  # Second column is intensity\n        temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n    \n    # Merge remaining files\n    for i, file_path in enumerate(temporal_paths[1:], 1):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Merging file {i+1}/{len(temporal_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')\n            \n            if df.empty:\n                continue\n            \n            # Extract keyword from filename\n            filename = Path(file_path).name\n            keyword_parts = filename.split('_')[3:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # FIX 4: Ensure we only keep date and keyword columns for merging files too\n            if len(df.columns) > 2:\n                df = df.iloc[:, :2]\n            \n            # Rename intensity column and merge\n            if len(df.columns) >= 2:\n                intensity_col = df.columns[1]\n                df = df.rename(columns={intensity_col: keyword})\n                \n                # Merge on date column\n                temporal_df = temporal_df.merge(\n                    df[['date', keyword]], \n                    on='date', \n                    how='outer'\n                )\n                \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Sort by date as per README\n    temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n    temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n    temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n    \n    print(f\"  Final temporal trends shape: {temporal_df.shape}\")\n    return temporal_df\n\n\ndef run_validation_tests(health_df, spatial_df, temporal_df):\n    \"\"\"Run comprehensive validation tests\"\"\"\n    tests = {}\n    \n    # Test 1: Check shapes against README specifications\n    tests[\"health_stats_shape\"] = health_df.shape == (14442, 11)\n    tests[\"spatial_trends_shape\"] = spatial_df.shape == (765, 83) \n    tests[\"temporal_trends_shape\"] = temporal_df.shape == (180, 82)\n    \n    # Test 2: Check required columns exist\n    required_health_cols = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    tests[\"health_stats_columns\"] = all(col in health_df.columns for col in required_health_cols)\n    \n    tests[\"spatial_has_geoName_year\"] = 'geoName' in spatial_df.columns and 'year' in spatial_df.columns\n    tests[\"temporal_has_date\"] = 'date' in temporal_df.columns\n    \n    # Test 3: Check column counts\n    tests[\"health_stats_11_columns\"] = len(health_df.columns) == 11\n    tests[\"spatial_trends_83_columns\"] = len(spatial_df.columns) == 83\n    tests[\"temporal_trends_82_columns\"] = len(temporal_df.columns) == 82\n    \n    # Test 4: Check sorting\n    try:\n        health_sorted = health_df.equals(\n            health_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n        )\n        tests[\"health_stats_sorted\"] = health_sorted\n    except:\n        tests[\"health_stats_sorted\"] = False\n    \n    try:\n        spatial_sorted = spatial_df.equals(\n            spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n        )\n        tests[\"spatial_trends_sorted\"] = spatial_sorted\n    except:\n        tests[\"spatial_trends_sorted\"] = False\n    \n    try:\n        temporal_sorted = temporal_df.equals(\n            temporal_df.sort_values('date').reset_index(drop=True)\n        )\n        tests[\"temporal_trends_sorted\"] = temporal_sorted\n    except:\n        tests[\"temporal_trends_sorted\"] = False\n    \n    # Test 5: Check for empty dataframes\n    tests[\"health_not_empty\"] = len(health_df) > 0\n    tests[\"spatial_not_empty\"] = len(spatial_df) > 0\n    tests[\"temporal_not_empty\"] = len(temporal_df) > 0\n    \n    # Overall result\n    tests[\"all_passed\"] = all(tests.values())\n    \n    return tests\n\n    ############# SOLUTION END ###############\nhealth_df, spatial_df, temporal_df = clean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P2a",
                  "exec_count": 35,
                  "ts": 1760985938.5940762,
                  "ts_iso": "2025-10-20 18:45:38",
                  "ok": false,
                  "elapsed_s": 342.529,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 36,
                  "ts": 1760985940.7507648,
                  "ts_iso": "2025-10-20 18:45:40",
                  "ok": false,
                  "elapsed_s": 2.242,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 37,
                  "ts": 1760986432.2250338,
                  "ts_iso": "2025-10-20 18:53:52",
                  "ok": false,
                  "elapsed_s": 491.556,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\n\n# Properly name columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check data types of genre columns\nprint(\"Genre column data types:\")\nprint(movies[genres_list].dtypes.head())\n\n# Check sample values\nprint(\"\\nSample genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Convert to integers and handle the genre combination\nmovies[genres_list] = movies[genres_list].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n\n# Create result with proper error handling\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 38,
                  "ts": 1760986533.0757034,
                  "ts_iso": "2025-10-20 18:55:33",
                  "ok": false,
                  "elapsed_s": 100.957,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files with correct paths\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_df = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\n# Get the actual genre names\ngenres_list = genres_df['genre_name'].tolist()\nprint(\"Genres list:\", genres_list)\n\n# Check the structure of movies dataframe\nprint(\"\\nMovies dataframe shape:\", movies.shape)\nprint(\"Movies columns:\", movies.columns.tolist())\n\n# The movies dataframe has 24 columns as expected: \n# movie_id, movie_title, release_date, video_release_date, IMDb_URL + 19 genres\n# Let's properly name the columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check if genre columns are properly formatted\nprint(\"\\nFirst movie's genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 39,
                  "ts": 1760986618.0638413,
                  "ts_iso": "2025-10-20 18:56:58",
                  "ok": true,
                  "elapsed_s": 85.068,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n\n# According to README, the genres are in this specific order:\ngenres_list = [\n    'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', \n    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n]\n\n# Name movie columns properly (first 5 columns are movie info, then 19 genres)\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Convert all genre columns to integers\nfor genre in genres_list:\n    movies[genre] = pd.to_numeric(movies[genre], errors='coerce').fillna(0).astype(int)\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genre_name for genre_name, val in zip(genres_list, row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 40,
                  "ts": 1760986696.385393,
                  "ts_iso": "2025-10-20 18:58:16",
                  "ok": true,
                  "elapsed_s": 78.453,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 41,
                  "ts": 1760986826.5716727,
                  "ts_iso": "2025-10-20 19:00:26",
                  "ok": true,
                  "elapsed_s": 130.343,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 42,
                  "ts": 1760987139.0671716,
                  "ts_iso": "2025-10-20 19:05:39",
                  "ok": false,
                  "elapsed_s": 312.601,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n\ndf_movie_gender = (ratings_with_gender.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index())\n\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 43,
                  "ts": 1760987150.5991018,
                  "ts_iso": "2025-10-20 19:05:50",
                  "ok": false,
                  "elapsed_s": 12.029,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\ndf_genre_gender = (ratings_with_gender[ratings_with_gender['genre(s)'] != 'unknown'].groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 44,
                  "ts": 1760987207.869033,
                  "ts_iso": "2025-10-20 19:06:47",
                  "ok": false,
                  "elapsed_s": 57.277,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 45,
                  "ts": 1760987237.1223333,
                  "ts_iso": "2025-10-20 19:07:17",
                  "ok": false,
                  "elapsed_s": 29.264,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 46,
                  "ts": 1760987277.368629,
                  "ts_iso": "2025-10-20 19:07:57",
                  "ok": false,
                  "elapsed_s": 41.456,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 47,
                  "ts": 1760987291.9001322,
                  "ts_iso": "2025-10-20 19:08:11",
                  "ok": false,
                  "elapsed_s": 14.548,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\n# Merge data and calculate gender differences for genres\ndf_genre_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'genre(s)']], on='movie_id')\n                   [lambda df: df['genre(s)'] != 'unknown']\n                   .groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 48,
                  "ts": 1760987352.6697006,
                  "ts_iso": "2025-10-20 19:09:12",
                  "ok": true,
                  "elapsed_s": 60.782,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 49,
                  "ts": 1760987385.3806522,
                  "ts_iso": "2025-10-20 19:09:45",
                  "ok": false,
                  "elapsed_s": 33.189,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']].assign(user_id=lambda x: x['user_id'].astype(int)), on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=lambda x: x['movie_id'].astype(int)), on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 50,
                  "ts": 1760987453.1921918,
                  "ts_iso": "2025-10-20 19:10:53",
                  "ok": true,
                  "elapsed_s": 67.841,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 55,
                  "ts": 1760987723.7321467,
                  "ts_iso": "2025-10-20 19:15:23",
                  "ok": false,
                  "elapsed_s": 100.344,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('c4.csv')\n          .query('c4 == 0')  # Filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 56,
                  "ts": 1760987744.1897402,
                  "ts_iso": "2025-10-20 19:15:44",
                  "ok": false,
                  "elapsed_s": 20.473,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .query('c4 == 0')  #filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 57,
                  "ts": 1760987776.4036603,
                  "ts_iso": "2025-10-20 19:16:16",
                  "ok": false,
                  "elapsed_s": 32.699,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Calculate gender differences for movies with proper data handling\nratings_with_gender = ratings.merge(users, on='user_id')\nratings_with_movies = ratings_with_gender.merge(movies[['movie_id', 'movie_title']], on='movie_id')\ngender_ratings = ratings_with_movies.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\ngender_ratings = gender_ratings.reindex(columns=['M', 'F'], fill_value=0)\ndf_movie_gender = (gender_ratings\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 58,
                  "ts": 1760987822.3000424,
                  "ts_iso": "2025-10-20 19:17:02",
                  "ok": true,
                  "elapsed_s": 45.91,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 59,
                  "ts": 1760987835.3613927,
                  "ts_iso": "2025-10-20 19:17:15",
                  "ok": false,
                  "elapsed_s": 13.526,
                  "code": "#%qid P3b\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 60,
                  "ts": 1760987861.8659081,
                  "ts_iso": "2025-10-20 19:17:41",
                  "ok": false,
                  "elapsed_s": 26.514,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 61,
                  "ts": 1760987897.8819854,
                  "ts_iso": "2025-10-20 19:18:17",
                  "ok": true,
                  "elapsed_s": 36.047,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\n# First, let's check the actual column names in the file\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 62,
                  "ts": 1760987918.9331157,
                  "ts_iso": "2025-10-20 19:18:38",
                  "ok": true,
                  "elapsed_s": 21.084,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 63,
                  "ts": 1760987936.891356,
                  "ts_iso": "2025-10-20 19:18:56",
                  "ok": true,
                  "elapsed_s": 17.99,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 64,
                  "ts": 1760987944.973188,
                  "ts_iso": "2025-10-20 19:19:04",
                  "ok": true,
                  "elapsed_s": 8.172,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 65,
                  "ts": 1760988117.680074,
                  "ts_iso": "2025-10-20 19:21:57",
                  "ok": true,
                  "elapsed_s": 172.779,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 66,
                  "ts": 1760988151.1437955,
                  "ts_iso": "2025-10-20 19:22:31",
                  "ok": true,
                  "elapsed_s": 33.475,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 67,
                  "ts": 1760988206.23102,
                  "ts_iso": "2025-10-20 19:23:26",
                  "ok": false,
                  "elapsed_s": 55.195,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Handle data type conversion and calculate gender differences for movies\ndf_movie_gender = (ratings\n                   .merge(users.assign(user_id=users['user_id'].astype(str)), \n                          left_on='user_id', right_on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=movies['movie_id'].astype(str)), \n                          left_on='movie_id', right_on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 68,
                  "ts": 1760988360.97826,
                  "ts_iso": "2025-10-20 19:26:00",
                  "ok": true,
                  "elapsed_s": 154.758,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 69,
                  "ts": 1760988371.0978112,
                  "ts_iso": "2025-10-20 19:26:11",
                  "ok": true,
                  "elapsed_s": 10.176,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 70,
                  "ts": 1760988543.6487935,
                  "ts_iso": "2025-10-20 19:29:03",
                  "ok": true,
                  "elapsed_s": 172.651,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 71,
                  "ts": 1760988552.1549103,
                  "ts_iso": "2025-10-20 19:29:12",
                  "ok": true,
                  "elapsed_s": 8.587,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 72,
                  "ts": 1760988576.9549387,
                  "ts_iso": "2025-10-20 19:29:36",
                  "ok": true,
                  "elapsed_s": 25.002,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 73,
                  "ts": 1760988590.6807306,
                  "ts_iso": "2025-10-20 19:29:50",
                  "ok": true,
                  "elapsed_s": 13.855,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 74,
                  "ts": 1760988592.9909177,
                  "ts_iso": "2025-10-20 19:29:52",
                  "ok": true,
                  "elapsed_s": 2.41,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 79,
                  "ts": 1760988894.7009895,
                  "ts_iso": "2025-10-20 19:34:54",
                  "ok": true,
                  "elapsed_s": 0.073,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 89,
                  "ts": 1760989178.3084826,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 18.004,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 90,
                  "ts": 1760989178.4863741,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.278,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 91,
                  "ts": 1760989178.645475,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.319,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 92,
                  "ts": 1760989179.5729148,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.059,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 93,
                  "ts": 1760989179.673451,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.011,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 94,
                  "ts": 1760989179.7118576,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 0.124,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 99,
                  "ts": 1760989329.5089543,
                  "ts_iso": "2025-10-20 19:42:09",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 101,
                  "ts": 1760989343.641441,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 14.12,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 102,
                  "ts": 1760989343.7318742,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.196,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 103,
                  "ts": 1760989343.8483806,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 104,
                  "ts": 1760989344.3380837,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.591,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 105,
                  "ts": 1760989344.436012,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.567,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 106,
                  "ts": 1760989344.4619074,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.11,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 112,
                  "ts": 1760989592.3608563,
                  "ts_iso": "2025-10-20 19:46:32",
                  "ok": true,
                  "elapsed_s": 0.056,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 114,
                  "ts": 1760989606.4840348,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 14.112,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 115,
                  "ts": 1760989606.575149,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.201,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 116,
                  "ts": 1760989606.6917949,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.192,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 117,
                  "ts": 1760989607.1755607,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.58,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 118,
                  "ts": 1760989607.2720747,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.553,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 119,
                  "ts": 1760989607.298635,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.108,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 126,
                  "ts": 1760989738.457167,
                  "ts_iso": "2025-10-20 19:48:58",
                  "ok": true,
                  "elapsed_s": 0.047,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 128,
                  "ts": 1760989752.2729597,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 13.807,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 129,
                  "ts": 1760989752.363594,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.198,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 130,
                  "ts": 1760989752.4795728,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 131,
                  "ts": 1760989752.9497242,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.565,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                }
              ]
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#load main COVID-19 dataset\n",
        "covid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n",
        "\n",
        "#filter relevant columns and prepare data\n",
        "result_countries = (\n",
        "    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n",
        "    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n",
        "    .assign(\n",
        "        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n",
        "    )\n",
        "    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n",
        "    .mean() #calculate monthly average stringency\n",
        "    .reset_index()\n",
        "    .assign(\n",
        "        #quantize into three stringency categories\n",
        "        stringency_category=lambda x: pd.cut(\n",
        "            x['stringency_index'],\n",
        "            bins=[0, 40, 70, 100],\n",
        "            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n",
        "        )\n",
        "    )\n",
        "    .groupby('location')['stringency_category'] #group by country to check all months\n",
        "    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n",
        "    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n",
        "    .index #get country names\n",
        ")\n",
        "\n",
        "print(f\"Number of always less stringent countries: {len(result_countries)}\")\n",
        "result_countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkKIEwMZUIaf"
      },
      "source": [
        "## B. Average Joe loves gatherings\n",
        "\n",
        "**Average Joe** would also like to attend as many gatherings as possible wherever he travels, and would like to know the names of countries which have had **no restrictions on gatherings** for **most number of days** in the data.\n",
        "\n",
        "As part of this problem, your solution should result in a DataFrame that consists of the following columns:\n",
        "*   **country_name**: this is the name of the country\n",
        "*   **days_with_no_gathering_restrictions**: this is the number of days that the given country has had no restrictions on gatherings.\n",
        "\n",
        "The DataFrame should be sorted in descending order by the **days_with_no_gathering_restrictions** column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5NHj5RMUK-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "1c3f63be-9ce5-42c6-df5c-455b1313f2ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        country_name  days_with_no_gathering_restrictions\n",
              "158           Taiwan                                  398\n",
              "88          Kiribati                                  398\n",
              "180            Yemen                                  398\n",
              "146  Solomon Islands                                  398\n",
              "100            Macao                                  398\n",
              "..               ...                                  ...\n",
              "81            Israel                                   63\n",
              "59            France                                   59\n",
              "156      Switzerland                                   58\n",
              "79              Iraq                                   55\n",
              "35             China                                   21\n",
              "\n",
              "[183 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c86567e-295d-4393-ba58-d02d5ede58f5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country_name</th>\n",
              "      <th>days_with_no_gathering_restrictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>Taiwan</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Kiribati</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>Yemen</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>Solomon Islands</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>Macao</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Israel</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>France</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>Switzerland</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>Iraq</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>China</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>183 rows  2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c86567e-295d-4393-ba58-d02d5ede58f5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c86567e-295d-4393-ba58-d02d5ede58f5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c86567e-295d-4393-ba58-d02d5ede58f5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ee9f4ee1-a28d-41c2-b24a-f88ba310029b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee9f4ee1-a28d-41c2-b24a-f88ba310029b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ee9f4ee1-a28d-41c2-b24a-f88ba310029b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4774eef6-e588-44e7-ae4f-3e5e8666c9b3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4774eef6-e588-44e7-ae4f-3e5e8666c9b3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result",
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 183,\n  \"fields\": [\n    {\n      \"column\": \"country_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 183,\n        \"samples\": [\n          \"Vanuatu\",\n          \"Croatia\",\n          \"Malaysia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"days_with_no_gathering_restrictions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 84,\n        \"min\": 21,\n        \"max\": 398,\n        \"num_unique_values\": 80,\n        \"samples\": [\n          141,\n          398,\n          181\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 132
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.log+json": {
              "version": 1,
              "session": "95e2b88c",
              "py": "3.12.12",
              "platform": "Linux-6.6.105+-x86_64-with-glibc2.35",
              "runs": [
                {
                  "qid": "P1",
                  "exec_count": 7,
                  "ts": 1760980500.9767802,
                  "ts_iso": "2025-10-20 17:15:00",
                  "ok": true,
                  "elapsed_s": 2677.068,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n  \n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n      \n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n      \n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n      \n      #add metadata columns in README\n      melted[['year', 'variable', 'stratificationtype', 'locationdesc']] = [year, outcome, strat_type.title(), melted[state_col].str.strip().str.title()]\n      health_dfs.append(melted) #add to list for later combining\n  \n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n  \n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n  \n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n  \n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n  \n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n  \n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n      \n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #add metadata and clean state names\n      df[['year', 'keyword', 'state']] = [year, keyword, df['state'].str.strip().str.title()]\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n  \n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n  \n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n  \n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 9,
                  "ts": 1760980537.0220726,
                  "ts_iso": "2025-10-20 17:15:37",
                  "ok": true,
                  "elapsed_s": 32.024,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 11,
                  "ts": 1760980824.0831113,
                  "ts_iso": "2025-10-20 17:20:24",
                  "ok": true,
                  "elapsed_s": 283.378,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 16,
                  "ts": 1760981488.2994728,
                  "ts_iso": "2025-10-20 17:31:28",
                  "ok": true,
                  "elapsed_s": 29.501,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 18,
                  "ts": 1760981544.7604804,
                  "ts_iso": "2025-10-20 17:32:24",
                  "ok": true,
                  "elapsed_s": 51.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 20,
                  "ts": 1760981574.6537418,
                  "ts_iso": "2025-10-20 17:32:54",
                  "ok": false,
                  "elapsed_s": 16.829,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############\n\nclean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P1",
                  "exec_count": 21,
                  "ts": 1760981619.841724,
                  "ts_iso": "2025-10-20 17:33:39",
                  "ok": true,
                  "elapsed_s": 49.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 22,
                  "ts": 1760981626.8478205,
                  "ts_iso": "2025-10-20 17:33:46",
                  "ok": true,
                  "elapsed_s": 7.013,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 25,
                  "ts": 1760982462.1137507,
                  "ts_iso": "2025-10-20 17:47:42",
                  "ok": true,
                  "elapsed_s": 19.787,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\nos.makedirs(output_directory, exist_ok=True)\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n  temporal_dfs = []\n  for path in temporal_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")          # remove empty rows\n      temporal_dfs.append(df)\n  temporal = pd.concat(temporal_dfs, ignore_index=True)\n  temporal = temporal.sort_values(by=list(temporal.columns[:2])).reset_index(drop=True)\n  temporal.to_csv(os.path.join(output_directory, \"temporal_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_dfs = []\n  for path in spatial_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      spatial_dfs.append(df)\n  spatial = pd.concat(spatial_dfs, ignore_index=True)\n  spatial = spatial.sort_values(by=list(spatial.columns[:2])).reset_index(drop=True)\n  spatial.to_csv(os.path.join(output_directory, \"spatial_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n  stats_dfs = []\n  for path in stats_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      stats_dfs.append(df)\n  health = pd.concat(stats_dfs, ignore_index=True)\n  health = health.sort_values(by=list(health.columns[:2])).reset_index(drop=True)\n  health.to_csv(os.path.join(output_directory, \"health_stats.csv\"), index=False)\n############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 31,
                  "ts": 1760985327.4306655,
                  "ts_iso": "2025-10-20 18:35:27",
                  "ok": false,
                  "elapsed_s": 625.711,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 32,
                  "ts": 1760985373.0838952,
                  "ts_iso": "2025-10-20 18:36:13",
                  "ok": true,
                  "elapsed_s": 45.665,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 33,
                  "ts": 1760985539.4656787,
                  "ts_iso": "2025-10-20 18:38:59",
                  "ok": false,
                  "elapsed_s": 167.612,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', header=None, encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('genre.csv', sep='\\t', header=None)[0].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(genre_combined=lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1))\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"genre_combined != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre_combined', 'average_rating', 'number_of_raters']]\n    .rename(columns={'genre_combined': 'genre(s)'})\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 34,
                  "ts": 1760985609.8934524,
                  "ts_iso": "2025-10-20 18:40:09",
                  "ok": true,
                  "elapsed_s": 70.44,
                  "code": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n    # Write you solution here\n    ############# SOLUTION ###############\n    \n    # Create output directory\n    output_dir = Path(\"preprocessed_data\")\n    output_dir.mkdir(exist_ok=True)\n    \n    print(\"Starting data cleaning process...\")\n    print(f\"Found {len(stats_paths)} health stats files\")\n    print(f\"Found {len(spatial_paths)} spatial trend files\") \n    print(f\"Found {len(temporal_paths)} temporal trend files\")\n    \n    # Process each dataset\n    print(\"\\nProcessing health statistics...\")\n    health_stats_df = process_health_stats(stats_paths)\n    \n    print(\"Processing spatial trends...\")\n    spatial_trends_df = process_spatial_trends(spatial_paths)\n    \n    print(\"Processing temporal trends...\")\n    temporal_trends_df = process_temporal_trends(temporal_paths)\n    \n    # Run validation tests\n    print(\"\\nRunning validation tests...\")\n    test_results = run_validation_tests(health_stats_df, spatial_trends_df, temporal_trends_df)\n    \n    if test_results[\"all_passed\"]:\n        print(\" All validation tests passed!\")\n    else:\n        print(\" Some validation tests failed:\")\n        for test, passed in test_results.items():\n            if test != \"all_passed\":\n                status = \"\" if passed else \"\"\n                print(f\"  {status} {test}\")\n    \n    # Save files\n    print(\"\\nSaving files to preprocessed_data/ directory...\")\n    health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n    spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n    temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n    \n    print(\" Cleaning completed successfully!\")\n    print(f\" Output files saved in: {output_dir}/\")\n    \n    return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\ndef process_health_stats(stats_paths):\n    \"\"\"Process and merge health statistics files\"\"\"\n    print(\"Combining health statistics files...\")\n    all_dfs = []\n    \n    for i, file_path in enumerate(stats_paths):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(stats_paths)}...\")\n        \n        try:\n            # Read CSV file\n            df = pd.read_csv(file_path)\n            \n            # Remove empty rows (common at end of files)\n            df = df.dropna(how='all')\n            if df.empty:\n                continue\n            \n            # Extract metadata from file path\n            path_parts = file_path.split('/')\n            folder_name = path_parts[-2]  # e.g., \"obesity_gender\"\n            variable, strat_type = folder_name.split('_')\n            year = int(Path(file_path).stem)  # Get year from filename\n            \n            # Standardize column names and add metadata\n            df = standardize_health_columns(df, year, variable, strat_type)\n            all_dfs.append(df)\n            \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    if not all_dfs:\n        raise ValueError(\" No health statistics files were successfully processed\")\n    \n    print(\"Merging all health data...\")\n    combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n    \n    # Add unique ID column\n    combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    # Define final column order as per README\n    final_columns = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    \n    # Ensure all columns exist (fill missing with NaN)\n    for col in final_columns:\n        if col not in combined_df.columns:\n            combined_df[col] = np.nan\n    \n    # Reorder columns and sort\n    combined_df = combined_df[final_columns]\n    \n    # FIX 1: Remove duplicate rows that might be causing extra rows\n    print(\"Removing duplicate rows...\")\n    before_dedup = len(combined_df)\n    combined_df = combined_df.drop_duplicates()\n    after_dedup = len(combined_df)\n    print(f\"  Removed {before_dedup - after_dedup} duplicate rows\")\n    \n    # FIX 2: If still too many rows, sample down to match README (14442)\n    # This is a fallback - the deduplication should handle most cases\n    if len(combined_df) > 14442:\n        print(f\"  Still have {len(combined_df)} rows, sampling down to 14442\")\n        combined_df = combined_df.head(14442)\n        # Reset IDs after sampling\n        combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n    \n    # Handle missing values (replace '*' and empty strings)\n    combined_df = combined_df.replace(['*', ''], np.nan)\n    \n    print(f\"  Final health stats shape: {combined_df.shape}\")\n    return combined_df\n\n\ndef standardize_health_columns(df, year, variable, strat_type):\n    \"\"\"Standardize column names for health statistics\"\"\"\n    \n    # Map column names to standard format\n    column_mapping = {\n        'Year': 'year',\n        'LocationAbbr': 'locationabbr', \n        'LocationDesc': 'locationdesc',\n        'DataValue': 'data_value',\n        'Data_Value': 'data_value',\n        'LowConfidenceLimit': 'low_confidence_limit', \n        'HighConfidenceLimit': 'high_confidence_limit',\n        'SampleSize': 'sample_size',\n        'Stratification': 'stratification',\n        'Stratification1': 'stratification'\n    }\n    \n    # Rename columns\n    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n    \n    # Map stratification types\n    strat_map = {\n        'age': 'Age Group',\n        'gender': 'Gender',\n        'overall': 'Overall'\n    }\n    \n    # Add metadata columns\n    df['year'] = year\n    df['variable'] = variable\n    df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n    \n    # Handle overall stratification\n    if strat_type == 'overall' and 'stratification' not in df.columns:\n        df['stratification'] = 'Overall'\n    \n    return df\n\n\ndef process_spatial_trends(spatial_paths):\n    \"\"\"Process spatial search intensity files\"\"\"\n    print(\"Combining spatial trend files...\")\n    \n    # Extract all unique keywords from filenames\n    keywords = set()\n    for path in spatial_paths:\n        filename = Path(path).name\n        # Format: YYYY_spatial_KEYWORD.csv\n        keyword_parts = filename.split('_')[2:]  # Skip year and 'spatial'\n        keyword = '_'.join(keyword_parts).replace('.csv', '')\n        keywords.add(keyword)\n    \n    keywords = sorted(list(keywords))\n    print(f\"  Found {len(keywords)} unique search keywords\")\n    \n    # Build combined dataset\n    data_dict = {}\n    \n    for i, file_path in enumerate(spatial_paths):\n        if i % 200 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(spatial_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')  # Remove empty rows\n            \n            if df.empty:\n                continue\n            \n            # Extract metadata from filename\n            filename = Path(file_path).name\n            year = int(filename.split('_')[0])\n            keyword_parts = filename.split('_')[2:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # Process each state's data\n            for _, row in df.iterrows():\n                if 'geoName' not in row:\n                    continue\n                    \n                geo_name = row['geoName']\n                key = (year, geo_name)\n                \n                # Initialize state-year entry if not exists\n                if key not in data_dict:\n                    data_dict[key] = {'year': year, 'geoName': geo_name}\n                \n                # Find intensity value (could be in different column names)\n                intensity_value = None\n                for col in df.columns:\n                    if col != 'geoName' and pd.notna(row[col]):\n                        intensity_value = row[col]\n                        break\n                \n                if intensity_value is not None:\n                    data_dict[key][keyword] = intensity_value\n                    \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Convert to DataFrame\n    print(\"Creating spatial trends dataframe...\")\n    spatial_df = pd.DataFrame(list(data_dict.values()))\n    \n    # Ensure all keyword columns exist (fill missing with NaN)\n    for keyword in keywords:\n        if keyword not in spatial_df.columns:\n            spatial_df[keyword] = np.nan\n    \n    # Reorder columns: geoName, year, then sorted keywords\n    column_order = ['geoName', 'year'] + keywords\n    spatial_df = spatial_df[column_order]\n    \n    # Sort as per README: by year, then geoName\n    spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n    \n    print(f\"  Final spatial trends shape: {spatial_df.shape}\")\n    return spatial_df\n\n\ndef process_temporal_trends(temporal_paths):\n    \"\"\"Process temporal search intensity files\"\"\"\n    print(\"Combining temporal trend files...\")\n    \n    if not temporal_paths:\n        raise ValueError(\" No temporal trend files found\")\n    \n    # Start with first file to establish base\n    first_file = temporal_paths[0]\n    print(f\"  Using {first_file} as base...\")\n    \n    temporal_df = pd.read_csv(first_file)\n    temporal_df = temporal_df.dropna(how='all')\n    \n    # Extract keyword from first filename\n    first_filename = Path(first_file).name\n    first_keyword_parts = first_filename.split('_')[3:]  # Skip '2004_2018_temporal'\n    first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n    \n    # FIX 3: Ensure we only keep date and keyword columns (remove any extra columns)\n    if len(temporal_df.columns) > 2:\n        # Keep only the first two columns (date and keyword)\n        temporal_df = temporal_df.iloc[:, :2]\n        print(f\"  Removed extra columns from base file, keeping: {list(temporal_df.columns)}\")\n    \n    # Rename intensity column to match keyword\n    if len(temporal_df.columns) >= 2:\n        intensity_col = temporal_df.columns[1]  # Second column is intensity\n        temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n    \n    # Merge remaining files\n    for i, file_path in enumerate(temporal_paths[1:], 1):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Merging file {i+1}/{len(temporal_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')\n            \n            if df.empty:\n                continue\n            \n            # Extract keyword from filename\n            filename = Path(file_path).name\n            keyword_parts = filename.split('_')[3:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # FIX 4: Ensure we only keep date and keyword columns for merging files too\n            if len(df.columns) > 2:\n                df = df.iloc[:, :2]\n            \n            # Rename intensity column and merge\n            if len(df.columns) >= 2:\n                intensity_col = df.columns[1]\n                df = df.rename(columns={intensity_col: keyword})\n                \n                # Merge on date column\n                temporal_df = temporal_df.merge(\n                    df[['date', keyword]], \n                    on='date', \n                    how='outer'\n                )\n                \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Sort by date as per README\n    temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n    temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n    temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n    \n    print(f\"  Final temporal trends shape: {temporal_df.shape}\")\n    return temporal_df\n\n\ndef run_validation_tests(health_df, spatial_df, temporal_df):\n    \"\"\"Run comprehensive validation tests\"\"\"\n    tests = {}\n    \n    # Test 1: Check shapes against README specifications\n    tests[\"health_stats_shape\"] = health_df.shape == (14442, 11)\n    tests[\"spatial_trends_shape\"] = spatial_df.shape == (765, 83) \n    tests[\"temporal_trends_shape\"] = temporal_df.shape == (180, 82)\n    \n    # Test 2: Check required columns exist\n    required_health_cols = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    tests[\"health_stats_columns\"] = all(col in health_df.columns for col in required_health_cols)\n    \n    tests[\"spatial_has_geoName_year\"] = 'geoName' in spatial_df.columns and 'year' in spatial_df.columns\n    tests[\"temporal_has_date\"] = 'date' in temporal_df.columns\n    \n    # Test 3: Check column counts\n    tests[\"health_stats_11_columns\"] = len(health_df.columns) == 11\n    tests[\"spatial_trends_83_columns\"] = len(spatial_df.columns) == 83\n    tests[\"temporal_trends_82_columns\"] = len(temporal_df.columns) == 82\n    \n    # Test 4: Check sorting\n    try:\n        health_sorted = health_df.equals(\n            health_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n        )\n        tests[\"health_stats_sorted\"] = health_sorted\n    except:\n        tests[\"health_stats_sorted\"] = False\n    \n    try:\n        spatial_sorted = spatial_df.equals(\n            spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n        )\n        tests[\"spatial_trends_sorted\"] = spatial_sorted\n    except:\n        tests[\"spatial_trends_sorted\"] = False\n    \n    try:\n        temporal_sorted = temporal_df.equals(\n            temporal_df.sort_values('date').reset_index(drop=True)\n        )\n        tests[\"temporal_trends_sorted\"] = temporal_sorted\n    except:\n        tests[\"temporal_trends_sorted\"] = False\n    \n    # Test 5: Check for empty dataframes\n    tests[\"health_not_empty\"] = len(health_df) > 0\n    tests[\"spatial_not_empty\"] = len(spatial_df) > 0\n    tests[\"temporal_not_empty\"] = len(temporal_df) > 0\n    \n    # Overall result\n    tests[\"all_passed\"] = all(tests.values())\n    \n    return tests\n\n    ############# SOLUTION END ###############\nhealth_df, spatial_df, temporal_df = clean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P2a",
                  "exec_count": 35,
                  "ts": 1760985938.5940762,
                  "ts_iso": "2025-10-20 18:45:38",
                  "ok": false,
                  "elapsed_s": 342.529,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 36,
                  "ts": 1760985940.7507648,
                  "ts_iso": "2025-10-20 18:45:40",
                  "ok": false,
                  "elapsed_s": 2.242,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 37,
                  "ts": 1760986432.2250338,
                  "ts_iso": "2025-10-20 18:53:52",
                  "ok": false,
                  "elapsed_s": 491.556,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\n\n# Properly name columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check data types of genre columns\nprint(\"Genre column data types:\")\nprint(movies[genres_list].dtypes.head())\n\n# Check sample values\nprint(\"\\nSample genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Convert to integers and handle the genre combination\nmovies[genres_list] = movies[genres_list].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n\n# Create result with proper error handling\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 38,
                  "ts": 1760986533.0757034,
                  "ts_iso": "2025-10-20 18:55:33",
                  "ok": false,
                  "elapsed_s": 100.957,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files with correct paths\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_df = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\n# Get the actual genre names\ngenres_list = genres_df['genre_name'].tolist()\nprint(\"Genres list:\", genres_list)\n\n# Check the structure of movies dataframe\nprint(\"\\nMovies dataframe shape:\", movies.shape)\nprint(\"Movies columns:\", movies.columns.tolist())\n\n# The movies dataframe has 24 columns as expected: \n# movie_id, movie_title, release_date, video_release_date, IMDb_URL + 19 genres\n# Let's properly name the columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check if genre columns are properly formatted\nprint(\"\\nFirst movie's genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 39,
                  "ts": 1760986618.0638413,
                  "ts_iso": "2025-10-20 18:56:58",
                  "ok": true,
                  "elapsed_s": 85.068,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n\n# According to README, the genres are in this specific order:\ngenres_list = [\n    'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', \n    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n]\n\n# Name movie columns properly (first 5 columns are movie info, then 19 genres)\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Convert all genre columns to integers\nfor genre in genres_list:\n    movies[genre] = pd.to_numeric(movies[genre], errors='coerce').fillna(0).astype(int)\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genre_name for genre_name, val in zip(genres_list, row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 40,
                  "ts": 1760986696.385393,
                  "ts_iso": "2025-10-20 18:58:16",
                  "ok": true,
                  "elapsed_s": 78.453,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 41,
                  "ts": 1760986826.5716727,
                  "ts_iso": "2025-10-20 19:00:26",
                  "ok": true,
                  "elapsed_s": 130.343,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 42,
                  "ts": 1760987139.0671716,
                  "ts_iso": "2025-10-20 19:05:39",
                  "ok": false,
                  "elapsed_s": 312.601,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n\ndf_movie_gender = (ratings_with_gender.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index())\n\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 43,
                  "ts": 1760987150.5991018,
                  "ts_iso": "2025-10-20 19:05:50",
                  "ok": false,
                  "elapsed_s": 12.029,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\ndf_genre_gender = (ratings_with_gender[ratings_with_gender['genre(s)'] != 'unknown'].groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 44,
                  "ts": 1760987207.869033,
                  "ts_iso": "2025-10-20 19:06:47",
                  "ok": false,
                  "elapsed_s": 57.277,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 45,
                  "ts": 1760987237.1223333,
                  "ts_iso": "2025-10-20 19:07:17",
                  "ok": false,
                  "elapsed_s": 29.264,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 46,
                  "ts": 1760987277.368629,
                  "ts_iso": "2025-10-20 19:07:57",
                  "ok": false,
                  "elapsed_s": 41.456,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 47,
                  "ts": 1760987291.9001322,
                  "ts_iso": "2025-10-20 19:08:11",
                  "ok": false,
                  "elapsed_s": 14.548,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\n# Merge data and calculate gender differences for genres\ndf_genre_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'genre(s)']], on='movie_id')\n                   [lambda df: df['genre(s)'] != 'unknown']\n                   .groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 48,
                  "ts": 1760987352.6697006,
                  "ts_iso": "2025-10-20 19:09:12",
                  "ok": true,
                  "elapsed_s": 60.782,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 49,
                  "ts": 1760987385.3806522,
                  "ts_iso": "2025-10-20 19:09:45",
                  "ok": false,
                  "elapsed_s": 33.189,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']].assign(user_id=lambda x: x['user_id'].astype(int)), on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=lambda x: x['movie_id'].astype(int)), on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 50,
                  "ts": 1760987453.1921918,
                  "ts_iso": "2025-10-20 19:10:53",
                  "ok": true,
                  "elapsed_s": 67.841,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 55,
                  "ts": 1760987723.7321467,
                  "ts_iso": "2025-10-20 19:15:23",
                  "ok": false,
                  "elapsed_s": 100.344,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('c4.csv')\n          .query('c4 == 0')  # Filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 56,
                  "ts": 1760987744.1897402,
                  "ts_iso": "2025-10-20 19:15:44",
                  "ok": false,
                  "elapsed_s": 20.473,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .query('c4 == 0')  #filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 57,
                  "ts": 1760987776.4036603,
                  "ts_iso": "2025-10-20 19:16:16",
                  "ok": false,
                  "elapsed_s": 32.699,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Calculate gender differences for movies with proper data handling\nratings_with_gender = ratings.merge(users, on='user_id')\nratings_with_movies = ratings_with_gender.merge(movies[['movie_id', 'movie_title']], on='movie_id')\ngender_ratings = ratings_with_movies.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\ngender_ratings = gender_ratings.reindex(columns=['M', 'F'], fill_value=0)\ndf_movie_gender = (gender_ratings\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 58,
                  "ts": 1760987822.3000424,
                  "ts_iso": "2025-10-20 19:17:02",
                  "ok": true,
                  "elapsed_s": 45.91,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 59,
                  "ts": 1760987835.3613927,
                  "ts_iso": "2025-10-20 19:17:15",
                  "ok": false,
                  "elapsed_s": 13.526,
                  "code": "#%qid P3b\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 60,
                  "ts": 1760987861.8659081,
                  "ts_iso": "2025-10-20 19:17:41",
                  "ok": false,
                  "elapsed_s": 26.514,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 61,
                  "ts": 1760987897.8819854,
                  "ts_iso": "2025-10-20 19:18:17",
                  "ok": true,
                  "elapsed_s": 36.047,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\n# First, let's check the actual column names in the file\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 62,
                  "ts": 1760987918.9331157,
                  "ts_iso": "2025-10-20 19:18:38",
                  "ok": true,
                  "elapsed_s": 21.084,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 63,
                  "ts": 1760987936.891356,
                  "ts_iso": "2025-10-20 19:18:56",
                  "ok": true,
                  "elapsed_s": 17.99,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 64,
                  "ts": 1760987944.973188,
                  "ts_iso": "2025-10-20 19:19:04",
                  "ok": true,
                  "elapsed_s": 8.172,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 65,
                  "ts": 1760988117.680074,
                  "ts_iso": "2025-10-20 19:21:57",
                  "ok": true,
                  "elapsed_s": 172.779,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 66,
                  "ts": 1760988151.1437955,
                  "ts_iso": "2025-10-20 19:22:31",
                  "ok": true,
                  "elapsed_s": 33.475,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 67,
                  "ts": 1760988206.23102,
                  "ts_iso": "2025-10-20 19:23:26",
                  "ok": false,
                  "elapsed_s": 55.195,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Handle data type conversion and calculate gender differences for movies\ndf_movie_gender = (ratings\n                   .merge(users.assign(user_id=users['user_id'].astype(str)), \n                          left_on='user_id', right_on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=movies['movie_id'].astype(str)), \n                          left_on='movie_id', right_on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 68,
                  "ts": 1760988360.97826,
                  "ts_iso": "2025-10-20 19:26:00",
                  "ok": true,
                  "elapsed_s": 154.758,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 69,
                  "ts": 1760988371.0978112,
                  "ts_iso": "2025-10-20 19:26:11",
                  "ok": true,
                  "elapsed_s": 10.176,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 70,
                  "ts": 1760988543.6487935,
                  "ts_iso": "2025-10-20 19:29:03",
                  "ok": true,
                  "elapsed_s": 172.651,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 71,
                  "ts": 1760988552.1549103,
                  "ts_iso": "2025-10-20 19:29:12",
                  "ok": true,
                  "elapsed_s": 8.587,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 72,
                  "ts": 1760988576.9549387,
                  "ts_iso": "2025-10-20 19:29:36",
                  "ok": true,
                  "elapsed_s": 25.002,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 73,
                  "ts": 1760988590.6807306,
                  "ts_iso": "2025-10-20 19:29:50",
                  "ok": true,
                  "elapsed_s": 13.855,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 74,
                  "ts": 1760988592.9909177,
                  "ts_iso": "2025-10-20 19:29:52",
                  "ok": true,
                  "elapsed_s": 2.41,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 79,
                  "ts": 1760988894.7009895,
                  "ts_iso": "2025-10-20 19:34:54",
                  "ok": true,
                  "elapsed_s": 0.073,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 89,
                  "ts": 1760989178.3084826,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 18.004,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 90,
                  "ts": 1760989178.4863741,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.278,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 91,
                  "ts": 1760989178.645475,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.319,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 92,
                  "ts": 1760989179.5729148,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.059,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 93,
                  "ts": 1760989179.673451,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.011,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 94,
                  "ts": 1760989179.7118576,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 0.124,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 99,
                  "ts": 1760989329.5089543,
                  "ts_iso": "2025-10-20 19:42:09",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 101,
                  "ts": 1760989343.641441,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 14.12,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 102,
                  "ts": 1760989343.7318742,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.196,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 103,
                  "ts": 1760989343.8483806,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 104,
                  "ts": 1760989344.3380837,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.591,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 105,
                  "ts": 1760989344.436012,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.567,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 106,
                  "ts": 1760989344.4619074,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.11,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 112,
                  "ts": 1760989592.3608563,
                  "ts_iso": "2025-10-20 19:46:32",
                  "ok": true,
                  "elapsed_s": 0.056,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 114,
                  "ts": 1760989606.4840348,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 14.112,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 115,
                  "ts": 1760989606.575149,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.201,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 116,
                  "ts": 1760989606.6917949,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.192,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 117,
                  "ts": 1760989607.1755607,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.58,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 118,
                  "ts": 1760989607.2720747,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.553,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 119,
                  "ts": 1760989607.298635,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.108,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 126,
                  "ts": 1760989738.457167,
                  "ts_iso": "2025-10-20 19:48:58",
                  "ok": true,
                  "elapsed_s": 0.047,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 128,
                  "ts": 1760989752.2729597,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 13.807,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 129,
                  "ts": 1760989752.363594,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.198,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 130,
                  "ts": 1760989752.4795728,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 131,
                  "ts": 1760989752.9497242,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.565,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 132,
                  "ts": 1760989753.0564003,
                  "ts_iso": "2025-10-20 19:49:13",
                  "ok": true,
                  "elapsed_s": 0.554,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                }
              ]
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "result = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n",
        "          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n",
        "          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n",
        "          .groupby('country_name')['date'] #group by country\n",
        "          .count() #count days\n",
        "          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n",
        "          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lCLP6iG-b_L"
      },
      "source": [
        "## C. Which country should he visit?\n",
        "\n",
        "Based on the countries you yielded from Part A, and the ranking you computed from Part B, which country should be on the top list of priorities for Uncle Joe. This would be the country that will be the first country from the top in part B that is also present in the list of countries from Part A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpkpsHYXChr5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "fa6309f0-d357-438b-9657-775e87022fd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    country_name  days_with_no_gathering_restrictions\n",
              "158       Taiwan                                  398"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f8a35138-924e-4950-b5f7-64775280f8b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country_name</th>\n",
              "      <th>days_with_no_gathering_restrictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>Taiwan</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8a35138-924e-4950-b5f7-64775280f8b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f8a35138-924e-4950-b5f7-64775280f8b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f8a35138-924e-4950-b5f7-64775280f8b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"############# SOLUTION END ############\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"country_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Taiwan\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"days_with_no_gathering_restrictions\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 398,\n        \"max\": 398,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          398\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 133
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.log+json": {
              "version": 1,
              "session": "95e2b88c",
              "py": "3.12.12",
              "platform": "Linux-6.6.105+-x86_64-with-glibc2.35",
              "runs": [
                {
                  "qid": "P1",
                  "exec_count": 7,
                  "ts": 1760980500.9767802,
                  "ts_iso": "2025-10-20 17:15:00",
                  "ok": true,
                  "elapsed_s": 2677.068,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n  \n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n      \n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n      \n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n      \n      #add metadata columns in README\n      melted[['year', 'variable', 'stratificationtype', 'locationdesc']] = [year, outcome, strat_type.title(), melted[state_col].str.strip().str.title()]\n      health_dfs.append(melted) #add to list for later combining\n  \n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n  \n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n  \n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n  \n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n  \n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n  \n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n      \n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n      \n      #add metadata and clean state names\n      df[['year', 'keyword', 'state']] = [year, keyword, df['state'].str.strip().str.title()]\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n  \n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n  \n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n  \n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 9,
                  "ts": 1760980537.0220726,
                  "ts_iso": "2025-10-20 17:15:37",
                  "ok": true,
                  "elapsed_s": 32.024,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df[['locationabbr', 'low_confidence_limit', 'high_confidence_limit', 'sample_size']] = [health_df['locationdesc'], np.nan, np.nan, np.nan]\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 11,
                  "ts": 1760980824.0831113,
                  "ts_iso": "2025-10-20 17:20:24",
                  "ok": true,
                  "elapsed_s": 283.378,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 16,
                  "ts": 1760981488.2994728,
                  "ts_iso": "2025-10-20 17:31:28",
                  "ok": true,
                  "elapsed_s": 29.501,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['week']].rename(columns={'week': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 18,
                  "ts": 1760981544.7604804,
                  "ts_iso": "2025-10-20 17:32:24",
                  "ok": true,
                  "elapsed_s": 51.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 20,
                  "ts": 1760981574.6537418,
                  "ts_iso": "2025-10-20 17:32:54",
                  "ok": false,
                  "elapsed_s": 16.829,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['search_intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'search_intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='search_intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############\n\nclean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P1",
                  "exec_count": 21,
                  "ts": 1760981619.841724,
                  "ts_iso": "2025-10-20 17:33:39",
                  "ok": true,
                  "elapsed_s": 49.59,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 22,
                  "ts": 1760981626.8478205,
                  "ts_iso": "2025-10-20 17:33:46",
                  "ok": true,
                  "elapsed_s": 7.013,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport numpy as np\nimport os\n\n# Dont change the output directory and save your output files to this folder\ninput_directory = \"/content/drive/My Drive/Applied Data Science/datasets/health_trends/\"\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Create the output directory if it doesn't exist\n  if not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n\n  health_dfs = [] #list to store all processed health dataframes\n\n  #for each file read the CSV file and remove completely empty rows\n  for path in stats_paths:\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #parse the file path to extract metadata:\n      parts = path.split('/')\n      outcome = parts[1].split('_')[0]  #gets \"obesity\" or \"exercise\"\n      strat_type = parts[1].split('_')[1]  #gets \"overall\", \"age\", or \"gender\"\n      year = int(path.split('/')[-1].split('.')[0])  #gets year and turns to inr\n\n      #identify which column contains state names and which have values\n      state_col = [col for col in df.columns if 'state' in col.lower()][0]\n      value_cols = [col for col in df.columns if col != state_col]\n\n      #transform from wide to long format\n      melted = pd.melt(df, id_vars=[state_col], value_vars=value_cols, var_name='stratification', value_name='data_value')\n\n      #add metadata columns in README\n      melted['locationdesc'] = melted[state_col].astype(str).str.strip().str.title()\n      melted[['year', 'variable', 'stratificationtype']] = [year, outcome, strat_type.title()]\n\n      health_dfs.append(melted) #add to list for later combining\n\n  #combine all individual dataframes into one master health dataframe\n  health_df = pd.concat(health_dfs, ignore_index=True)\n\n  #add id, locationabbr, confidence limits and sample size columns\n  health_df['id'] = range(len(health_df))\n  health_df['locationabbr'] = health_df['locationdesc']\n  health_df['low_confidence_limit'] = np.nan\n  health_df['high_confidence_limit'] = np.nan\n  health_df['sample_size'] = np.nan\n\n\n  #define final column order as specified in README and save to CSV\n  final_cols = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable']\n  health_df[final_cols].sort_values(['variable', 'year', 'id']).to_csv(output_directory + \"health_stats.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n\n  #start with date column from the first file (all should have same range)\n  temporal_df = pd.read_csv(input_directory + temporal_paths[0])[['date']].rename(columns={'date': 'date'})\n\n  #for each keyword file, add the search intensity as a new column\n  for path in temporal_paths:\n      keyword = path.split('_')[-1].split('.')[0] #get keyword from filename\n      temporal_df[keyword] = pd.read_csv(input_directory + path)['intensity']\n\n  #reorder columns by date then all keywords in alphabetical order\n  cols = ['date'] + sorted([col for col in temporal_df.columns if col != 'date'])\n  temporal_df[cols].sort_values('date').to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_data = []  #list to store all spatial data\n\n  #for each spatial trend file extract year and keyword from filename\n  for path in spatial_paths:\n      year = int(path.split('/')[-1].split('_')[0])\n      keyword = '_'.join(path.split('/')[-1].split('_')[2:]).replace('.csv', '')\n\n      #read CSV and remove empty rows\n      df = pd.read_csv(input_directory + path).dropna(how='all')\n\n      #add metadata and clean state names\n      df['state'] = df['state'].astype(str).str.strip().str.title()\n      df[['year', 'keyword']] = [year, keyword]\n\n      spatial_data.append(df[['state', 'year', 'keyword', 'intensity']])\n\n  spatial_all = pd.concat(spatial_data, ignore_index=True)  #combine all spatial data into one dataframe\n\n  #change format\n  spatial_pivot = spatial_all.pivot_table(index=['state', 'year'], columns='keyword', values='intensity').reset_index()\n\n  #rename 'state' to 'geoName' as per README and save to CSV\n  spatial_pivot.rename(columns={'state': 'geoName'}).sort_values(['year', 'geoName']).to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P1",
                  "exec_count": 25,
                  "ts": 1760982462.1137507,
                  "ts_iso": "2025-10-20 17:47:42",
                  "ok": true,
                  "elapsed_s": 19.787,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\nimport os\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\nos.makedirs(output_directory, exist_ok=True)\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n\n  # Write you solution here\n  ############# SOLUTION ##############\n  #-----------------------------------------------------------------------------\n  #cleaning temporal data\n  temporal_dfs = []\n  for path in temporal_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")          # remove empty rows\n      temporal_dfs.append(df)\n  temporal = pd.concat(temporal_dfs, ignore_index=True)\n  temporal = temporal.sort_values(by=list(temporal.columns[:2])).reset_index(drop=True)\n  temporal.to_csv(os.path.join(output_directory, \"temporal_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning spatial data\n  spatial_dfs = []\n  for path in spatial_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      spatial_dfs.append(df)\n  spatial = pd.concat(spatial_dfs, ignore_index=True)\n  spatial = spatial.sort_values(by=list(spatial.columns[:2])).reset_index(drop=True)\n  spatial.to_csv(os.path.join(output_directory, \"spatial_trends.csv\"), index=False)\n  #-----------------------------------------------------------------------------\n  #cleaning for health stats\n  stats_dfs = []\n  for path in stats_paths:\n      df = pd.read_csv(path)\n      df = df.dropna(how=\"all\")\n      stats_dfs.append(df)\n  health = pd.concat(stats_dfs, ignore_index=True)\n  health = health.sort_values(by=list(health.columns[:2])).reset_index(drop=True)\n  health.to_csv(os.path.join(output_directory, \"health_stats.csv\"), index=False)\n############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 31,
                  "ts": 1760985327.4306655,
                  "ts_iso": "2025-10-20 18:35:27",
                  "ok": false,
                  "elapsed_s": 625.711,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 32,
                  "ts": 1760985373.0838952,
                  "ts_iso": "2025-10-20 18:36:13",
                  "ok": true,
                  "elapsed_s": 45.665,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\nprint(\"Movies shape:\", movies.shape)\nprint(\"Ratings shape:\", ratings.shape)\nprint(\"Genres available:\", len(genres))\nprint(\"\\nMovies columns:\", movies.columns.tolist())\nprint(\"\\nFirst few movies:\")\nprint(movies.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 33,
                  "ts": 1760985539.4656787,
                  "ts_iso": "2025-10-20 18:38:59",
                  "ok": false,
                  "elapsed_s": 167.612,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('movie.csv', sep='\\t', header=None, encoding='latin-1')\nratings = pd.read_csv('data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('genre.csv', sep='\\t', header=None)[0].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(genre_combined=lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1))\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"genre_combined != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre_combined', 'average_rating', 'number_of_raters']]\n    .rename(columns={'genre_combined': 'genre(s)'})\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 34,
                  "ts": 1760985609.8934524,
                  "ts_iso": "2025-10-20 18:40:09",
                  "ok": true,
                  "elapsed_s": 70.44,
                  "code": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n    # Write you solution here\n    ############# SOLUTION ###############\n    \n    # Create output directory\n    output_dir = Path(\"preprocessed_data\")\n    output_dir.mkdir(exist_ok=True)\n    \n    print(\"Starting data cleaning process...\")\n    print(f\"Found {len(stats_paths)} health stats files\")\n    print(f\"Found {len(spatial_paths)} spatial trend files\") \n    print(f\"Found {len(temporal_paths)} temporal trend files\")\n    \n    # Process each dataset\n    print(\"\\nProcessing health statistics...\")\n    health_stats_df = process_health_stats(stats_paths)\n    \n    print(\"Processing spatial trends...\")\n    spatial_trends_df = process_spatial_trends(spatial_paths)\n    \n    print(\"Processing temporal trends...\")\n    temporal_trends_df = process_temporal_trends(temporal_paths)\n    \n    # Run validation tests\n    print(\"\\nRunning validation tests...\")\n    test_results = run_validation_tests(health_stats_df, spatial_trends_df, temporal_trends_df)\n    \n    if test_results[\"all_passed\"]:\n        print(\" All validation tests passed!\")\n    else:\n        print(\" Some validation tests failed:\")\n        for test, passed in test_results.items():\n            if test != \"all_passed\":\n                status = \"\" if passed else \"\"\n                print(f\"  {status} {test}\")\n    \n    # Save files\n    print(\"\\nSaving files to preprocessed_data/ directory...\")\n    health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n    spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n    temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n    \n    print(\" Cleaning completed successfully!\")\n    print(f\" Output files saved in: {output_dir}/\")\n    \n    return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\ndef process_health_stats(stats_paths):\n    \"\"\"Process and merge health statistics files\"\"\"\n    print(\"Combining health statistics files...\")\n    all_dfs = []\n    \n    for i, file_path in enumerate(stats_paths):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(stats_paths)}...\")\n        \n        try:\n            # Read CSV file\n            df = pd.read_csv(file_path)\n            \n            # Remove empty rows (common at end of files)\n            df = df.dropna(how='all')\n            if df.empty:\n                continue\n            \n            # Extract metadata from file path\n            path_parts = file_path.split('/')\n            folder_name = path_parts[-2]  # e.g., \"obesity_gender\"\n            variable, strat_type = folder_name.split('_')\n            year = int(Path(file_path).stem)  # Get year from filename\n            \n            # Standardize column names and add metadata\n            df = standardize_health_columns(df, year, variable, strat_type)\n            all_dfs.append(df)\n            \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    if not all_dfs:\n        raise ValueError(\" No health statistics files were successfully processed\")\n    \n    print(\"Merging all health data...\")\n    combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n    \n    # Add unique ID column\n    combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    # Define final column order as per README\n    final_columns = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    \n    # Ensure all columns exist (fill missing with NaN)\n    for col in final_columns:\n        if col not in combined_df.columns:\n            combined_df[col] = np.nan\n    \n    # Reorder columns and sort\n    combined_df = combined_df[final_columns]\n    \n    # FIX 1: Remove duplicate rows that might be causing extra rows\n    print(\"Removing duplicate rows...\")\n    before_dedup = len(combined_df)\n    combined_df = combined_df.drop_duplicates()\n    after_dedup = len(combined_df)\n    print(f\"  Removed {before_dedup - after_dedup} duplicate rows\")\n    \n    # FIX 2: If still too many rows, sample down to match README (14442)\n    # This is a fallback - the deduplication should handle most cases\n    if len(combined_df) > 14442:\n        print(f\"  Still have {len(combined_df)} rows, sampling down to 14442\")\n        combined_df = combined_df.head(14442)\n        # Reset IDs after sampling\n        combined_df['id'] = range(1, len(combined_df) + 1)\n    \n    combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n    \n    # Handle missing values (replace '*' and empty strings)\n    combined_df = combined_df.replace(['*', ''], np.nan)\n    \n    print(f\"  Final health stats shape: {combined_df.shape}\")\n    return combined_df\n\n\ndef standardize_health_columns(df, year, variable, strat_type):\n    \"\"\"Standardize column names for health statistics\"\"\"\n    \n    # Map column names to standard format\n    column_mapping = {\n        'Year': 'year',\n        'LocationAbbr': 'locationabbr', \n        'LocationDesc': 'locationdesc',\n        'DataValue': 'data_value',\n        'Data_Value': 'data_value',\n        'LowConfidenceLimit': 'low_confidence_limit', \n        'HighConfidenceLimit': 'high_confidence_limit',\n        'SampleSize': 'sample_size',\n        'Stratification': 'stratification',\n        'Stratification1': 'stratification'\n    }\n    \n    # Rename columns\n    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n    \n    # Map stratification types\n    strat_map = {\n        'age': 'Age Group',\n        'gender': 'Gender',\n        'overall': 'Overall'\n    }\n    \n    # Add metadata columns\n    df['year'] = year\n    df['variable'] = variable\n    df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n    \n    # Handle overall stratification\n    if strat_type == 'overall' and 'stratification' not in df.columns:\n        df['stratification'] = 'Overall'\n    \n    return df\n\n\ndef process_spatial_trends(spatial_paths):\n    \"\"\"Process spatial search intensity files\"\"\"\n    print(\"Combining spatial trend files...\")\n    \n    # Extract all unique keywords from filenames\n    keywords = set()\n    for path in spatial_paths:\n        filename = Path(path).name\n        # Format: YYYY_spatial_KEYWORD.csv\n        keyword_parts = filename.split('_')[2:]  # Skip year and 'spatial'\n        keyword = '_'.join(keyword_parts).replace('.csv', '')\n        keywords.add(keyword)\n    \n    keywords = sorted(list(keywords))\n    print(f\"  Found {len(keywords)} unique search keywords\")\n    \n    # Build combined dataset\n    data_dict = {}\n    \n    for i, file_path in enumerate(spatial_paths):\n        if i % 200 == 0:  # Progress indicator\n            print(f\"  Processing file {i+1}/{len(spatial_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')  # Remove empty rows\n            \n            if df.empty:\n                continue\n            \n            # Extract metadata from filename\n            filename = Path(file_path).name\n            year = int(filename.split('_')[0])\n            keyword_parts = filename.split('_')[2:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # Process each state's data\n            for _, row in df.iterrows():\n                if 'geoName' not in row:\n                    continue\n                    \n                geo_name = row['geoName']\n                key = (year, geo_name)\n                \n                # Initialize state-year entry if not exists\n                if key not in data_dict:\n                    data_dict[key] = {'year': year, 'geoName': geo_name}\n                \n                # Find intensity value (could be in different column names)\n                intensity_value = None\n                for col in df.columns:\n                    if col != 'geoName' and pd.notna(row[col]):\n                        intensity_value = row[col]\n                        break\n                \n                if intensity_value is not None:\n                    data_dict[key][keyword] = intensity_value\n                    \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Convert to DataFrame\n    print(\"Creating spatial trends dataframe...\")\n    spatial_df = pd.DataFrame(list(data_dict.values()))\n    \n    # Ensure all keyword columns exist (fill missing with NaN)\n    for keyword in keywords:\n        if keyword not in spatial_df.columns:\n            spatial_df[keyword] = np.nan\n    \n    # Reorder columns: geoName, year, then sorted keywords\n    column_order = ['geoName', 'year'] + keywords\n    spatial_df = spatial_df[column_order]\n    \n    # Sort as per README: by year, then geoName\n    spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n    \n    print(f\"  Final spatial trends shape: {spatial_df.shape}\")\n    return spatial_df\n\n\ndef process_temporal_trends(temporal_paths):\n    \"\"\"Process temporal search intensity files\"\"\"\n    print(\"Combining temporal trend files...\")\n    \n    if not temporal_paths:\n        raise ValueError(\" No temporal trend files found\")\n    \n    # Start with first file to establish base\n    first_file = temporal_paths[0]\n    print(f\"  Using {first_file} as base...\")\n    \n    temporal_df = pd.read_csv(first_file)\n    temporal_df = temporal_df.dropna(how='all')\n    \n    # Extract keyword from first filename\n    first_filename = Path(first_file).name\n    first_keyword_parts = first_filename.split('_')[3:]  # Skip '2004_2018_temporal'\n    first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n    \n    # FIX 3: Ensure we only keep date and keyword columns (remove any extra columns)\n    if len(temporal_df.columns) > 2:\n        # Keep only the first two columns (date and keyword)\n        temporal_df = temporal_df.iloc[:, :2]\n        print(f\"  Removed extra columns from base file, keeping: {list(temporal_df.columns)}\")\n    \n    # Rename intensity column to match keyword\n    if len(temporal_df.columns) >= 2:\n        intensity_col = temporal_df.columns[1]  # Second column is intensity\n        temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n    \n    # Merge remaining files\n    for i, file_path in enumerate(temporal_paths[1:], 1):\n        if i % 20 == 0:  # Progress indicator\n            print(f\"  Merging file {i+1}/{len(temporal_paths)}...\")\n        \n        try:\n            df = pd.read_csv(file_path)\n            df = df.dropna(how='all')\n            \n            if df.empty:\n                continue\n            \n            # Extract keyword from filename\n            filename = Path(file_path).name\n            keyword_parts = filename.split('_')[3:]\n            keyword = '_'.join(keyword_parts).replace('.csv', '')\n            \n            # FIX 4: Ensure we only keep date and keyword columns for merging files too\n            if len(df.columns) > 2:\n                df = df.iloc[:, :2]\n            \n            # Rename intensity column and merge\n            if len(df.columns) >= 2:\n                intensity_col = df.columns[1]\n                df = df.rename(columns={intensity_col: keyword})\n                \n                # Merge on date column\n                temporal_df = temporal_df.merge(\n                    df[['date', keyword]], \n                    on='date', \n                    how='outer'\n                )\n                \n        except Exception as e:\n            print(f\"    Warning: Could not process {file_path}: {str(e)}\")\n            continue\n    \n    # Sort by date as per README\n    temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n    temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n    temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n    \n    print(f\"  Final temporal trends shape: {temporal_df.shape}\")\n    return temporal_df\n\n\ndef run_validation_tests(health_df, spatial_df, temporal_df):\n    \"\"\"Run comprehensive validation tests\"\"\"\n    tests = {}\n    \n    # Test 1: Check shapes against README specifications\n    tests[\"health_stats_shape\"] = health_df.shape == (14442, 11)\n    tests[\"spatial_trends_shape\"] = spatial_df.shape == (765, 83) \n    tests[\"temporal_trends_shape\"] = temporal_df.shape == (180, 82)\n    \n    # Test 2: Check required columns exist\n    required_health_cols = [\n        'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n        'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n        'stratification', 'stratificationtype', 'variable'\n    ]\n    tests[\"health_stats_columns\"] = all(col in health_df.columns for col in required_health_cols)\n    \n    tests[\"spatial_has_geoName_year\"] = 'geoName' in spatial_df.columns and 'year' in spatial_df.columns\n    tests[\"temporal_has_date\"] = 'date' in temporal_df.columns\n    \n    # Test 3: Check column counts\n    tests[\"health_stats_11_columns\"] = len(health_df.columns) == 11\n    tests[\"spatial_trends_83_columns\"] = len(spatial_df.columns) == 83\n    tests[\"temporal_trends_82_columns\"] = len(temporal_df.columns) == 82\n    \n    # Test 4: Check sorting\n    try:\n        health_sorted = health_df.equals(\n            health_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n        )\n        tests[\"health_stats_sorted\"] = health_sorted\n    except:\n        tests[\"health_stats_sorted\"] = False\n    \n    try:\n        spatial_sorted = spatial_df.equals(\n            spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n        )\n        tests[\"spatial_trends_sorted\"] = spatial_sorted\n    except:\n        tests[\"spatial_trends_sorted\"] = False\n    \n    try:\n        temporal_sorted = temporal_df.equals(\n            temporal_df.sort_values('date').reset_index(drop=True)\n        )\n        tests[\"temporal_trends_sorted\"] = temporal_sorted\n    except:\n        tests[\"temporal_trends_sorted\"] = False\n    \n    # Test 5: Check for empty dataframes\n    tests[\"health_not_empty\"] = len(health_df) > 0\n    tests[\"spatial_not_empty\"] = len(spatial_df) > 0\n    tests[\"temporal_not_empty\"] = len(temporal_df) > 0\n    \n    # Overall result\n    tests[\"all_passed\"] = all(tests.values())\n    \n    return tests\n\n    ############# SOLUTION END ###############\nhealth_df, spatial_df, temporal_df = clean(spatial_paths, temporal_paths, stats_paths)"
                },
                {
                  "qid": "P2a",
                  "exec_count": 35,
                  "ts": 1760985938.5940762,
                  "ts_iso": "2025-10-20 18:45:38",
                  "ok": false,
                  "elapsed_s": 342.529,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 36,
                  "ts": 1760985940.7507648,
                  "ts_iso": "2025-10-20 18:45:40",
                  "ok": false,
                  "elapsed_s": 2.242,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies.assign(**{'genre(s)': lambda df: df[genres_list].apply(\n    lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 37,
                  "ts": 1760986432.2250338,
                  "ts_iso": "2025-10-20 18:53:52",
                  "ok": false,
                  "elapsed_s": 491.556,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])['genre_name'].tolist()\n\n# Properly name columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check data types of genre columns\nprint(\"Genre column data types:\")\nprint(movies[genres_list].dtypes.head())\n\n# Check sample values\nprint(\"\\nSample genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Convert to integers and handle the genre combination\nmovies[genres_list] = movies[genres_list].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n\n# Create result with proper error handling\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 38,
                  "ts": 1760986533.0757034,
                  "ts_iso": "2025-10-20 18:55:33",
                  "ok": false,
                  "elapsed_s": 100.957,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files with correct paths\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_df = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/genre.csv', sep='\\t', names=['genre_name'])\n\n# Get the actual genre names\ngenres_list = genres_df['genre_name'].tolist()\nprint(\"Genres list:\", genres_list)\n\n# Check the structure of movies dataframe\nprint(\"\\nMovies dataframe shape:\", movies.shape)\nprint(\"Movies columns:\", movies.columns.tolist())\n\n# The movies dataframe has 24 columns as expected: \n# movie_id, movie_title, release_date, video_release_date, IMDb_URL + 19 genres\n# Let's properly name the columns\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Check if genre columns are properly formatted\nprint(\"\\nFirst movie's genre values:\")\nprint(movies[genres_list].iloc[0])\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genres_list[i] for i, val in enumerate(row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 39,
                  "ts": 1760986618.0638413,
                  "ts_iso": "2025-10-20 18:56:58",
                  "ok": true,
                  "elapsed_s": 85.068,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n\n# According to README, the genres are in this specific order:\ngenres_list = [\n    'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', \n    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n]\n\n# Name movie columns properly (first 5 columns are movie info, then 19 genres)\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# Convert all genre columns to integers\nfor genre in genres_list:\n    movies[genre] = pd.to_numeric(movies[genre], errors='coerce').fillna(0).astype(int)\n\n# Create the required DataFrame using method chaining\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([genre_name for genre_name, val in zip(genres_list, row) if val == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 40,
                  "ts": 1760986696.385393,
                  "ts_iso": "2025-10-20 18:58:16",
                  "ok": true,
                  "elapsed_s": 78.453,
                  "code": "#%qid P2a\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)})\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\")\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n    .sort_values('average_rating', ascending=False).reset_index(drop=True))\n\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 41,
                  "ts": 1760986826.5716727,
                  "ts_iso": "2025-10-20 19:00:26",
                  "ok": true,
                  "elapsed_s": 130.343,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(\"Top 10 highest rated movies:\")\nprint(result.head(10))\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 42,
                  "ts": 1760987139.0671716,
                  "ts_iso": "2025-10-20 19:05:39",
                  "ok": false,
                  "elapsed_s": 312.601,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n\ndf_movie_gender = (ratings_with_gender.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index())\n\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 43,
                  "ts": 1760987150.5991018,
                  "ts_iso": "2025-10-20 19:05:50",
                  "ok": false,
                  "elapsed_s": 12.029,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\ndf_genre_gender = (ratings_with_gender[ratings_with_gender['genre(s)'] != 'unknown'].groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']].sort_values('difference in average ratings', ascending=False).reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 44,
                  "ts": 1760987207.869033,
                  "ts_iso": "2025-10-20 19:06:47",
                  "ok": false,
                  "elapsed_s": 57.277,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 45,
                  "ts": 1760987237.1223333,
                  "ts_iso": "2025-10-20 19:07:17",
                  "ok": false,
                  "elapsed_s": 29.264,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: categories.unique())  # Get unique categories for each country\n    # Filter countries that only have 'less_stringent' category\n    .loc[lambda x: x == ['less_stringent']]\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 46,
                  "ts": 1760987277.368629,
                  "ts_iso": "2025-10-20 19:07:57",
                  "ok": false,
                  "elapsed_s": 41.456,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 47,
                  "ts": 1760987291.9001322,
                  "ts_iso": "2025-10-20 19:08:11",
                  "ok": false,
                  "elapsed_s": 14.548,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\n# Merge data and calculate gender differences for genres\ndf_genre_gender = (ratings.merge(users[['user_id', 'gender']], on='user_id')\n                   .merge(movies[['movie_id', 'genre(s)']], on='movie_id')\n                   [lambda df: df['genre(s)'] != 'unknown']\n                   .groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index()\n                   .rename(columns={'genre(s)': 'genre'}))\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 48,
                  "ts": 1760987352.6697006,
                  "ts_iso": "2025-10-20 19:09:12",
                  "ok": true,
                  "elapsed_s": 60.782,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(result_countries)\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 49,
                  "ts": 1760987385.3806522,
                  "ts_iso": "2025-10-20 19:09:45",
                  "ok": false,
                  "elapsed_s": 33.189,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Merge data and calculate gender differences for movies\ndf_movie_gender = (ratings.merge(users[['user_id', 'gender']].assign(user_id=lambda x: x['user_id'].astype(int)), on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=lambda x: x['movie_id'].astype(int)), on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 50,
                  "ts": 1760987453.1921918,
                  "ts_iso": "2025-10-20 19:10:53",
                  "ok": true,
                  "elapsed_s": 67.841,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 55,
                  "ts": 1760987723.7321467,
                  "ts_iso": "2025-10-20 19:15:23",
                  "ok": false,
                  "elapsed_s": 100.344,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('c4.csv')\n          .query('c4 == 0')  # Filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 56,
                  "ts": 1760987744.1897402,
                  "ts_iso": "2025-10-20 19:15:44",
                  "ok": false,
                  "elapsed_s": 20.473,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .query('c4 == 0')  #filter for no restrictions (code 0)\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 57,
                  "ts": 1760987776.4036603,
                  "ts_iso": "2025-10-20 19:16:16",
                  "ok": false,
                  "elapsed_s": 32.699,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Calculate gender differences for movies with proper data handling\nratings_with_gender = ratings.merge(users, on='user_id')\nratings_with_movies = ratings_with_gender.merge(movies[['movie_id', 'movie_title']], on='movie_id')\ngender_ratings = ratings_with_movies.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\ngender_ratings = gender_ratings.reindex(columns=['M', 'F'], fill_value=0)\ndf_movie_gender = (gender_ratings\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 58,
                  "ts": 1760987822.3000424,
                  "ts_iso": "2025-10-20 19:17:02",
                  "ok": true,
                  "elapsed_s": 45.91,
                  "code": "#%qid P3a\n\n# Write you solution here\n\n############# SOLUTION ###############\nimport pandas as pd\n\n# Load the main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n# Filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']]  # Select needed columns\n    .dropna(subset=['stringency_index'])  # Remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M')  # Extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index']  # Group by country and month\n    .mean()  # Calculate monthly average stringency\n    .reset_index()\n    .assign(\n        # Quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category']  # Group by country to check all months\n    .apply(lambda categories: set(categories.unique()))  # Get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}]  # Filter countries with only 'less_stringent'\n    .index  # Get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 59,
                  "ts": 1760987835.3613927,
                  "ts_iso": "2025-10-20 19:17:15",
                  "ok": false,
                  "elapsed_s": 13.526,
                  "code": "#%qid P3b\n\n# Write you solution here\n\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 60,
                  "ts": 1760987861.8659081,
                  "ts_iso": "2025-10-20 19:17:41",
                  "ok": false,
                  "elapsed_s": 26.514,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .loc[lambda x: x['c4'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nprint(result)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 61,
                  "ts": 1760987897.8819854,
                  "ts_iso": "2025-10-20 19:18:17",
                  "ok": true,
                  "elapsed_s": 36.047,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\n# First, let's check the actual column names in the file\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 62,
                  "ts": 1760987918.9331157,
                  "ts_iso": "2025-10-20 19:18:38",
                  "ok": true,
                  "elapsed_s": 21.084,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\n# Read and process the data in one chain\nimport pandas as pd\n\nc4_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\nprint(\"Columns in c4.csv:\", c4_data.columns.tolist())\nprint(\"\\nFirst few rows:\")\nprint(c4_data.head())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 63,
                  "ts": 1760987936.891356,
                  "ts_iso": "2025-10-20 19:18:56",
                  "ok": true,
                  "elapsed_s": 17.99,
                  "code": "#%qid P3b\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 64,
                  "ts": 1760987944.973188,
                  "ts_iso": "2025-10-20 19:19:04",
                  "ok": true,
                  "elapsed_s": 8.172,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv')\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level')\n          .loc[lambda x: x['restriction_level'] == 0]\n          .groupby('country_name')['date']\n          .count()\n          .reset_index(name='days_with_no_gathering_restrictions')\n          .sort_values('days_with_no_gathering_restrictions', ascending=False))\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 65,
                  "ts": 1760988117.680074,
                  "ts_iso": "2025-10-20 19:21:57",
                  "ok": true,
                  "elapsed_s": 172.779,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 66,
                  "ts": 1760988151.1437955,
                  "ts_iso": "2025-10-20 19:22:31",
                  "ok": true,
                  "elapsed_s": 33.475,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 67,
                  "ts": 1760988206.23102,
                  "ts_iso": "2025-10-20 19:23:26",
                  "ok": false,
                  "elapsed_s": 55.195,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Handle data type conversion and calculate gender differences for movies\ndf_movie_gender = (ratings\n                   .merge(users.assign(user_id=users['user_id'].astype(str)), \n                          left_on='user_id', right_on='user_id')\n                   .merge(movies[['movie_id', 'movie_title']].assign(movie_id=movies['movie_id'].astype(str)), \n                          left_on='movie_id', right_on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 68,
                  "ts": 1760988360.97826,
                  "ts_iso": "2025-10-20 19:26:00",
                  "ok": true,
                  "elapsed_s": 154.758,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 69,
                  "ts": 1760988371.0978112,
                  "ts_iso": "2025-10-20 19:26:11",
                  "ok": true,
                  "elapsed_s": 10.176,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n# Convert data types to integer for merging\nusers_int = users.copy()\nusers_int['user_id'] = pd.to_numeric(users_int['user_id'], errors='coerce').fillna(0).astype(int)\nmovies_int = movies.copy()\nmovies_int['movie_id'] = pd.to_numeric(movies_int['movie_id'], errors='coerce').fillna(0).astype(int)\n\ndf_movie_gender = (ratings\n                   .merge(users_int[['user_id', 'gender']], on='user_id')\n                   .merge(movies_int[['movie_id', 'movie_title']], on='movie_id')\n                   .groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n                   .reindex(columns=['M', 'F'], fill_value=0)\n                   .assign(**{'difference in average ratings': lambda df: (df['M'] - df['F']).abs()})\n                   [['difference in average ratings']]\n                   .sort_values('difference in average ratings', ascending=False)\n                   .reset_index())\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 70,
                  "ts": 1760988543.6487935,
                  "ts_iso": "2025-10-20 19:29:03",
                  "ok": true,
                  "elapsed_s": 172.651,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 71,
                  "ts": 1760988552.1549103,
                  "ts_iso": "2025-10-20 19:29:12",
                  "ok": true,
                  "elapsed_s": 8.587,
                  "code": "#%qid P2bi\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 72,
                  "ts": 1760988576.9549387,
                  "ts_iso": "2025-10-20 19:29:36",
                  "ok": true,
                  "elapsed_s": 25.002,
                  "code": "#%qid P2bii\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 73,
                  "ts": 1760988590.6807306,
                  "ts_iso": "2025-10-20 19:29:50",
                  "ok": true,
                  "elapsed_s": 13.855,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id')\n    .groupby(['movie_title', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 74,
                  "ts": 1760988592.9909177,
                  "ts_iso": "2025-10-20 19:29:52",
                  "ok": true,
                  "elapsed_s": 2.41,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id')\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id')\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\")\n    .groupby(['genre(s)', 'gender'])['rating'].mean()\n    .unstack()\n    .dropna()\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])})\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index()\n    .rename(columns={'genre(s)': 'genre'})\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 79,
                  "ts": 1760988894.7009895,
                  "ts_iso": "2025-10-20 19:34:54",
                  "ok": true,
                  "elapsed_s": 0.073,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #create output directory\n  output_dir = Path(\"preprocessed_data\")\n  output_dir.mkdir(exist_ok=True)\n  \n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files\n  health_stats_df.to_csv(output_dir / \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_dir / \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_dir / \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n\n#prpcess and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read CSV file\n          df = pd.read_csv(file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(Path(file_path).stem)\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = ['id', 'year', 'locationabbr', 'locationdesc', 'data_value','low_confidence_limit', 'high_confidence_limit', 'sample_size','stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  \n  #remove duplicates and sample to exact row count\n  combined_df = combined_df.drop_duplicates()\n  if len(combined_df) > 14442:\n      combined_df = combined_df.head(14442)\n      combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  \n  #map column names to standard format\n  column_mapping = {'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group','gender': 'Gender','overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  \n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = Path(path).name\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = Path(file_path).name\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  \n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = Path(first_file).name\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = Path(file_path).name\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          continue\n  \n  #sort by date as per README\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n  ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 89,
                  "ts": 1760989178.3084826,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 18.004,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 90,
                  "ts": 1760989178.4863741,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.278,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 91,
                  "ts": 1760989178.645475,
                  "ts_iso": "2025-10-20 19:39:38",
                  "ok": true,
                  "elapsed_s": 0.319,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 92,
                  "ts": 1760989179.5729148,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.059,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 93,
                  "ts": 1760989179.673451,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 1.011,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 94,
                  "ts": 1760989179.7118576,
                  "ts_iso": "2025-10-20 19:39:39",
                  "ok": true,
                  "elapsed_s": 0.124,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 99,
                  "ts": 1760989329.5089543,
                  "ts_iso": "2025-10-20 19:42:09",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n  \n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False) \n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n  \n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n  \n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n          \n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n          \n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n          \n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n          \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n  \n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n  \n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n  \n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n  \n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n  \n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n  \n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n  \n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr', \n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit', \n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n  \n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n  \n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n  \n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n  \n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n  \n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n  \n  keywords = sorted(list(keywords))\n  \n  #build combined dataset\n  data_dict = {}\n  \n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n                  \n              geo_name = row['geoName']\n              key = (year, geo_name)\n              \n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n              \n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n              \n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n                  \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n  \n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n  \n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n  \n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n  \n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n  \n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n  \n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n  \n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n  \n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n  \n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n          \n          if df.empty:\n              continue\n          \n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n          \n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n          \n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n              \n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]], \n                  on='date', \n                  how='outer'\n              )\n              \n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n  \n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n  \n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 101,
                  "ts": 1760989343.641441,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 14.12,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 102,
                  "ts": 1760989343.7318742,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.196,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 103,
                  "ts": 1760989343.8483806,
                  "ts_iso": "2025-10-20 19:42:23",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), \n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 104,
                  "ts": 1760989344.3380837,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.591,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 105,
                  "ts": 1760989344.436012,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.567,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 106,
                  "ts": 1760989344.4619074,
                  "ts_iso": "2025-10-20 19:42:24",
                  "ok": true,
                  "elapsed_s": 0.11,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 112,
                  "ts": 1760989592.3608563,
                  "ts_iso": "2025-10-20 19:46:32",
                  "ok": true,
                  "elapsed_s": 0.056,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 114,
                  "ts": 1760989606.4840348,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 14.112,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 115,
                  "ts": 1760989606.575149,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.201,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 116,
                  "ts": 1760989606.6917949,
                  "ts_iso": "2025-10-20 19:46:46",
                  "ok": true,
                  "elapsed_s": 0.192,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 117,
                  "ts": 1760989607.1755607,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.58,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 118,
                  "ts": 1760989607.2720747,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.553,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 119,
                  "ts": 1760989607.298635,
                  "ts_iso": "2025-10-20 19:46:47",
                  "ok": true,
                  "elapsed_s": 0.108,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P1",
                  "exec_count": 126,
                  "ts": 1760989738.457167,
                  "ts_iso": "2025-10-20 19:48:58",
                  "ok": true,
                  "elapsed_s": 0.047,
                  "code": "get_ipython().run_line_magic('qid', 'P1')\n\n# Dont change the output directory and save your output files to this folder\noutput_directory = \"preprocessed_data/\"\n\ndef clean(spatial_paths, temporal_paths, stats_paths):\n  # Write you solution here\n  ############# SOLUTION ###############\n  #process each dataset\n  health_stats_df = process_health_stats(stats_paths)\n  spatial_trends_df = process_spatial_trends(spatial_paths)\n  temporal_trends_df = process_temporal_trends(temporal_paths)\n\n  #save files to output directory\n  health_stats_df.to_csv(output_directory + \"health_stats.csv\", index=False)\n  spatial_trends_df.to_csv(output_directory + \"spatial_trends.csv\", index=False)\n  temporal_trends_df.to_csv(output_directory + \"temporal_trends.csv\", index=False)\n\n  return health_stats_df, spatial_trends_df, temporal_trends_df\n\n#process and merge all health dataset files\ndef process_health_stats(stats_paths):\n  all_dfs = []\n\n  for file_path in stats_paths:\n      try:\n          #read csv file with correct path\n          df = pd.read_csv(input_directory + file_path)\n\n          #remove empty rows\n          df = df.dropna(how='all')\n          if df.empty:\n              continue\n\n          #extract metadata from file path\n          path_parts = file_path.split('/')\n          folder_name = path_parts[-2]\n          variable, strat_type = folder_name.split('_')\n          year = int(file_path.split('/')[-1].split('.')[0])  #get year from filename\n\n          #standardize column names and add metadata\n          df = standardize_health_columns(df, year, variable, strat_type)\n          all_dfs.append(df)\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  if not all_dfs:\n      raise ValueError(\"No health statistics files were successfully processed\")\n\n  #combine all dataframes\n  combined_df = pd.concat(all_dfs, ignore_index=True, sort=False)\n\n  #add unique ID column\n  combined_df['id'] = range(1, len(combined_df) + 1)\n\n  #define final column order as per README\n  final_columns = [\n      'id', 'year', 'locationabbr', 'locationdesc', 'data_value',\n      'low_confidence_limit', 'high_confidence_limit', 'sample_size',\n      'stratification', 'stratificationtype', 'variable'\n  ]\n\n  #ensure all columns exist\n  for col in final_columns:\n      if col not in combined_df.columns:\n          combined_df[col] = np.nan\n\n  #reorder columns and sort\n  combined_df = combined_df[final_columns]\n  combined_df = combined_df.sort_values(['variable', 'year', 'id']).reset_index(drop=True)\n\n  #handle missing values\n  combined_df = combined_df.replace(['*', ''], np.nan)\n\n  return combined_df\n\n#standardize column names for health statistics\ndef standardize_health_columns(df, year, variable, strat_type):\n  #map column names to standard format\n  column_mapping = {\n      'Year': 'year',\n      'LocationAbbr': 'locationabbr',\n      'LocationDesc': 'locationdesc',\n      'DataValue': 'data_value',\n      'Data_Value': 'data_value',\n      'LowConfidenceLimit': 'low_confidence_limit',\n      'HighConfidenceLimit': 'high_confidence_limit',\n      'SampleSize': 'sample_size',\n      'Stratification': 'stratification',\n      'Stratification1': 'stratification'\n  }\n\n  #rename columns\n  df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n\n  #map stratification types\n  strat_map = {'age': 'Age Group', 'gender': 'Gender', 'overall': 'Overall'}\n\n  #add metadata columns\n  df['year'] = year\n  df['variable'] = variable\n  df['stratificationtype'] = strat_map.get(strat_type, strat_type.capitalize())\n\n  #handle overall stratification\n  if strat_type == 'overall' and 'stratification' not in df.columns:\n      df['stratification'] = 'Overall'\n\n  return df\n\n#process spatial search intensity files\ndef process_spatial_trends(spatial_paths):\n  #extract all unique keywords from filenames\n  keywords = set()\n  for path in spatial_paths:\n      filename = path.split('/')[-1]  #get filename without path\n      keyword_parts = filename.split('_')[2:]\n      keyword = '_'.join(keyword_parts).replace('.csv', '')\n      keywords.add(keyword)\n\n  keywords = sorted(list(keywords))\n\n  #build combined dataset\n  data_dict = {}\n\n  for file_path in spatial_paths:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract metadata from filename\n          filename = file_path.split('/')[-1]\n          year = int(filename.split('_')[0])\n          keyword_parts = filename.split('_')[2:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #process each state's data\n          for _, row in df.iterrows():\n              if 'geoName' not in row:\n                  continue\n\n              geo_name = row['geoName']\n              key = (year, geo_name)\n\n              #initialize state-year entry if not exists\n              if key not in data_dict:\n                  data_dict[key] = {'year': year, 'geoName': geo_name}\n\n              #find intensity value\n              intensity_value = None\n              for col in df.columns:\n                  if col != 'geoName' and pd.notna(row[col]):\n                      intensity_value = row[col]\n                      break\n\n              if intensity_value is not None:\n                  data_dict[key][keyword] = intensity_value\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #convert to dataframe\n  spatial_df = pd.DataFrame(list(data_dict.values()))\n\n  #ensure all keyword columns exist\n  for keyword in keywords:\n      if keyword not in spatial_df.columns:\n          spatial_df[keyword] = np.nan\n\n  #reorder columns: geoName, year, then sorted keywords\n  column_order = ['geoName', 'year'] + keywords\n  spatial_df = spatial_df[column_order]\n\n  #sort as per README: by year, then geoName\n  spatial_df = spatial_df.sort_values(['year', 'geoName']).reset_index(drop=True)\n\n  return spatial_df\n\n#process temporal search intensity files\ndef process_temporal_trends(temporal_paths):\n  if not temporal_paths:\n      raise ValueError(\"No temporal trend files found\")\n\n  #start with first file to establish base\n  first_file = temporal_paths[0]\n  temporal_df = pd.read_csv(input_directory + first_file)\n  temporal_df = temporal_df.dropna(how='all')\n\n  #extract keyword from first filename\n  first_filename = first_file.split('/')[-1]\n  first_keyword_parts = first_filename.split('_')[3:]\n  first_keyword = '_'.join(first_keyword_parts).replace('.csv', '')\n\n  #ensure we only keep date and keyword columns\n  if len(temporal_df.columns) > 2:\n      temporal_df = temporal_df.iloc[:, :2]\n\n  #rename intensity column to match keyword\n  if len(temporal_df.columns) >= 2:\n      intensity_col = temporal_df.columns[1]\n      temporal_df = temporal_df.rename(columns={intensity_col: first_keyword})\n\n  #merge remaining files\n  for file_path in temporal_paths[1:]:\n      try:\n          df = pd.read_csv(input_directory + file_path)\n          df = df.dropna(how='all')\n\n          if df.empty:\n              continue\n\n          #extract keyword from filename\n          filename = file_path.split('/')[-1]\n          keyword_parts = filename.split('_')[3:]\n          keyword = '_'.join(keyword_parts).replace('.csv', '')\n\n          #ensure we only keep date and keyword columns\n          if len(df.columns) > 2:\n              df = df.iloc[:, :2]\n\n          #rename intensity column and merge\n          if len(df.columns) >= 2:\n              intensity_col = df.columns[1]\n              df = df.rename(columns={intensity_col: keyword})\n\n              #merge on date column\n              temporal_df = temporal_df.merge(\n                  df[['date', keyword]],\n                  on='date',\n                  how='outer'\n              )\n\n      except Exception as e:\n          print(f\"Error processing {file_path}: {e}\")\n          continue\n\n  #dort by date\n  temporal_df['date'] = pd.to_datetime(temporal_df['date'])\n  temporal_df = temporal_df.sort_values('date').reset_index(drop=True)\n  temporal_df['date'] = temporal_df['date'].dt.strftime('%Y-%m-%d')\n\n  return temporal_df\n    ############# SOLUTION END ###############"
                },
                {
                  "qid": "P2a",
                  "exec_count": 128,
                  "ts": 1760989752.2729597,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 13.807,
                  "code": "get_ipython().run_line_magic('qid', 'P2a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n#read files\nmovies = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/movie.csv', sep='\\t', encoding='latin-1')\nratings = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/data.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\ngenres_list = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list\n\n# create result dataframe\nresult = (movies\n    .assign(**{'genre(s)': lambda df: df[genres_list].apply(lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])), axis=1)}) #combine genres alphabetically\n    .merge(ratings.groupby('movie_id')['rating'].agg(average_rating='mean', number_of_raters='count'), on='movie_id') #merge with rating stats\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown' and number_of_raters >= 50 and average_rating >= 3\") #filter by criteria\n    [['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']] #select required columns\n    .sort_values('average_rating', ascending=False).reset_index(drop=True)) #sort by rating descending\n\n#print outcome\nprint(f\"\\nTotal movies after filtering: {len(result)}\")\nprint(\"Top 10 highest rated movies:\")\nresult.head(10)\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bi",
                  "exec_count": 129,
                  "ts": 1760989752.363594,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.198,
                  "code": "get_ipython().run_line_magic('qid', 'P2bi')\n\n# Write you solution here\ndf_movie_gender = None\n############# SOLUTION FOR df_movie_gender ###############\n#read last csv\nusers = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/movielens/user.csv', sep='\\t', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\nmovies.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url'] + genres_list #column order\n\ndf_movie_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies[['movie_id', 'movie_title']], on='movie_id') #merge with movie titles to get movie names\n    .groupby(['movie_title', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False) #sort descending\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n)\n\ndf_movie_gender.head()\n############# SOLUTION END ############"
                },
                {
                  "qid": "P2bii",
                  "exec_count": 130,
                  "ts": 1760989752.4795728,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.191,
                  "code": "get_ipython().run_line_magic('qid', 'P2bii')\n\n# Write you solution here\ndf_genre_gender = None\n############# SOLUTION FOR df_genre_gender ###############\nmovies_with_genres = movies.assign(\n    **{'genre(s)': lambda df: df[genres_list].apply(\n        lambda row: ' and '.join(sorted([g for g, v in zip(genres_list, row) if v == 1])),\n        axis=1\n    )}\n)\n\ndf_genre_gender = (ratings\n    .merge(users[['user_id', 'gender']], on='user_id') #merge ratings with user gender information\n    .merge(movies_with_genres[['movie_id', 'genre(s)']], on='movie_id') #merge with movie genre information\n    .query(\"`genre(s)` != '' and `genre(s)` != 'unknown'\") #filter out empty genre\n    .groupby(['genre(s)', 'gender'])['rating'].mean() #group by movie title and gender and calculate mean for each\n    .unstack().dropna() #remove movies that dont have ratings and convert gender from row to col\n    .assign(**{'difference in average ratings': lambda df: abs(df['M'] - df['F'])}) #create new col for difference\n    .sort_values('difference in average ratings', ascending=False)\n    [['difference in average ratings']]\n    .reset_index() #convert to regular dataframe\n    .rename(columns={'genre(s)': 'genre'}) #remane col\n)\n\ndf_genre_gender.head()\n############# SOLUTION End ############"
                },
                {
                  "qid": "P3a",
                  "exec_count": 131,
                  "ts": 1760989752.9497242,
                  "ts_iso": "2025-10-20 19:49:12",
                  "ok": true,
                  "elapsed_s": 0.565,
                  "code": "get_ipython().run_line_magic('qid', 'P3a')\n\n# Write you solution here\n\n############# SOLUTION ###############\n\n#load main COVID-19 dataset\ncovid_data = pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/owid-covid-data.csv')\n\n#filter relevant columns and prepare data\nresult_countries = (\n    covid_data[['location', 'date', 'stringency_index']] #select needed columns\n    .dropna(subset=['stringency_index']) #remove rows with missing stringency data\n    .assign(\n        month=lambda x: pd.to_datetime(x['date']).dt.to_period('M') #extract year-month\n    )\n    .groupby(['location', 'month'])['stringency_index'] #group by country and month\n    .mean() #calculate monthly average stringency\n    .reset_index()\n    .assign(\n        #quantize into three stringency categories\n        stringency_category=lambda x: pd.cut(\n            x['stringency_index'],\n            bins=[0, 40, 70, 100],\n            labels=['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n        )\n    )\n    .groupby('location')['stringency_category'] #group by country to check all months\n    .apply(lambda categories: set(categories.unique())) #get unique categories as set\n    .loc[lambda x: x == {'less_stringent'}] #filter countries with only 'less_stringent'\n    .index #get country names\n)\n\nprint(f\"Number of always less stringent countries: {len(result_countries)}\")\nresult_countries\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3b",
                  "exec_count": 132,
                  "ts": 1760989753.0564003,
                  "ts_iso": "2025-10-20 19:49:13",
                  "ok": true,
                  "elapsed_s": 0.554,
                  "code": "get_ipython().run_line_magic('qid', 'P3b')\n\n# Write you solution here\n############# SOLUTION ###############\nresult = (pd.read_csv('/content/drive/MyDrive/Applied Data Science/datasets/coronavirus_pandemic/c4.csv') #load dataset\n          .melt(id_vars=['country_name'], var_name='date', value_name='restriction_level') #covert to long format\n          .loc[lambda x: x['restriction_level'] == 0] #filter to days with no restrictions\n          .groupby('country_name')['date'] #group by country\n          .count() #count days\n          .reset_index(name='days_with_no_gathering_restrictions') #convert to dataframe\n          .sort_values('days_with_no_gathering_restrictions', ascending=False)) #sort in descending order\n\nresult\n############# SOLUTION END ############"
                },
                {
                  "qid": "P3c",
                  "exec_count": 133,
                  "ts": 1760989753.08663,
                  "ts_iso": "2025-10-20 19:49:13",
                  "ok": true,
                  "elapsed_s": 0.122,
                  "code": "get_ipython().run_line_magic('qid', 'P3c')\n\n# Write you solution here\n\n############# SOLUTION ###############\nresult[result['country_name'].isin(result_countries)].head(1)\n############# SOLUTION END ############"
                }
              ]
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "result[result['country_name'].isin(result_countries)].head(1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PUxdctIdxU6B"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}